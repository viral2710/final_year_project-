{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Driving Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "We are here building a minimal version of self driving car. Here, we have a front camera view. This will transfer input to the computer. Then Deep Learning algorithm in computer predicts the steering angle to avoid all sorts of collisions. Predicting steering angle can be thought of as a regression problem. We will feed images to Convolutional Neural Network and the label will be the steering angle in that image. Model will learn the steering angle from the as per the turns in the image and will finally predicts steering angle for unknown images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Refer this: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "\n",
    "There are total 45406 images in the dataset along with their steering angles. We will split the dataset into train and test in a ratio of 80:20 <b>sequentially</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\viral\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import pi\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading images from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = r\"C:\\Users\\viral\\Desktop\\final_project\\driving_dataset\"\n",
    "DATA_FILE = os.path.join(DATA_FOLDER, \"data.txt\")\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45406 45406\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_FILE) as f:\n",
    "    for line in f:\n",
    "        image_name, angle = line.split()\n",
    "        \n",
    "        image_path = os.path.join(DATA_FOLDER, image_name)\n",
    "        x.append(image_path)\n",
    "        \n",
    "        angle_radians = float(angle) * (pi / 180)  #converting angle into radians\n",
    "        y.append(angle_radians)\n",
    "y = np.array(y)\n",
    "print(str(len(x))+\" \"+str(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36324, 36324, 9082, 9082)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio = int(len(x) * 0.8)\n",
    "\n",
    "train_x = x[:split_ratio]\n",
    "train_y = y[:split_ratio]\n",
    "\n",
    "test_x = x[split_ratio:]\n",
    "test_y = y[split_ratio:]\n",
    "\n",
    "len(train_x), len(train_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (10, 7))\n",
    "plt.hist(train_y, bins = 50, histtype = \"step\")\n",
    "plt.hist(test_y, bins = 50, histtype = \"step\")\n",
    "plt.title(\"Steering Wheel angle in train and test\")\n",
    "plt.xlabel(\"Angle\")\n",
    "plt.ylabel(\"Bin count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above histogram plot clearly shows that most of the values list on 0. This is obvious as well as most of the time car runs on straight road so therefore, steering wheel angle is 0 most of the time during driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing function for creating batch of images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(train_x[(train_batch_pointer + i) % len(train_x)]) #here % len(train_x) is used to make sure that\n",
    "        #\"train_batch_pointer + i\" should not cross the number of train images. As soon as the value of \"train_batch_pointer\" is\n",
    "        #equal to number of train images then it will again start reading the train images from the beginning means from 0th\n",
    "        #index onwards.\n",
    "        read_image_road = read_image[-150:] #here, we are taking only the lower part of the images where there is a road in the\n",
    "        #image. As, we are concern only with the curves of the road to predict angles so therefore, we are discarding the upper\n",
    "        #part of the image. Hence, here -\"150\" is equivalent to the last 150 matrix pixels of the image.\n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) #After, resizing, each image will be of size (66, 200, 3). \n",
    "        #now since we have kept only the last 150 matrices in the image so the size of our image is now (150, 455, 3). \n",
    "        #Now 455/150 = 3.0303. Also 200/66 = 3.0303. Hence, here we are keeping the aspect ratio of images same.\n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(train_y[(train_batch_pointer + i) % len(train_y)]) #appending corresponding labels\n",
    "        \n",
    "    train_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestBatch(batch_size):\n",
    "    global test_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(test_x[(test_batch_pointer + i) % len(test_x)]) #here % len(test_x) is used to make sure that\n",
    "        #\"test_batch_pointer + i\" should not cross the number of test images. As soon as the value of \"test_batch_pointer\" is\n",
    "        #equal to number of test images then it will again start reading the test images from the beginning means from 0th\n",
    "        #index onwards.\n",
    "        read_image_road = read_image[-150:] #here, we are taking only the lower part of the images where there is a road in the\n",
    "        #image. As, we are concern only with the curves of the road to predict angles so therefore, we are discarding the upper\n",
    "        #part of the image. Hence, here -\"150\" is equivalent to the last 150 matrix pixels of the image.\n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) #After, resizing, each image will be of size (66, 200, 3). \n",
    "        #now since we have kept only the last 150 matrices in the image so the size of our image is now (150, 455, 3). \n",
    "        #Now 455/150 = 3.0303. Also 200/66 = 3.0303. Hence, here we are keeping the aspect ratio of images same.\n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(test_y[(test_batch_pointer + i) % len(test_y)]) #appending corresponding labels\n",
    "        \n",
    "    test_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightVariable(shape):\n",
    "    initial = tf.truncated_normal(shape = shape, stddev = 0.1)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def convolution(previous_input, filter_input, strides):\n",
    "    return tf.nn.conv2d(previous_input, filter_input, strides = [1, strides, strides, 1], padding = \"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape = [None, 66, 200, 3], name = \"Plc_1\")\n",
    "y_true = tf.placeholder(tf.float32, name = \"Plc_2\")\n",
    "\n",
    "input_image = x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Model_Architecture.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\viral\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#Convolution Layers\n",
    "#First convolution layer\n",
    "W_Conv1 = weightVariable([5,5,3,24])\n",
    "B_Conv1 = bias_variable([24])\n",
    "Conv1 = tf.nn.relu(convolution(input_image, W_Conv1, 2) + B_Conv1)\n",
    "#strides = 2\n",
    "#Output size: 31*98*24\n",
    "\n",
    "#Second convolution layer\n",
    "W_Conv2 = weightVariable([5,5,24,36])\n",
    "B_Conv2 = bias_variable([36])\n",
    "Conv2 = tf.nn.relu(convolution(Conv1, W_Conv2, 2) + B_Conv2)\n",
    "#strides = 2\n",
    "#Output size: 14*47*36\n",
    "\n",
    "#Third convolution layer\n",
    "W_Conv3 = weightVariable([5,5,36,48])\n",
    "B_Conv3 = bias_variable([48])\n",
    "Conv3 = tf.nn.relu(convolution(Conv2, W_Conv3, 2) + B_Conv3)\n",
    "#strides = 2\n",
    "#Output size: 5*22*48\n",
    "\n",
    "#Fourth convolution layer\n",
    "W_Conv4 = weightVariable([3,3,48,64])\n",
    "B_Conv4 = bias_variable([64])\n",
    "Conv4 = tf.nn.relu(convolution(Conv3, W_Conv4, 1) + B_Conv4)\n",
    "#strides = 1\n",
    "#Output size: 3*20*64\n",
    "\n",
    "\n",
    "#Fifth convolution layer\n",
    "W_Conv5 = weightVariable([3,3,64,64])\n",
    "B_Conv5 = bias_variable([64])\n",
    "Conv5 = tf.nn.relu(convolution(Conv4, W_Conv5, 1) + B_Conv5)\n",
    "#strides = 1\n",
    "#Output size: 1*18*64\n",
    "\n",
    "#Fully-Connected Dense Layers\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "#First FC-Dense\n",
    "#Input = 1*18*64 = 1152\n",
    "W_FC1 = weightVariable([1152, 1164])\n",
    "B_FC1 = bias_variable([1164])\n",
    "FC1_Flatten = tf.reshape(Conv5, [-1, 1152]) #here, -1 indicates 1. It means that the shape of FC1_Flatten will be 1*1152\n",
    "Output_FC1 = tf.nn.relu(tf.matmul(FC1_Flatten, W_FC1) + B_FC1) #so, here shape of FC1_Flatten is 1*1152 and shape of W_FC1 will\n",
    "#be 1152*1164. Therefore, there will be a matrix multiplication of matrices: (1*1152) * (1152*1164) = (1*1164).\n",
    "Output_FC1_drop = tf.nn.dropout(Output_FC1, keep_prob)\n",
    "\n",
    "#Second FC-Dense\n",
    "#Input = 1*1164 = 1164\n",
    "W_FC2 = weightVariable([1164, 100])\n",
    "B_FC2 = bias_variable([100])\n",
    "Output_FC2 = tf.nn.relu(tf.matmul(Output_FC1_drop, W_FC2) + B_FC2) #so, here shape of Output_FC1_drop is 1*1164 and shape of \n",
    "#W_FC2 will be 1164*100. Therefore, there will be a matrix multiplication of matrices: (1*1164) * (1164*100) = (1*100).\n",
    "Output_FC2_drop = tf.nn.dropout(Output_FC2, keep_prob)\n",
    "\n",
    "#Third FC-Dense\n",
    "#Input = 1*100 = 100\n",
    "W_FC3 = weightVariable([100, 50])\n",
    "B_FC3 = bias_variable([50])\n",
    "Output_FC3 = tf.nn.relu(tf.matmul(Output_FC2_drop, W_FC3) + B_FC3) #so, here shape of Output_FC2_drop is 1*100 and shape of \n",
    "#W_FC3 will be 100*50. Therefore, there will be a matrix multiplication of matrices: (1*100) * (100*50) = (1*50).\n",
    "Output_FC3_drop = tf.nn.dropout(Output_FC3, keep_prob)\n",
    "\n",
    "#Fourth FC-Dense\n",
    "#Input = 1*50 = 50\n",
    "W_FC4 = weightVariable([50, 10])\n",
    "B_FC4 = bias_variable([10])\n",
    "Output_FC4 = tf.nn.relu(tf.matmul(Output_FC3_drop, W_FC4) + B_FC4) #so, here shape of Output_FC3_drop is 1*50 and shape of \n",
    "#W_FC4 will be 50*10. Therefore, there will be a matrix multiplication of matrices: (1*50) * (50*10) = (1*10).\n",
    "Output_FC4_drop = tf.nn.dropout(Output_FC4, keep_prob)\n",
    "\n",
    "#Final Output to one neuron with linear/identity function\n",
    "#Input = 1*10 = 10\n",
    "W_FC5 = weightVariable([10, 1])\n",
    "B_FC5 = bias_variable([1])\n",
    "y_predicted = tf.identity(tf.matmul(Output_FC4_drop, W_FC5) + B_FC5) #so, here shape of Output_FC4_drop is 1*10 and shape of \n",
    "#W_FC5 will be 10*1. Therefore, there will be a matrix multiplication of matrices: (1*10) * (10*1) = (1*1). Since, this is a \n",
    "#regression problem so we have applied identity fuction in the end. We can also apply \"atan\" function here. If computational\n",
    "#power is available then the model should be tested with both identity and atan functions. In the end, that function should be\n",
    "#considered which gives better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454.06\n",
      "1:Epoch: 1, Train_Loss: 7.376565933227539, Test_Loss: 7.553927421569824 *\n",
      "2:Epoch: 1, Train_Loss: 6.781405448913574, Test_Loss: 6.195307731628418 *\n",
      "3:Epoch: 1, Train_Loss: 19.47220230102539, Test_Loss: 6.180707931518555 *\n",
      "4:Epoch: 1, Train_Loss: 7.801889419555664, Test_Loss: 6.177549362182617 *\n",
      "5:Epoch: 1, Train_Loss: 7.830547332763672, Test_Loss: 6.1470842361450195 *\n",
      "6:Epoch: 1, Train_Loss: 9.521758079528809, Test_Loss: 6.161056041717529\n",
      "7:Epoch: 1, Train_Loss: 6.195389270782471, Test_Loss: 6.257521152496338\n",
      "8:Epoch: 1, Train_Loss: 6.1840739250183105, Test_Loss: 6.829221248626709\n",
      "9:Epoch: 1, Train_Loss: 7.411583423614502, Test_Loss: 7.373073577880859\n",
      "10:Epoch: 1, Train_Loss: 17.844032287597656, Test_Loss: 6.294064998626709 *\n",
      "11:Epoch: 1, Train_Loss: 6.9775004386901855, Test_Loss: 6.24146032333374 *\n",
      "12:Epoch: 1, Train_Loss: 6.137892246246338, Test_Loss: 6.108552932739258 *\n",
      "13:Epoch: 1, Train_Loss: 11.470985412597656, Test_Loss: 6.103013515472412 *\n",
      "14:Epoch: 1, Train_Loss: 7.3961944580078125, Test_Loss: 6.097992897033691 *\n",
      "15:Epoch: 1, Train_Loss: 6.116572380065918, Test_Loss: 6.108577728271484\n",
      "16:Epoch: 1, Train_Loss: 6.109114646911621, Test_Loss: 6.116448402404785\n",
      "17:Epoch: 1, Train_Loss: 6.093748092651367, Test_Loss: 6.130139350891113\n",
      "18:Epoch: 1, Train_Loss: 6.089522838592529, Test_Loss: 6.093142986297607 *\n",
      "19:Epoch: 1, Train_Loss: 6.1145477294921875, Test_Loss: 6.133346080780029\n",
      "20:Epoch: 1, Train_Loss: 6.11128568649292, Test_Loss: 6.411437511444092\n",
      "21:Epoch: 1, Train_Loss: 6.088627338409424, Test_Loss: 6.1203532218933105 *\n",
      "22:Epoch: 1, Train_Loss: 6.05985164642334, Test_Loss: 6.216825008392334\n",
      "23:Epoch: 1, Train_Loss: 6.04550838470459, Test_Loss: 6.070703029632568 *\n",
      "24:Epoch: 1, Train_Loss: 6.04293155670166, Test_Loss: 6.069511890411377 *\n",
      "25:Epoch: 1, Train_Loss: 6.036816120147705, Test_Loss: 6.061604976654053 *\n",
      "26:Epoch: 1, Train_Loss: 6.041200637817383, Test_Loss: 6.049156665802002 *\n",
      "27:Epoch: 1, Train_Loss: 6.044107437133789, Test_Loss: 6.049979209899902\n",
      "28:Epoch: 1, Train_Loss: 6.0386271476745605, Test_Loss: 10.563014030456543\n",
      "29:Epoch: 1, Train_Loss: 6.012881755828857, Test_Loss: 6.860937595367432 *\n",
      "30:Epoch: 1, Train_Loss: 6.010727405548096, Test_Loss: 6.017923355102539 *\n",
      "31:Epoch: 1, Train_Loss: 6.006711483001709, Test_Loss: 6.005200386047363 *\n",
      "32:Epoch: 1, Train_Loss: 5.998481273651123, Test_Loss: 6.006975173950195\n",
      "33:Epoch: 1, Train_Loss: 5.990589618682861, Test_Loss: 6.010857105255127\n",
      "34:Epoch: 1, Train_Loss: 5.98297119140625, Test_Loss: 5.9925856590271 *\n",
      "35:Epoch: 1, Train_Loss: 5.978198051452637, Test_Loss: 5.979592323303223 *\n",
      "36:Epoch: 1, Train_Loss: 5.972456455230713, Test_Loss: 5.97413969039917 *\n",
      "37:Epoch: 1, Train_Loss: 5.967337608337402, Test_Loss: 5.969466686248779 *\n",
      "38:Epoch: 1, Train_Loss: 5.961987018585205, Test_Loss: 5.963737964630127 *\n",
      "39:Epoch: 1, Train_Loss: 5.958313941955566, Test_Loss: 5.957972526550293 *\n",
      "40:Epoch: 1, Train_Loss: 5.974826812744141, Test_Loss: 5.9723801612854\n",
      "41:Epoch: 1, Train_Loss: 5.997372627258301, Test_Loss: 5.994836330413818\n",
      "42:Epoch: 1, Train_Loss: 5.963618755340576, Test_Loss: 5.967672348022461 *\n",
      "43:Epoch: 1, Train_Loss: 5.946987152099609, Test_Loss: 5.937768936157227 *\n",
      "44:Epoch: 1, Train_Loss: 5.937882900238037, Test_Loss: 5.931498050689697 *\n",
      "45:Epoch: 1, Train_Loss: 15.453617095947266, Test_Loss: 5.931827068328857\n",
      "46:Epoch: 1, Train_Loss: 6.233761787414551, Test_Loss: 5.92692756652832 *\n",
      "47:Epoch: 1, Train_Loss: 5.940999984741211, Test_Loss: 5.917746543884277 *\n",
      "48:Epoch: 1, Train_Loss: 5.911093235015869, Test_Loss: 5.918677806854248\n",
      "49:Epoch: 1, Train_Loss: 5.915306568145752, Test_Loss: 5.907689094543457 *\n",
      "50:Epoch: 1, Train_Loss: 5.952025413513184, Test_Loss: 5.910922050476074\n",
      "51:Epoch: 1, Train_Loss: 5.931609630584717, Test_Loss: 5.910356521606445 *\n",
      "52:Epoch: 1, Train_Loss: 5.896206378936768, Test_Loss: 5.905335426330566 *\n",
      "53:Epoch: 1, Train_Loss: 6.031764507293701, Test_Loss: 5.897130012512207 *\n",
      "54:Epoch: 1, Train_Loss: 6.064670085906982, Test_Loss: 5.889319896697998 *\n",
      "55:Epoch: 1, Train_Loss: 6.031893253326416, Test_Loss: 5.887502193450928 *\n",
      "56:Epoch: 1, Train_Loss: 5.8796610832214355, Test_Loss: 5.884490013122559 *\n",
      "57:Epoch: 1, Train_Loss: 5.999846458435059, Test_Loss: 5.875202178955078 *\n",
      "58:Epoch: 1, Train_Loss: 6.008723735809326, Test_Loss: 5.933553218841553\n",
      "59:Epoch: 1, Train_Loss: 6.033445358276367, Test_Loss: 6.169345855712891\n",
      "60:Epoch: 1, Train_Loss: 6.004940032958984, Test_Loss: 11.267782211303711\n",
      "61:Epoch: 1, Train_Loss: 5.988256931304932, Test_Loss: 5.864692687988281 *\n",
      "62:Epoch: 1, Train_Loss: 5.885959625244141, Test_Loss: 5.841059684753418 *\n",
      "63:Epoch: 1, Train_Loss: 5.850826263427734, Test_Loss: 5.852859020233154\n",
      "64:Epoch: 1, Train_Loss: 5.8927202224731445, Test_Loss: 5.869847774505615\n",
      "65:Epoch: 1, Train_Loss: 5.842070579528809, Test_Loss: 5.878742218017578\n",
      "66:Epoch: 1, Train_Loss: 5.824260234832764, Test_Loss: 5.8202643394470215 *\n",
      "67:Epoch: 1, Train_Loss: 5.815966606140137, Test_Loss: 5.947079658508301\n",
      "68:Epoch: 1, Train_Loss: 5.807229518890381, Test_Loss: 5.863681793212891 *\n",
      "69:Epoch: 1, Train_Loss: 5.980100154876709, Test_Loss: 5.801701068878174 *\n",
      "70:Epoch: 1, Train_Loss: 11.48684310913086, Test_Loss: 5.863593101501465\n",
      "71:Epoch: 1, Train_Loss: 5.802198886871338, Test_Loss: 5.793254852294922 *\n",
      "72:Epoch: 1, Train_Loss: 5.792359352111816, Test_Loss: 5.811960697174072\n",
      "73:Epoch: 1, Train_Loss: 5.792376518249512, Test_Loss: 5.811068058013916 *\n",
      "74:Epoch: 1, Train_Loss: 5.792497158050537, Test_Loss: 5.856919765472412\n",
      "75:Epoch: 1, Train_Loss: 5.779353618621826, Test_Loss: 5.835973739624023 *\n",
      "76:Epoch: 1, Train_Loss: 5.768661975860596, Test_Loss: 5.949725151062012\n",
      "77:Epoch: 1, Train_Loss: 5.766834735870361, Test_Loss: 5.824670791625977 *\n",
      "78:Epoch: 1, Train_Loss: 5.794118881225586, Test_Loss: 5.768793106079102 *\n",
      "79:Epoch: 1, Train_Loss: 5.77595329284668, Test_Loss: 5.756869316101074 *\n",
      "80:Epoch: 1, Train_Loss: 5.749886989593506, Test_Loss: 5.750001907348633 *\n",
      "81:Epoch: 1, Train_Loss: 5.744848728179932, Test_Loss: 5.748485088348389 *\n",
      "82:Epoch: 1, Train_Loss: 5.738553047180176, Test_Loss: 5.743066787719727 *\n",
      "83:Epoch: 1, Train_Loss: 5.752781867980957, Test_Loss: 5.738452434539795 *\n",
      "84:Epoch: 1, Train_Loss: 5.730033874511719, Test_Loss: 5.730290412902832 *\n",
      "85:Epoch: 1, Train_Loss: 5.724722862243652, Test_Loss: 5.729796409606934 *\n",
      "86:Epoch: 1, Train_Loss: 5.7568488121032715, Test_Loss: 5.7246904373168945 *\n",
      "87:Epoch: 1, Train_Loss: 5.7676825523376465, Test_Loss: 5.727281093597412\n",
      "88:Epoch: 1, Train_Loss: 5.7307868003845215, Test_Loss: 5.70749568939209 *\n",
      "89:Epoch: 1, Train_Loss: 5.70383882522583, Test_Loss: 5.710968971252441\n",
      "90:Epoch: 1, Train_Loss: 5.700743198394775, Test_Loss: 5.75314474105835\n",
      "91:Epoch: 1, Train_Loss: 5.77001428604126, Test_Loss: 5.70341157913208 *\n",
      "92:Epoch: 1, Train_Loss: 5.7842326164245605, Test_Loss: 5.9909987449646\n",
      "93:Epoch: 1, Train_Loss: 5.779036998748779, Test_Loss: 6.235046863555908\n",
      "94:Epoch: 1, Train_Loss: 5.731017589569092, Test_Loss: 5.865997791290283 *\n",
      "95:Epoch: 1, Train_Loss: 5.690167427062988, Test_Loss: 5.711608409881592 *\n",
      "96:Epoch: 1, Train_Loss: 5.746189594268799, Test_Loss: 5.6884660720825195 *\n",
      "97:Epoch: 1, Train_Loss: 5.7360429763793945, Test_Loss: 5.68119478225708 *\n",
      "98:Epoch: 1, Train_Loss: 5.669064521789551, Test_Loss: 5.814563274383545\n",
      "99:Epoch: 1, Train_Loss: 5.759322643280029, Test_Loss: 6.757142066955566\n",
      "100:Epoch: 1, Train_Loss: 5.6557087898254395, Test_Loss: 7.086007118225098\n",
      "Model saved at location ../Saver/model.ckpt at epoch 1\n",
      "101:Epoch: 1, Train_Loss: 5.650784492492676, Test_Loss: 5.72283411026001 *\n",
      "102:Epoch: 1, Train_Loss: 5.635562419891357, Test_Loss: 5.712960720062256 *\n",
      "103:Epoch: 1, Train_Loss: 5.6306562423706055, Test_Loss: 5.630928039550781 *\n",
      "104:Epoch: 1, Train_Loss: 5.624642848968506, Test_Loss: 5.634461879730225\n",
      "105:Epoch: 1, Train_Loss: 5.620965003967285, Test_Loss: 5.625256538391113 *\n",
      "106:Epoch: 1, Train_Loss: 5.7707977294921875, Test_Loss: 5.636630058288574\n",
      "107:Epoch: 1, Train_Loss: 10.349221229553223, Test_Loss: 5.657064914703369\n",
      "108:Epoch: 1, Train_Loss: 5.661561965942383, Test_Loss: 5.634881019592285 *\n",
      "109:Epoch: 1, Train_Loss: 5.610295295715332, Test_Loss: 5.604933738708496 *\n",
      "110:Epoch: 1, Train_Loss: 5.61186408996582, Test_Loss: 5.703221797943115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111:Epoch: 1, Train_Loss: 5.588990211486816, Test_Loss: 5.998798370361328\n",
      "112:Epoch: 1, Train_Loss: 5.582352638244629, Test_Loss: 5.713013172149658 *\n",
      "113:Epoch: 1, Train_Loss: 5.578407287597656, Test_Loss: 5.706211090087891 *\n",
      "114:Epoch: 1, Train_Loss: 5.572708606719971, Test_Loss: 5.587041854858398 *\n",
      "115:Epoch: 1, Train_Loss: 5.567555904388428, Test_Loss: 5.584359169006348 *\n",
      "116:Epoch: 1, Train_Loss: 5.562981605529785, Test_Loss: 5.582019329071045 *\n",
      "117:Epoch: 1, Train_Loss: 5.62104606628418, Test_Loss: 5.576471328735352 *\n",
      "118:Epoch: 1, Train_Loss: 5.626044750213623, Test_Loss: 5.614011287689209\n",
      "119:Epoch: 1, Train_Loss: 5.62944221496582, Test_Loss: 10.645492553710938\n",
      "120:Epoch: 1, Train_Loss: 5.6053385734558105, Test_Loss: 5.730656623840332 *\n",
      "121:Epoch: 1, Train_Loss: 5.540542125701904, Test_Loss: 5.556214332580566 *\n",
      "122:Epoch: 1, Train_Loss: 5.684439182281494, Test_Loss: 5.5454535484313965 *\n",
      "123:Epoch: 1, Train_Loss: 5.783184051513672, Test_Loss: 5.554389476776123\n",
      "124:Epoch: 1, Train_Loss: 5.7812700271606445, Test_Loss: 5.545366287231445 *\n",
      "125:Epoch: 1, Train_Loss: 5.733525276184082, Test_Loss: 5.535395622253418 *\n",
      "126:Epoch: 1, Train_Loss: 5.517110824584961, Test_Loss: 5.515122890472412 *\n",
      "127:Epoch: 1, Train_Loss: 5.5114617347717285, Test_Loss: 5.51202392578125 *\n",
      "128:Epoch: 1, Train_Loss: 5.507608890533447, Test_Loss: 5.505817890167236 *\n",
      "129:Epoch: 1, Train_Loss: 5.520437717437744, Test_Loss: 5.501112461090088 *\n",
      "130:Epoch: 1, Train_Loss: 5.518679141998291, Test_Loss: 5.496773719787598 *\n",
      "131:Epoch: 1, Train_Loss: 5.5048723220825195, Test_Loss: 5.523134231567383\n",
      "132:Epoch: 1, Train_Loss: 5.490232467651367, Test_Loss: 5.5370988845825195\n",
      "133:Epoch: 1, Train_Loss: 5.480573654174805, Test_Loss: 5.50642204284668 *\n",
      "134:Epoch: 1, Train_Loss: 5.483606815338135, Test_Loss: 5.476893424987793 *\n",
      "135:Epoch: 1, Train_Loss: 5.491711139678955, Test_Loss: 5.473382949829102 *\n",
      "136:Epoch: 1, Train_Loss: 5.662971019744873, Test_Loss: 5.467190742492676 *\n",
      "137:Epoch: 1, Train_Loss: 5.658503532409668, Test_Loss: 5.463253974914551 *\n",
      "138:Epoch: 1, Train_Loss: 5.682172775268555, Test_Loss: 5.457351207733154 *\n",
      "139:Epoch: 1, Train_Loss: 5.489344596862793, Test_Loss: 5.454921722412109 *\n",
      "140:Epoch: 1, Train_Loss: 5.589363098144531, Test_Loss: 5.448171138763428 *\n",
      "141:Epoch: 1, Train_Loss: 5.552663326263428, Test_Loss: 5.445227146148682 *\n",
      "142:Epoch: 1, Train_Loss: 5.517935276031494, Test_Loss: 5.439798355102539 *\n",
      "143:Epoch: 1, Train_Loss: 5.5548601150512695, Test_Loss: 5.435349941253662 *\n",
      "144:Epoch: 1, Train_Loss: 5.710602760314941, Test_Loss: 5.430118083953857 *\n",
      "145:Epoch: 1, Train_Loss: 5.460657119750977, Test_Loss: 5.42477560043335 *\n",
      "146:Epoch: 1, Train_Loss: 5.431939601898193, Test_Loss: 5.41937780380249 *\n",
      "147:Epoch: 1, Train_Loss: 7.7879743576049805, Test_Loss: 5.415432929992676 *\n",
      "148:Epoch: 1, Train_Loss: 6.236674785614014, Test_Loss: 5.413015842437744 *\n",
      "149:Epoch: 1, Train_Loss: 5.447451114654541, Test_Loss: 5.466353893280029\n",
      "150:Epoch: 1, Train_Loss: 5.4630842208862305, Test_Loss: 6.749222755432129\n",
      "151:Epoch: 1, Train_Loss: 5.470592975616455, Test_Loss: 9.431722640991211\n",
      "152:Epoch: 1, Train_Loss: 5.434004783630371, Test_Loss: 5.396312236785889 *\n",
      "153:Epoch: 1, Train_Loss: 5.387223720550537, Test_Loss: 5.386801719665527 *\n",
      "154:Epoch: 1, Train_Loss: 5.410346984863281, Test_Loss: 5.434885025024414\n",
      "155:Epoch: 1, Train_Loss: 5.4968719482421875, Test_Loss: 5.450466632843018\n",
      "156:Epoch: 1, Train_Loss: 5.463709831237793, Test_Loss: 5.455406188964844\n",
      "157:Epoch: 1, Train_Loss: 5.44977331161499, Test_Loss: 5.386011600494385 *\n",
      "158:Epoch: 1, Train_Loss: 5.449194431304932, Test_Loss: 5.46303129196167\n",
      "159:Epoch: 1, Train_Loss: 5.401449680328369, Test_Loss: 5.372623443603516 *\n",
      "160:Epoch: 1, Train_Loss: 5.384732246398926, Test_Loss: 5.363868713378906 *\n",
      "161:Epoch: 1, Train_Loss: 5.3604512214660645, Test_Loss: 5.37741756439209\n",
      "162:Epoch: 1, Train_Loss: 5.397517204284668, Test_Loss: 5.369718551635742 *\n",
      "163:Epoch: 1, Train_Loss: 5.377187728881836, Test_Loss: 5.346426010131836 *\n",
      "164:Epoch: 1, Train_Loss: 5.340516090393066, Test_Loss: 5.4272565841674805\n",
      "165:Epoch: 1, Train_Loss: 5.337090015411377, Test_Loss: 5.477290630340576\n",
      "166:Epoch: 1, Train_Loss: 5.368229866027832, Test_Loss: 5.372785568237305 *\n",
      "167:Epoch: 1, Train_Loss: 5.36470365524292, Test_Loss: 5.407233715057373\n",
      "168:Epoch: 1, Train_Loss: 5.32471227645874, Test_Loss: 5.343578815460205 *\n",
      "169:Epoch: 1, Train_Loss: 5.313404560089111, Test_Loss: 5.378178596496582\n",
      "170:Epoch: 1, Train_Loss: 5.311071872711182, Test_Loss: 5.358373641967773 *\n",
      "171:Epoch: 1, Train_Loss: 5.304306507110596, Test_Loss: 5.343515872955322 *\n",
      "172:Epoch: 1, Train_Loss: 5.3013997077941895, Test_Loss: 5.344962120056152\n",
      "173:Epoch: 1, Train_Loss: 5.296459674835205, Test_Loss: 5.335903167724609 *\n",
      "174:Epoch: 1, Train_Loss: 5.293772220611572, Test_Loss: 5.333139896392822 *\n",
      "175:Epoch: 1, Train_Loss: 5.289084434509277, Test_Loss: 5.326754093170166 *\n",
      "176:Epoch: 1, Train_Loss: 5.281847953796387, Test_Loss: 5.340839862823486\n",
      "177:Epoch: 1, Train_Loss: 5.277912616729736, Test_Loss: 5.319937229156494 *\n",
      "178:Epoch: 1, Train_Loss: 5.274770259857178, Test_Loss: 5.324080944061279\n",
      "179:Epoch: 1, Train_Loss: 5.287634372711182, Test_Loss: 5.273756504058838 *\n",
      "180:Epoch: 1, Train_Loss: 5.281741619110107, Test_Loss: 5.314436912536621\n",
      "181:Epoch: 1, Train_Loss: 5.275882720947266, Test_Loss: 5.387794017791748\n",
      "182:Epoch: 1, Train_Loss: 5.280457496643066, Test_Loss: 5.272988319396973 *\n",
      "183:Epoch: 1, Train_Loss: 5.263070106506348, Test_Loss: 5.8268938064575195\n",
      "184:Epoch: 1, Train_Loss: 5.245303630828857, Test_Loss: 5.926465034484863\n",
      "185:Epoch: 1, Train_Loss: 5.2417073249816895, Test_Loss: 5.474071979522705 *\n",
      "186:Epoch: 1, Train_Loss: 5.24913215637207, Test_Loss: 5.294836044311523 *\n",
      "187:Epoch: 1, Train_Loss: 5.26350736618042, Test_Loss: 5.278534412384033 *\n",
      "188:Epoch: 1, Train_Loss: 5.232291221618652, Test_Loss: 5.238470077514648 *\n",
      "189:Epoch: 1, Train_Loss: 5.226542949676514, Test_Loss: 5.381001949310303\n",
      "190:Epoch: 1, Train_Loss: 5.218860626220703, Test_Loss: 6.388047218322754\n",
      "191:Epoch: 1, Train_Loss: 5.2344255447387695, Test_Loss: 6.209735870361328 *\n",
      "192:Epoch: 1, Train_Loss: 5.263970851898193, Test_Loss: 5.265992641448975 *\n",
      "193:Epoch: 1, Train_Loss: 5.262635707855225, Test_Loss: 5.303574562072754\n",
      "194:Epoch: 1, Train_Loss: 5.225844383239746, Test_Loss: 5.203065872192383 *\n",
      "195:Epoch: 1, Train_Loss: 5.1949462890625, Test_Loss: 5.197179794311523 *\n",
      "196:Epoch: 1, Train_Loss: 5.2515549659729, Test_Loss: 5.201755046844482\n",
      "197:Epoch: 1, Train_Loss: 5.204226493835449, Test_Loss: 5.198884963989258 *\n",
      "198:Epoch: 1, Train_Loss: 5.186348915100098, Test_Loss: 5.228138446807861\n",
      "199:Epoch: 1, Train_Loss: 5.201462745666504, Test_Loss: 5.184951305389404 *\n",
      "200:Epoch: 1, Train_Loss: 5.190940856933594, Test_Loss: 5.189117908477783\n",
      "Model saved at location ../Saver/model.ckpt at epoch 1\n",
      "201:Epoch: 1, Train_Loss: 5.289531230926514, Test_Loss: 5.30348539352417\n",
      "202:Epoch: 1, Train_Loss: 5.252144813537598, Test_Loss: 5.587100505828857\n",
      "203:Epoch: 1, Train_Loss: 5.209754943847656, Test_Loss: 5.414299964904785 *\n",
      "204:Epoch: 1, Train_Loss: 5.1671977043151855, Test_Loss: 5.184363842010498 *\n",
      "205:Epoch: 1, Train_Loss: 5.162694931030273, Test_Loss: 5.171083450317383 *\n",
      "206:Epoch: 1, Train_Loss: 5.159590721130371, Test_Loss: 5.167232036590576 *\n",
      "207:Epoch: 1, Train_Loss: 5.144761562347412, Test_Loss: 5.1640825271606445 *\n",
      "208:Epoch: 1, Train_Loss: 5.146493911743164, Test_Loss: 5.158670902252197 *\n",
      "209:Epoch: 1, Train_Loss: 5.164337158203125, Test_Loss: 5.500768661499023\n",
      "210:Epoch: 1, Train_Loss: 5.169756889343262, Test_Loss: 10.043357849121094\n",
      "211:Epoch: 1, Train_Loss: 5.233880996704102, Test_Loss: 5.176650524139404 *\n",
      "212:Epoch: 1, Train_Loss: 5.12602424621582, Test_Loss: 5.133826732635498 *\n",
      "213:Epoch: 1, Train_Loss: 5.180117607116699, Test_Loss: 5.125891208648682 *\n",
      "214:Epoch: 1, Train_Loss: 5.134042739868164, Test_Loss: 5.13358211517334\n",
      "215:Epoch: 1, Train_Loss: 5.146162986755371, Test_Loss: 5.119024753570557 *\n",
      "216:Epoch: 1, Train_Loss: 5.195797920227051, Test_Loss: 5.113438606262207 *\n",
      "217:Epoch: 1, Train_Loss: 5.347004413604736, Test_Loss: 5.098354816436768 *\n",
      "218:Epoch: 1, Train_Loss: 5.10915470123291, Test_Loss: 5.0952067375183105 *\n",
      "219:Epoch: 1, Train_Loss: 5.128526210784912, Test_Loss: 5.090372562408447 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220:Epoch: 1, Train_Loss: 5.08554220199585, Test_Loss: 5.08575963973999 *\n",
      "221:Epoch: 1, Train_Loss: 5.081219673156738, Test_Loss: 5.083554744720459 *\n",
      "222:Epoch: 1, Train_Loss: 5.077425956726074, Test_Loss: 5.117257595062256\n",
      "223:Epoch: 1, Train_Loss: 5.0723700523376465, Test_Loss: 5.120200157165527\n",
      "224:Epoch: 1, Train_Loss: 5.0840582847595215, Test_Loss: 5.0867018699646 *\n",
      "225:Epoch: 1, Train_Loss: 5.079495429992676, Test_Loss: 5.063966274261475 *\n",
      "226:Epoch: 1, Train_Loss: 5.076505661010742, Test_Loss: 5.060523986816406 *\n",
      "227:Epoch: 1, Train_Loss: 5.07558536529541, Test_Loss: 5.055707931518555 *\n",
      "228:Epoch: 1, Train_Loss: 5.066937446594238, Test_Loss: 5.051118850708008 *\n",
      "229:Epoch: 1, Train_Loss: 5.05433464050293, Test_Loss: 5.046769618988037 *\n",
      "230:Epoch: 1, Train_Loss: 5.043471813201904, Test_Loss: 5.043356895446777 *\n",
      "231:Epoch: 1, Train_Loss: 5.037475109100342, Test_Loss: 5.037748336791992 *\n",
      "232:Epoch: 1, Train_Loss: 5.063601493835449, Test_Loss: 5.034241676330566 *\n",
      "233:Epoch: 1, Train_Loss: 5.070381164550781, Test_Loss: 5.029609680175781 *\n",
      "234:Epoch: 1, Train_Loss: 5.059476375579834, Test_Loss: 5.025731563568115 *\n",
      "235:Epoch: 1, Train_Loss: 5.021788120269775, Test_Loss: 5.021231651306152 *\n",
      "236:Epoch: 1, Train_Loss: 5.066951751708984, Test_Loss: 5.016237735748291 *\n",
      "237:Epoch: 1, Train_Loss: 5.05547571182251, Test_Loss: 5.01221227645874 *\n",
      "238:Epoch: 1, Train_Loss: 5.040222644805908, Test_Loss: 5.008081912994385 *\n",
      "239:Epoch: 1, Train_Loss: 5.013095855712891, Test_Loss: 5.008550643920898\n",
      "240:Epoch: 1, Train_Loss: 5.0398077964782715, Test_Loss: 5.060339450836182\n",
      "241:Epoch: 1, Train_Loss: 4.995749473571777, Test_Loss: 7.460592269897461\n",
      "242:Epoch: 1, Train_Loss: 5.005337238311768, Test_Loss: 7.889473915100098\n",
      "243:Epoch: 1, Train_Loss: 5.002557277679443, Test_Loss: 4.988976001739502 *\n",
      "244:Epoch: 1, Train_Loss: 5.0239338874816895, Test_Loss: 4.982652187347412 *\n",
      "245:Epoch: 1, Train_Loss: 6.297283172607422, Test_Loss: 5.048773765563965\n",
      "246:Epoch: 1, Train_Loss: 8.965978622436523, Test_Loss: 5.05188512802124\n",
      "247:Epoch: 1, Train_Loss: 5.172039031982422, Test_Loss: 5.03611421585083 *\n",
      "248:Epoch: 1, Train_Loss: 4.979626178741455, Test_Loss: 5.005749225616455 *\n",
      "249:Epoch: 1, Train_Loss: 4.971367359161377, Test_Loss: 5.0616559982299805\n",
      "250:Epoch: 1, Train_Loss: 5.130835056304932, Test_Loss: 4.96229362487793 *\n",
      "251:Epoch: 1, Train_Loss: 5.008410930633545, Test_Loss: 4.97462272644043\n",
      "252:Epoch: 1, Train_Loss: 4.958680152893066, Test_Loss: 4.96949577331543 *\n",
      "253:Epoch: 1, Train_Loss: 4.944591045379639, Test_Loss: 4.963728904724121 *\n",
      "254:Epoch: 1, Train_Loss: 5.000444412231445, Test_Loss: 4.947953224182129 *\n",
      "255:Epoch: 1, Train_Loss: 4.9489850997924805, Test_Loss: 5.0390706062316895\n",
      "256:Epoch: 1, Train_Loss: 4.959211826324463, Test_Loss: 5.038540363311768 *\n",
      "257:Epoch: 1, Train_Loss: 5.323951721191406, Test_Loss: 4.99519157409668 *\n",
      "258:Epoch: 1, Train_Loss: 6.296080589294434, Test_Loss: 5.02386999130249\n",
      "259:Epoch: 1, Train_Loss: 5.894844055175781, Test_Loss: 4.944380760192871 *\n",
      "260:Epoch: 1, Train_Loss: 5.013305187225342, Test_Loss: 4.971312522888184\n",
      "261:Epoch: 1, Train_Loss: 5.174376487731934, Test_Loss: 4.963881015777588 *\n",
      "262:Epoch: 1, Train_Loss: 7.440707683563232, Test_Loss: 4.953118801116943 *\n",
      "263:Epoch: 1, Train_Loss: 5.939896583557129, Test_Loss: 4.946600437164307 *\n",
      "264:Epoch: 1, Train_Loss: 4.981199741363525, Test_Loss: 4.94395637512207 *\n",
      "265:Epoch: 1, Train_Loss: 4.945876121520996, Test_Loss: 4.937582015991211 *\n",
      "266:Epoch: 1, Train_Loss: 5.744699954986572, Test_Loss: 4.92913293838501 *\n",
      "267:Epoch: 1, Train_Loss: 6.510639190673828, Test_Loss: 4.9427618980407715\n",
      "268:Epoch: 1, Train_Loss: 5.52671480178833, Test_Loss: 4.930391788482666 *\n",
      "269:Epoch: 1, Train_Loss: 4.894460201263428, Test_Loss: 4.924114227294922 *\n",
      "270:Epoch: 1, Train_Loss: 4.884042263031006, Test_Loss: 4.883973598480225 *\n",
      "271:Epoch: 1, Train_Loss: 5.328909873962402, Test_Loss: 4.936336517333984\n",
      "272:Epoch: 1, Train_Loss: 5.263115882873535, Test_Loss: 4.992202281951904\n",
      "273:Epoch: 1, Train_Loss: 4.884509086608887, Test_Loss: 4.909050941467285 *\n",
      "274:Epoch: 1, Train_Loss: 4.9250078201293945, Test_Loss: 5.534553527832031\n",
      "275:Epoch: 1, Train_Loss: 5.006247043609619, Test_Loss: 5.469465255737305 *\n",
      "276:Epoch: 1, Train_Loss: 5.07474422454834, Test_Loss: 5.0385589599609375 *\n",
      "277:Epoch: 1, Train_Loss: 4.9337158203125, Test_Loss: 4.907323837280273 *\n",
      "278:Epoch: 1, Train_Loss: 5.241135120391846, Test_Loss: 4.8778977394104 *\n",
      "279:Epoch: 1, Train_Loss: 4.948307037353516, Test_Loss: 4.863764762878418 *\n",
      "280:Epoch: 1, Train_Loss: 4.907609462738037, Test_Loss: 5.117373466491699\n",
      "281:Epoch: 1, Train_Loss: 5.14026403427124, Test_Loss: 6.145596981048584\n",
      "282:Epoch: 1, Train_Loss: 5.23190450668335, Test_Loss: 5.539847373962402 *\n",
      "283:Epoch: 1, Train_Loss: 5.363732814788818, Test_Loss: 4.920133113861084 *\n",
      "284:Epoch: 1, Train_Loss: 5.041965484619141, Test_Loss: 4.872157096862793 *\n",
      "285:Epoch: 1, Train_Loss: 4.8902482986450195, Test_Loss: 4.819525241851807 *\n",
      "286:Epoch: 1, Train_Loss: 4.924520015716553, Test_Loss: 4.811674118041992 *\n",
      "287:Epoch: 1, Train_Loss: 4.866591453552246, Test_Loss: 4.818324089050293\n",
      "288:Epoch: 1, Train_Loss: 4.820958614349365, Test_Loss: 4.819676399230957\n",
      "289:Epoch: 1, Train_Loss: 4.798996925354004, Test_Loss: 4.8433709144592285\n",
      "290:Epoch: 1, Train_Loss: 4.795458793640137, Test_Loss: 4.796865463256836 *\n",
      "291:Epoch: 1, Train_Loss: 4.793445110321045, Test_Loss: 4.843667030334473\n",
      "292:Epoch: 1, Train_Loss: 4.79022216796875, Test_Loss: 4.919625759124756\n",
      "293:Epoch: 1, Train_Loss: 4.809035301208496, Test_Loss: 5.185881614685059\n",
      "294:Epoch: 1, Train_Loss: 4.851031303405762, Test_Loss: 5.031220436096191 *\n",
      "295:Epoch: 1, Train_Loss: 4.861729621887207, Test_Loss: 4.803091526031494 *\n",
      "296:Epoch: 1, Train_Loss: 4.847707748413086, Test_Loss: 4.792852401733398 *\n",
      "297:Epoch: 1, Train_Loss: 5.238272190093994, Test_Loss: 4.788824558258057 *\n",
      "298:Epoch: 1, Train_Loss: 4.930443286895752, Test_Loss: 4.785492420196533 *\n",
      "299:Epoch: 1, Train_Loss: 4.778960227966309, Test_Loss: 4.785486698150635 *\n",
      "300:Epoch: 1, Train_Loss: 4.8660125732421875, Test_Loss: 5.9668121337890625\n",
      "Model saved at location ../Saver/model.ckpt at epoch 1\n",
      "301:Epoch: 1, Train_Loss: 5.143735408782959, Test_Loss: 8.87594223022461\n",
      "302:Epoch: 1, Train_Loss: 5.243830680847168, Test_Loss: 4.765763282775879 *\n",
      "303:Epoch: 1, Train_Loss: 4.748338222503662, Test_Loss: 4.7511982917785645 *\n",
      "304:Epoch: 1, Train_Loss: 4.747501373291016, Test_Loss: 4.749810218811035 *\n",
      "305:Epoch: 1, Train_Loss: 5.407705783843994, Test_Loss: 4.753718376159668\n",
      "306:Epoch: 1, Train_Loss: 5.536044120788574, Test_Loss: 4.739701747894287 *\n",
      "307:Epoch: 1, Train_Loss: 4.933036804199219, Test_Loss: 4.736435413360596 *\n",
      "308:Epoch: 1, Train_Loss: 4.779845237731934, Test_Loss: 4.726410388946533 *\n",
      "309:Epoch: 1, Train_Loss: 4.738860130310059, Test_Loss: 4.722662925720215 *\n",
      "310:Epoch: 1, Train_Loss: 5.394096374511719, Test_Loss: 4.719691276550293 *\n",
      "311:Epoch: 1, Train_Loss: 6.265525817871094, Test_Loss: 4.714069843292236 *\n",
      "312:Epoch: 1, Train_Loss: 4.80758810043335, Test_Loss: 4.714829921722412\n",
      "313:Epoch: 1, Train_Loss: 4.72495174407959, Test_Loss: 4.745593547821045\n",
      "314:Epoch: 1, Train_Loss: 4.705947399139404, Test_Loss: 4.7417120933532715 *\n",
      "315:Epoch: 1, Train_Loss: 4.70361852645874, Test_Loss: 4.708160877227783 *\n",
      "316:Epoch: 1, Train_Loss: 5.184019565582275, Test_Loss: 4.695130348205566 *\n",
      "317:Epoch: 1, Train_Loss: 4.720134735107422, Test_Loss: 4.691824436187744 *\n",
      "318:Epoch: 1, Train_Loss: 4.7993035316467285, Test_Loss: 4.688390254974365 *\n",
      "319:Epoch: 1, Train_Loss: 4.924361228942871, Test_Loss: 4.683490753173828 *\n",
      "320:Epoch: 1, Train_Loss: 4.692972660064697, Test_Loss: 4.680269241333008 *\n",
      "321:Epoch: 1, Train_Loss: 4.795289039611816, Test_Loss: 4.676618576049805 *\n",
      "322:Epoch: 1, Train_Loss: 4.813387870788574, Test_Loss: 4.672114372253418 *\n",
      "323:Epoch: 1, Train_Loss: 5.061036586761475, Test_Loss: 4.66951847076416 *\n",
      "324:Epoch: 1, Train_Loss: 4.706495761871338, Test_Loss: 4.665260314941406 *\n",
      "325:Epoch: 1, Train_Loss: 4.812549591064453, Test_Loss: 4.661707401275635 *\n",
      "326:Epoch: 1, Train_Loss: 4.965390682220459, Test_Loss: 4.657705307006836 *\n",
      "327:Epoch: 1, Train_Loss: 4.759637832641602, Test_Loss: 4.6536359786987305 *\n",
      "328:Epoch: 1, Train_Loss: 4.696741104125977, Test_Loss: 4.650016784667969 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329:Epoch: 1, Train_Loss: 4.649383068084717, Test_Loss: 4.646263599395752 *\n",
      "330:Epoch: 1, Train_Loss: 4.6744279861450195, Test_Loss: 4.667341232299805\n",
      "331:Epoch: 1, Train_Loss: 4.68637228012085, Test_Loss: 4.683467864990234\n",
      "332:Epoch: 1, Train_Loss: 5.120826721191406, Test_Loss: 8.0895357131958\n",
      "333:Epoch: 1, Train_Loss: 5.073978424072266, Test_Loss: 6.585526466369629 *\n",
      "334:Epoch: 1, Train_Loss: 5.174041748046875, Test_Loss: 4.629127502441406 *\n",
      "335:Epoch: 1, Train_Loss: 5.5649614334106445, Test_Loss: 4.629449367523193\n",
      "336:Epoch: 1, Train_Loss: 4.799297332763672, Test_Loss: 4.687580108642578\n",
      "337:Epoch: 1, Train_Loss: 5.2161383628845215, Test_Loss: 4.693064212799072\n",
      "338:Epoch: 1, Train_Loss: 4.82344388961792, Test_Loss: 4.648045539855957 *\n",
      "339:Epoch: 1, Train_Loss: 4.617072582244873, Test_Loss: 4.6692352294921875\n",
      "340:Epoch: 1, Train_Loss: 4.628212928771973, Test_Loss: 4.691802024841309\n",
      "341:Epoch: 1, Train_Loss: 4.699524402618408, Test_Loss: 4.605859756469727 *\n",
      "342:Epoch: 1, Train_Loss: 4.76837158203125, Test_Loss: 4.628541946411133\n",
      "343:Epoch: 1, Train_Loss: 5.551803112030029, Test_Loss: 4.61414098739624 *\n",
      "344:Epoch: 1, Train_Loss: 4.920026779174805, Test_Loss: 4.605723857879639 *\n",
      "345:Epoch: 1, Train_Loss: 6.80570125579834, Test_Loss: 4.592170238494873 *\n",
      "346:Epoch: 1, Train_Loss: 5.097193717956543, Test_Loss: 4.702258110046387\n",
      "347:Epoch: 1, Train_Loss: 5.479841709136963, Test_Loss: 4.643707275390625 *\n",
      "348:Epoch: 1, Train_Loss: 4.603634834289551, Test_Loss: 4.678110122680664\n",
      "349:Epoch: 1, Train_Loss: 4.5793280601501465, Test_Loss: 4.666771411895752 *\n",
      "350:Epoch: 1, Train_Loss: 4.986445903778076, Test_Loss: 4.5980024337768555 *\n",
      "351:Epoch: 1, Train_Loss: 6.041774749755859, Test_Loss: 4.586855888366699 *\n",
      "352:Epoch: 1, Train_Loss: 4.885961532592773, Test_Loss: 4.580201148986816 *\n",
      "353:Epoch: 1, Train_Loss: 4.693768501281738, Test_Loss: 4.5711870193481445 *\n",
      "354:Epoch: 1, Train_Loss: 4.5608110427856445, Test_Loss: 4.567019462585449 *\n",
      "355:Epoch: 1, Train_Loss: 4.671305179595947, Test_Loss: 4.563077926635742 *\n",
      "356:Epoch: 1, Train_Loss: 4.906543254852295, Test_Loss: 4.556774616241455 *\n",
      "357:Epoch: 1, Train_Loss: 4.8679022789001465, Test_Loss: 4.549964904785156 *\n",
      "358:Epoch: 1, Train_Loss: 5.8715128898620605, Test_Loss: 4.559726238250732\n",
      "359:Epoch: 1, Train_Loss: 5.057126522064209, Test_Loss: 4.55518102645874 *\n",
      "360:Epoch: 1, Train_Loss: 4.537073135375977, Test_Loss: 4.549006462097168 *\n",
      "361:Epoch: 1, Train_Loss: 4.542014122009277, Test_Loss: 4.537866115570068 *\n",
      "362:Epoch: 1, Train_Loss: 4.547976970672607, Test_Loss: 4.5657572746276855\n",
      "363:Epoch: 1, Train_Loss: 4.555699825286865, Test_Loss: 4.580165863037109\n",
      "364:Epoch: 1, Train_Loss: 4.535093307495117, Test_Loss: 4.63280725479126\n",
      "365:Epoch: 1, Train_Loss: 4.537548065185547, Test_Loss: 5.097084045410156\n",
      "366:Epoch: 1, Train_Loss: 7.2765913009643555, Test_Loss: 4.940004348754883 *\n",
      "367:Epoch: 1, Train_Loss: 19.38221549987793, Test_Loss: 4.6270294189453125 *\n",
      "368:Epoch: 1, Train_Loss: 4.892859935760498, Test_Loss: 4.540011882781982 *\n",
      "369:Epoch: 1, Train_Loss: 7.739877700805664, Test_Loss: 4.5186591148376465 *\n",
      "370:Epoch: 1, Train_Loss: 5.115512371063232, Test_Loss: 4.569852828979492\n",
      "371:Epoch: 1, Train_Loss: 4.5474629402160645, Test_Loss: 5.019763946533203\n",
      "372:Epoch: 1, Train_Loss: 4.5496649742126465, Test_Loss: 5.9507880210876465\n",
      "373:Epoch: 1, Train_Loss: 14.352287292480469, Test_Loss: 5.049830436706543 *\n",
      "374:Epoch: 1, Train_Loss: 7.320507049560547, Test_Loss: 4.584255695343018 *\n",
      "375:Epoch: 1, Train_Loss: 4.507256507873535, Test_Loss: 4.492708683013916 *\n",
      "376:Epoch: 1, Train_Loss: 6.387027263641357, Test_Loss: 4.4913716316223145 *\n",
      "377:Epoch: 1, Train_Loss: 8.949623107910156, Test_Loss: 4.486800193786621 *\n",
      "378:Epoch: 1, Train_Loss: 4.499799728393555, Test_Loss: 4.483820915222168 *\n",
      "379:Epoch: 1, Train_Loss: 4.492383003234863, Test_Loss: 4.543088436126709\n",
      "380:Epoch: 1, Train_Loss: 4.475273609161377, Test_Loss: 4.575870037078857\n",
      "381:Epoch: 1, Train_Loss: 4.474894046783447, Test_Loss: 4.484912395477295 *\n",
      "382:Epoch: 1, Train_Loss: 4.47982120513916, Test_Loss: 4.5166144371032715\n",
      "383:Epoch: 1, Train_Loss: 4.474292278289795, Test_Loss: 4.649813175201416\n",
      "384:Epoch: 1, Train_Loss: 4.472680568695068, Test_Loss: 4.659763813018799\n",
      "385:Epoch: 1, Train_Loss: 4.474035739898682, Test_Loss: 4.587920665740967 *\n",
      "386:Epoch: 1, Train_Loss: 4.476189613342285, Test_Loss: 4.445220470428467 *\n",
      "387:Epoch: 1, Train_Loss: 4.458029747009277, Test_Loss: 4.441267490386963 *\n",
      "388:Epoch: 1, Train_Loss: 4.4600725173950195, Test_Loss: 4.438165664672852 *\n",
      "389:Epoch: 1, Train_Loss: 4.455671310424805, Test_Loss: 4.4348344802856445 *\n",
      "390:Epoch: 1, Train_Loss: 4.506178855895996, Test_Loss: 4.4356536865234375\n",
      "391:Epoch: 1, Train_Loss: 4.493605136871338, Test_Loss: 7.146058082580566\n",
      "392:Epoch: 1, Train_Loss: 4.443814754486084, Test_Loss: 7.579303741455078\n",
      "393:Epoch: 1, Train_Loss: 4.433984279632568, Test_Loss: 4.430144786834717 *\n",
      "394:Epoch: 1, Train_Loss: 4.424018383026123, Test_Loss: 4.422619342803955 *\n",
      "395:Epoch: 1, Train_Loss: 4.415102958679199, Test_Loss: 4.416335582733154 *\n",
      "396:Epoch: 1, Train_Loss: 4.412217140197754, Test_Loss: 4.413731098175049 *\n",
      "397:Epoch: 1, Train_Loss: 4.408164024353027, Test_Loss: 4.41268253326416 *\n",
      "398:Epoch: 1, Train_Loss: 4.406375885009766, Test_Loss: 4.413687229156494\n",
      "399:Epoch: 1, Train_Loss: 4.403587818145752, Test_Loss: 4.41048002243042 *\n",
      "400:Epoch: 1, Train_Loss: 4.3979878425598145, Test_Loss: 4.40945291519165 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 1\n",
      "401:Epoch: 1, Train_Loss: 4.395176887512207, Test_Loss: 4.406693935394287 *\n",
      "402:Epoch: 1, Train_Loss: 4.392714023590088, Test_Loss: 4.397791862487793 *\n",
      "403:Epoch: 1, Train_Loss: 4.39019775390625, Test_Loss: 4.398890972137451\n",
      "404:Epoch: 1, Train_Loss: 4.408814907073975, Test_Loss: 4.405972480773926\n",
      "405:Epoch: 1, Train_Loss: 4.397582054138184, Test_Loss: 4.4002814292907715 *\n",
      "406:Epoch: 1, Train_Loss: 4.3930463790893555, Test_Loss: 4.386746406555176 *\n",
      "407:Epoch: 1, Train_Loss: 4.379077911376953, Test_Loss: 4.378349781036377 *\n",
      "408:Epoch: 1, Train_Loss: 12.129389762878418, Test_Loss: 4.377445697784424 *\n",
      "409:Epoch: 1, Train_Loss: 6.055440902709961, Test_Loss: 4.372549057006836 *\n",
      "410:Epoch: 1, Train_Loss: 4.385010242462158, Test_Loss: 4.371383190155029 *\n",
      "411:Epoch: 1, Train_Loss: 4.3682074546813965, Test_Loss: 4.372653007507324\n",
      "412:Epoch: 1, Train_Loss: 4.366034984588623, Test_Loss: 4.3627448081970215 *\n",
      "413:Epoch: 1, Train_Loss: 4.390031337738037, Test_Loss: 4.361592769622803 *\n",
      "414:Epoch: 1, Train_Loss: 4.398982048034668, Test_Loss: 4.365046501159668\n",
      "415:Epoch: 1, Train_Loss: 4.356626987457275, Test_Loss: 4.356467247009277 *\n",
      "416:Epoch: 1, Train_Loss: 4.433671474456787, Test_Loss: 4.356341361999512 *\n",
      "417:Epoch: 1, Train_Loss: 4.564759731292725, Test_Loss: 4.35076379776001 *\n",
      "418:Epoch: 1, Train_Loss: 4.53939151763916, Test_Loss: 4.348099708557129 *\n",
      "419:Epoch: 1, Train_Loss: 4.374940395355225, Test_Loss: 4.346202373504639 *\n",
      "420:Epoch: 1, Train_Loss: 4.415956497192383, Test_Loss: 4.342413902282715 *\n",
      "421:Epoch: 1, Train_Loss: 4.448805332183838, Test_Loss: 4.389679431915283\n",
      "422:Epoch: 1, Train_Loss: 4.4586076736450195, Test_Loss: 4.354745864868164 *\n",
      "423:Epoch: 1, Train_Loss: 4.4772138595581055, Test_Loss: 9.309343338012695\n",
      "424:Epoch: 1, Train_Loss: 4.4437103271484375, Test_Loss: 5.162408351898193 *\n",
      "425:Epoch: 1, Train_Loss: 4.392459392547607, Test_Loss: 4.326997756958008 *\n",
      "426:Epoch: 1, Train_Loss: 4.323694705963135, Test_Loss: 4.329643726348877\n",
      "427:Epoch: 1, Train_Loss: 4.388823986053467, Test_Loss: 4.344442844390869\n",
      "428:Epoch: 1, Train_Loss: 4.334279537200928, Test_Loss: 4.352862358093262\n",
      "429:Epoch: 1, Train_Loss: 4.316171646118164, Test_Loss: 4.311999797821045 *\n",
      "430:Epoch: 1, Train_Loss: 4.305368900299072, Test_Loss: 4.4302215576171875\n",
      "431:Epoch: 1, Train_Loss: 4.302414894104004, Test_Loss: 4.416868686676025 *\n",
      "432:Epoch: 1, Train_Loss: 4.337013244628906, Test_Loss: 4.299689292907715 *\n",
      "433:Epoch: 1, Train_Loss: 9.876932144165039, Test_Loss: 4.370985507965088\n",
      "434:Epoch: 1, Train_Loss: 4.4510393142700195, Test_Loss: 4.294960021972656 *\n",
      "435:Epoch: 1, Train_Loss: 4.304512977600098, Test_Loss: 4.316287994384766\n",
      "436:Epoch: 1, Train_Loss: 4.303614616394043, Test_Loss: 4.2901201248168945 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437:Epoch: 1, Train_Loss: 4.305482864379883, Test_Loss: 4.379343032836914\n",
      "438:Epoch: 1, Train_Loss: 4.2987260818481445, Test_Loss: 4.3135480880737305 *\n",
      "439:Epoch: 1, Train_Loss: 4.282912731170654, Test_Loss: 4.465814590454102\n",
      "440:Epoch: 1, Train_Loss: 4.281124114990234, Test_Loss: 4.385811805725098 *\n",
      "441:Epoch: 1, Train_Loss: 4.2941813468933105, Test_Loss: 4.280174255371094 *\n",
      "442:Epoch: 1, Train_Loss: 4.292111396789551, Test_Loss: 4.2671966552734375 *\n",
      "443:Epoch: 1, Train_Loss: 4.270833492279053, Test_Loss: 4.264869213104248 *\n",
      "444:Epoch: 1, Train_Loss: 4.268433094024658, Test_Loss: 4.260959148406982 *\n",
      "445:Epoch: 1, Train_Loss: 4.265833854675293, Test_Loss: 4.257707118988037 *\n",
      "446:Epoch: 1, Train_Loss: 4.2813639640808105, Test_Loss: 4.254848003387451 *\n",
      "447:Epoch: 1, Train_Loss: 4.252103328704834, Test_Loss: 4.251359462738037 *\n",
      "448:Epoch: 1, Train_Loss: 4.251935005187988, Test_Loss: 4.247280597686768 *\n",
      "449:Epoch: 1, Train_Loss: 4.271884441375732, Test_Loss: 4.2525434494018555\n",
      "450:Epoch: 1, Train_Loss: 4.313112735748291, Test_Loss: 4.248860836029053 *\n",
      "451:Epoch: 1, Train_Loss: 4.2883429527282715, Test_Loss: 4.2426018714904785 *\n",
      "452:Epoch: 1, Train_Loss: 4.238027572631836, Test_Loss: 4.242938995361328\n",
      "453:Epoch: 1, Train_Loss: 4.237848281860352, Test_Loss: 4.27077054977417\n",
      "454:Epoch: 1, Train_Loss: 4.2823591232299805, Test_Loss: 4.251714706420898 *\n",
      "1:Epoch: 2, Train_Loss: 4.330963611602783, Test_Loss: 4.428747653961182 *\n",
      "2:Epoch: 2, Train_Loss: 4.307870864868164, Test_Loss: 4.734022617340088\n",
      "3:Epoch: 2, Train_Loss: 4.296741008758545, Test_Loss: 4.483860969543457 *\n",
      "4:Epoch: 2, Train_Loss: 4.239902973175049, Test_Loss: 4.29115104675293 *\n",
      "5:Epoch: 2, Train_Loss: 4.307061672210693, Test_Loss: 4.237285137176514 *\n",
      "6:Epoch: 2, Train_Loss: 4.289548873901367, Test_Loss: 4.224285125732422 *\n",
      "7:Epoch: 2, Train_Loss: 4.241992950439453, Test_Loss: 4.332214832305908\n",
      "8:Epoch: 2, Train_Loss: 4.286214351654053, Test_Loss: 5.011180400848389\n",
      "9:Epoch: 2, Train_Loss: 4.234499454498291, Test_Loss: 5.79303503036499\n",
      "10:Epoch: 2, Train_Loss: 4.214442253112793, Test_Loss: 4.55529260635376 *\n",
      "11:Epoch: 2, Train_Loss: 4.199160575866699, Test_Loss: 4.280360221862793 *\n",
      "12:Epoch: 2, Train_Loss: 4.194607734680176, Test_Loss: 4.1933369636535645 *\n",
      "13:Epoch: 2, Train_Loss: 4.191915988922119, Test_Loss: 4.200979232788086\n",
      "14:Epoch: 2, Train_Loss: 4.189195156097412, Test_Loss: 4.193972587585449 *\n",
      "15:Epoch: 2, Train_Loss: 4.185878276824951, Test_Loss: 4.196608066558838\n",
      "16:Epoch: 2, Train_Loss: 8.688396453857422, Test_Loss: 4.243119716644287\n",
      "17:Epoch: 2, Train_Loss: 4.708293437957764, Test_Loss: 4.2452216148376465\n",
      "18:Epoch: 2, Train_Loss: 4.182361125946045, Test_Loss: 4.1875481605529785 *\n",
      "19:Epoch: 2, Train_Loss: 4.2079997062683105, Test_Loss: 4.257595062255859\n",
      "20:Epoch: 2, Train_Loss: 4.175691604614258, Test_Loss: 4.514842987060547\n",
      "21:Epoch: 2, Train_Loss: 4.164065361022949, Test_Loss: 4.247354507446289 *\n",
      "22:Epoch: 2, Train_Loss: 4.164895057678223, Test_Loss: 4.324713706970215\n",
      "23:Epoch: 2, Train_Loss: 4.160427570343018, Test_Loss: 4.157393932342529 *\n",
      "24:Epoch: 2, Train_Loss: 4.1567606925964355, Test_Loss: 4.153955459594727 *\n",
      "25:Epoch: 2, Train_Loss: 4.152732849121094, Test_Loss: 4.15093469619751 *\n",
      "26:Epoch: 2, Train_Loss: 4.224215507507324, Test_Loss: 4.1483869552612305 *\n",
      "27:Epoch: 2, Train_Loss: 4.269649028778076, Test_Loss: 4.157595634460449\n",
      "28:Epoch: 2, Train_Loss: 4.277294158935547, Test_Loss: 8.189104080200195\n",
      "29:Epoch: 2, Train_Loss: 4.268296718597412, Test_Loss: 5.7395405769348145 *\n",
      "30:Epoch: 2, Train_Loss: 4.140380859375, Test_Loss: 4.142302989959717 *\n",
      "31:Epoch: 2, Train_Loss: 4.17711067199707, Test_Loss: 4.134035110473633 *\n",
      "32:Epoch: 2, Train_Loss: 4.307404041290283, Test_Loss: 4.130484104156494 *\n",
      "33:Epoch: 2, Train_Loss: 4.2957329750061035, Test_Loss: 4.13114070892334\n",
      "34:Epoch: 2, Train_Loss: 4.307278633117676, Test_Loss: 4.126063823699951 *\n",
      "35:Epoch: 2, Train_Loss: 4.132715702056885, Test_Loss: 4.127272605895996\n",
      "36:Epoch: 2, Train_Loss: 4.118075370788574, Test_Loss: 4.120849132537842 *\n",
      "37:Epoch: 2, Train_Loss: 4.118690490722656, Test_Loss: 4.121549606323242\n",
      "38:Epoch: 2, Train_Loss: 4.1120171546936035, Test_Loss: 4.120173454284668 *\n",
      "39:Epoch: 2, Train_Loss: 4.110665321350098, Test_Loss: 4.115967273712158 *\n",
      "40:Epoch: 2, Train_Loss: 4.106853485107422, Test_Loss: 4.113959789276123 *\n",
      "41:Epoch: 2, Train_Loss: 4.104806423187256, Test_Loss: 4.126325607299805\n",
      "42:Epoch: 2, Train_Loss: 4.104826927185059, Test_Loss: 4.114992141723633 *\n",
      "43:Epoch: 2, Train_Loss: 4.098851203918457, Test_Loss: 4.1030592918396 *\n",
      "44:Epoch: 2, Train_Loss: 4.096518039703369, Test_Loss: 4.095195293426514 *\n",
      "45:Epoch: 2, Train_Loss: 4.204677104949951, Test_Loss: 4.09545373916626\n",
      "46:Epoch: 2, Train_Loss: 4.245329856872559, Test_Loss: 4.090925216674805 *\n",
      "47:Epoch: 2, Train_Loss: 4.254568099975586, Test_Loss: 4.087386608123779 *\n",
      "48:Epoch: 2, Train_Loss: 4.136334419250488, Test_Loss: 4.089244842529297\n",
      "49:Epoch: 2, Train_Loss: 4.256398677825928, Test_Loss: 4.080831050872803 *\n",
      "50:Epoch: 2, Train_Loss: 4.249945163726807, Test_Loss: 4.07969331741333 *\n",
      "51:Epoch: 2, Train_Loss: 4.105493545532227, Test_Loss: 4.081680774688721\n",
      "52:Epoch: 2, Train_Loss: 4.256192207336426, Test_Loss: 4.075157165527344 *\n",
      "53:Epoch: 2, Train_Loss: 4.231649398803711, Test_Loss: 4.073155879974365 *\n",
      "54:Epoch: 2, Train_Loss: 4.306026458740234, Test_Loss: 4.068297386169434 *\n",
      "55:Epoch: 2, Train_Loss: 4.087242603302002, Test_Loss: 4.0667524337768555 *\n",
      "56:Epoch: 2, Train_Loss: 5.536580562591553, Test_Loss: 4.063421726226807 *\n",
      "57:Epoch: 2, Train_Loss: 5.935000419616699, Test_Loss: 4.062397003173828 *\n",
      "58:Epoch: 2, Train_Loss: 4.088468074798584, Test_Loss: 4.1087446212768555\n",
      "59:Epoch: 2, Train_Loss: 4.086206436157227, Test_Loss: 4.109536647796631\n",
      "60:Epoch: 2, Train_Loss: 4.086399078369141, Test_Loss: 9.64553451538086\n",
      "61:Epoch: 2, Train_Loss: 4.078088760375977, Test_Loss: 4.13023567199707 *\n",
      "62:Epoch: 2, Train_Loss: 4.04413366317749, Test_Loss: 4.04606294631958 *\n",
      "63:Epoch: 2, Train_Loss: 4.06076717376709, Test_Loss: 4.056825160980225\n",
      "64:Epoch: 2, Train_Loss: 4.202445030212402, Test_Loss: 4.079326629638672\n",
      "65:Epoch: 2, Train_Loss: 4.161391735076904, Test_Loss: 4.079822540283203\n",
      "66:Epoch: 2, Train_Loss: 4.160219192504883, Test_Loss: 4.033712387084961 *\n",
      "67:Epoch: 2, Train_Loss: 4.150465965270996, Test_Loss: 4.162426948547363\n",
      "68:Epoch: 2, Train_Loss: 4.120766639709473, Test_Loss: 4.097536087036133 *\n",
      "69:Epoch: 2, Train_Loss: 4.070374965667725, Test_Loss: 4.021623134613037 *\n",
      "70:Epoch: 2, Train_Loss: 4.052491188049316, Test_Loss: 4.081164360046387\n",
      "71:Epoch: 2, Train_Loss: 4.026569843292236, Test_Loss: 4.0222320556640625 *\n",
      "72:Epoch: 2, Train_Loss: 4.0256547927856445, Test_Loss: 4.031651020050049\n",
      "73:Epoch: 2, Train_Loss: 4.014739990234375, Test_Loss: 4.031586170196533 *\n",
      "74:Epoch: 2, Train_Loss: 4.008706092834473, Test_Loss: 4.124059677124023\n",
      "75:Epoch: 2, Train_Loss: 4.071991443634033, Test_Loss: 4.048836708068848 *\n",
      "76:Epoch: 2, Train_Loss: 4.081779479980469, Test_Loss: 4.142317771911621\n",
      "77:Epoch: 2, Train_Loss: 4.028684616088867, Test_Loss: 4.063016891479492 *\n",
      "78:Epoch: 2, Train_Loss: 3.9956884384155273, Test_Loss: 4.020754814147949 *\n",
      "79:Epoch: 2, Train_Loss: 3.9927000999450684, Test_Loss: 4.00501012802124 *\n",
      "80:Epoch: 2, Train_Loss: 3.98944091796875, Test_Loss: 3.9973061084747314 *\n",
      "81:Epoch: 2, Train_Loss: 3.985902786254883, Test_Loss: 3.9956138134002686 *\n",
      "82:Epoch: 2, Train_Loss: 3.9848690032958984, Test_Loss: 3.992309331893921 *\n",
      "83:Epoch: 2, Train_Loss: 3.9796786308288574, Test_Loss: 3.9905073642730713 *\n",
      "84:Epoch: 2, Train_Loss: 3.9772584438323975, Test_Loss: 3.9861228466033936 *\n",
      "85:Epoch: 2, Train_Loss: 3.975316286087036, Test_Loss: 3.98576021194458 *\n",
      "86:Epoch: 2, Train_Loss: 3.9713573455810547, Test_Loss: 3.9891510009765625\n",
      "87:Epoch: 2, Train_Loss: 3.9686057567596436, Test_Loss: 3.9959754943847656\n",
      "88:Epoch: 2, Train_Loss: 3.971381187438965, Test_Loss: 3.972606658935547 *\n",
      "89:Epoch: 2, Train_Loss: 3.97019624710083, Test_Loss: 3.9778552055358887\n",
      "90:Epoch: 2, Train_Loss: 3.9640116691589355, Test_Loss: 4.033608913421631\n",
      "91:Epoch: 2, Train_Loss: 3.9733173847198486, Test_Loss: 3.9815382957458496 *\n",
      "92:Epoch: 2, Train_Loss: 3.9580459594726562, Test_Loss: 4.262143611907959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93:Epoch: 2, Train_Loss: 3.9530513286590576, Test_Loss: 4.552103042602539\n",
      "94:Epoch: 2, Train_Loss: 3.948925018310547, Test_Loss: 4.193178653717041 *\n",
      "95:Epoch: 2, Train_Loss: 3.9463653564453125, Test_Loss: 4.020932197570801 *\n",
      "96:Epoch: 2, Train_Loss: 3.958345413208008, Test_Loss: 3.9827487468719482 *\n",
      "97:Epoch: 2, Train_Loss: 3.94563889503479, Test_Loss: 3.9545555114746094 *\n",
      "98:Epoch: 2, Train_Loss: 3.9383060932159424, Test_Loss: 4.056698322296143\n",
      "99:Epoch: 2, Train_Loss: 3.9355697631835938, Test_Loss: 4.854087829589844\n",
      "100:Epoch: 2, Train_Loss: 3.947550058364868, Test_Loss: 5.3762078285217285\n",
      "Model saved at location ../Saver/model.ckpt at epoch 2\n",
      "101:Epoch: 2, Train_Loss: 4.0188422203063965, Test_Loss: 4.0666728019714355 *\n",
      "102:Epoch: 2, Train_Loss: 3.9566969871520996, Test_Loss: 4.029537200927734 *\n",
      "103:Epoch: 2, Train_Loss: 3.956146240234375, Test_Loss: 3.9243500232696533 *\n",
      "104:Epoch: 2, Train_Loss: 3.9211859703063965, Test_Loss: 3.930579662322998\n",
      "105:Epoch: 2, Train_Loss: 3.955186367034912, Test_Loss: 3.924839496612549 *\n",
      "106:Epoch: 2, Train_Loss: 3.9339957237243652, Test_Loss: 3.932309865951538\n",
      "107:Epoch: 2, Train_Loss: 3.9159631729125977, Test_Loss: 3.9630661010742188\n",
      "108:Epoch: 2, Train_Loss: 3.9427008628845215, Test_Loss: 3.95544171333313 *\n",
      "109:Epoch: 2, Train_Loss: 3.9388861656188965, Test_Loss: 3.9167914390563965 *\n",
      "110:Epoch: 2, Train_Loss: 4.019042491912842, Test_Loss: 4.0109758377075195\n",
      "111:Epoch: 2, Train_Loss: 4.005256175994873, Test_Loss: 4.2995476722717285\n",
      "112:Epoch: 2, Train_Loss: 3.9758760929107666, Test_Loss: 3.981889247894287 *\n",
      "113:Epoch: 2, Train_Loss: 3.927061080932617, Test_Loss: 4.043024063110352\n",
      "114:Epoch: 2, Train_Loss: 3.8969526290893555, Test_Loss: 3.8972713947296143 *\n",
      "115:Epoch: 2, Train_Loss: 3.920012950897217, Test_Loss: 3.8943958282470703 *\n",
      "116:Epoch: 2, Train_Loss: 3.888882875442505, Test_Loss: 3.8916783332824707 *\n",
      "117:Epoch: 2, Train_Loss: 3.8943467140197754, Test_Loss: 3.890639543533325 *\n",
      "118:Epoch: 2, Train_Loss: 3.8966481685638428, Test_Loss: 3.9024147987365723\n",
      "119:Epoch: 2, Train_Loss: 3.8976449966430664, Test_Loss: 8.962284088134766\n",
      "120:Epoch: 2, Train_Loss: 3.9758777618408203, Test_Loss: 4.347299098968506 *\n",
      "121:Epoch: 2, Train_Loss: 3.878448009490967, Test_Loss: 3.8829264640808105 *\n",
      "122:Epoch: 2, Train_Loss: 3.958238363265991, Test_Loss: 3.873030185699463 *\n",
      "123:Epoch: 2, Train_Loss: 3.8774960041046143, Test_Loss: 3.8728811740875244 *\n",
      "124:Epoch: 2, Train_Loss: 3.8999099731445312, Test_Loss: 3.8729655742645264\n",
      "125:Epoch: 2, Train_Loss: 3.8962182998657227, Test_Loss: 3.866067409515381 *\n",
      "126:Epoch: 2, Train_Loss: 4.145033836364746, Test_Loss: 3.8652560710906982 *\n",
      "127:Epoch: 2, Train_Loss: 3.8719494342803955, Test_Loss: 3.8609180450439453 *\n",
      "128:Epoch: 2, Train_Loss: 3.884941816329956, Test_Loss: 3.85862398147583 *\n",
      "129:Epoch: 2, Train_Loss: 3.8534927368164062, Test_Loss: 3.8584773540496826 *\n",
      "130:Epoch: 2, Train_Loss: 3.851621150970459, Test_Loss: 3.8572885990142822 *\n",
      "131:Epoch: 2, Train_Loss: 3.849735975265503, Test_Loss: 3.855752944946289 *\n",
      "132:Epoch: 2, Train_Loss: 3.845698833465576, Test_Loss: 3.8744943141937256\n",
      "133:Epoch: 2, Train_Loss: 3.859539270401001, Test_Loss: 3.8582990169525146 *\n",
      "134:Epoch: 2, Train_Loss: 3.862074136734009, Test_Loss: 3.8418848514556885 *\n",
      "135:Epoch: 2, Train_Loss: 3.868833541870117, Test_Loss: 3.83652400970459 *\n",
      "136:Epoch: 2, Train_Loss: 3.8584325313568115, Test_Loss: 3.835653066635132 *\n",
      "137:Epoch: 2, Train_Loss: 3.8537662029266357, Test_Loss: 3.8316409587860107 *\n",
      "138:Epoch: 2, Train_Loss: 3.847895622253418, Test_Loss: 3.8278439044952393 *\n",
      "139:Epoch: 2, Train_Loss: 3.8294568061828613, Test_Loss: 3.8284764289855957\n",
      "140:Epoch: 2, Train_Loss: 3.8223743438720703, Test_Loss: 3.8227856159210205 *\n",
      "141:Epoch: 2, Train_Loss: 3.8350491523742676, Test_Loss: 3.8215129375457764 *\n",
      "142:Epoch: 2, Train_Loss: 3.8379669189453125, Test_Loss: 3.8201181888580322 *\n",
      "143:Epoch: 2, Train_Loss: 3.846055030822754, Test_Loss: 3.8168063163757324 *\n",
      "144:Epoch: 2, Train_Loss: 3.8118269443511963, Test_Loss: 3.8129000663757324 *\n",
      "145:Epoch: 2, Train_Loss: 3.8589494228363037, Test_Loss: 3.8111984729766846 *\n",
      "146:Epoch: 2, Train_Loss: 3.871346950531006, Test_Loss: 3.8079235553741455 *\n",
      "147:Epoch: 2, Train_Loss: 3.858391523361206, Test_Loss: 3.805704355239868 *\n",
      "148:Epoch: 2, Train_Loss: 3.8011081218719482, Test_Loss: 3.8071157932281494\n",
      "149:Epoch: 2, Train_Loss: 3.8310866355895996, Test_Loss: 3.8594703674316406\n",
      "150:Epoch: 2, Train_Loss: 3.7977747917175293, Test_Loss: 4.494184494018555\n",
      "151:Epoch: 2, Train_Loss: 3.8170371055603027, Test_Loss: 8.673528671264648\n",
      "152:Epoch: 2, Train_Loss: 3.7925729751586914, Test_Loss: 3.8008840084075928 *\n",
      "153:Epoch: 2, Train_Loss: 3.8132851123809814, Test_Loss: 3.7894694805145264 *\n",
      "154:Epoch: 2, Train_Loss: 3.9131150245666504, Test_Loss: 3.819514751434326\n",
      "155:Epoch: 2, Train_Loss: 7.225334167480469, Test_Loss: 3.8370201587677\n",
      "156:Epoch: 2, Train_Loss: 5.712307929992676, Test_Loss: 3.836793899536133 *\n",
      "157:Epoch: 2, Train_Loss: 3.798656702041626, Test_Loss: 3.7870497703552246 *\n",
      "158:Epoch: 2, Train_Loss: 3.7770493030548096, Test_Loss: 3.9112846851348877\n",
      "159:Epoch: 2, Train_Loss: 3.9276938438415527, Test_Loss: 3.805760145187378 *\n",
      "160:Epoch: 2, Train_Loss: 3.8901424407958984, Test_Loss: 3.772289514541626 *\n",
      "161:Epoch: 2, Train_Loss: 3.7920427322387695, Test_Loss: 3.8156332969665527\n",
      "162:Epoch: 2, Train_Loss: 3.765728235244751, Test_Loss: 3.774658441543579 *\n",
      "163:Epoch: 2, Train_Loss: 3.8264307975769043, Test_Loss: 3.7756083011627197\n",
      "164:Epoch: 2, Train_Loss: 3.7909812927246094, Test_Loss: 3.810018301010132\n",
      "165:Epoch: 2, Train_Loss: 3.7732036113739014, Test_Loss: 3.875120162963867\n",
      "166:Epoch: 2, Train_Loss: 3.923229694366455, Test_Loss: 3.817739486694336 *\n",
      "167:Epoch: 2, Train_Loss: 5.116857528686523, Test_Loss: 3.875872850418091\n",
      "168:Epoch: 2, Train_Loss: 5.144448280334473, Test_Loss: 3.782980442047119 *\n",
      "169:Epoch: 2, Train_Loss: 3.8743293285369873, Test_Loss: 3.791811466217041\n",
      "170:Epoch: 2, Train_Loss: 3.8007378578186035, Test_Loss: 3.775009870529175 *\n",
      "171:Epoch: 2, Train_Loss: 5.840603351593018, Test_Loss: 3.765998125076294 *\n",
      "172:Epoch: 2, Train_Loss: 5.258823871612549, Test_Loss: 3.76218581199646 *\n",
      "173:Epoch: 2, Train_Loss: 3.7906901836395264, Test_Loss: 3.7566890716552734 *\n",
      "174:Epoch: 2, Train_Loss: 3.775092601776123, Test_Loss: 3.753161907196045 *\n",
      "175:Epoch: 2, Train_Loss: 4.139346122741699, Test_Loss: 3.74823260307312 *\n",
      "176:Epoch: 2, Train_Loss: 5.492914199829102, Test_Loss: 3.7565572261810303\n",
      "177:Epoch: 2, Train_Loss: 4.81296443939209, Test_Loss: 3.7522051334381104 *\n",
      "178:Epoch: 2, Train_Loss: 3.7349512577056885, Test_Loss: 3.7668347358703613\n",
      "179:Epoch: 2, Train_Loss: 3.740224599838257, Test_Loss: 3.7260215282440186 *\n",
      "180:Epoch: 2, Train_Loss: 3.9106032848358154, Test_Loss: 3.753278970718384\n",
      "181:Epoch: 2, Train_Loss: 4.319880485534668, Test_Loss: 3.8261237144470215\n",
      "182:Epoch: 2, Train_Loss: 3.732140064239502, Test_Loss: 3.7331676483154297 *\n",
      "183:Epoch: 2, Train_Loss: 3.7722156047821045, Test_Loss: 4.181014060974121\n",
      "184:Epoch: 2, Train_Loss: 3.7912187576293945, Test_Loss: 4.3791656494140625\n",
      "185:Epoch: 2, Train_Loss: 3.894167184829712, Test_Loss: 3.9537651538848877 *\n",
      "186:Epoch: 2, Train_Loss: 3.8310694694519043, Test_Loss: 3.7671241760253906 *\n",
      "187:Epoch: 2, Train_Loss: 4.045409202575684, Test_Loss: 3.7469642162323 *\n",
      "188:Epoch: 2, Train_Loss: 3.874871253967285, Test_Loss: 3.7060365676879883 *\n",
      "189:Epoch: 2, Train_Loss: 3.7482171058654785, Test_Loss: 3.809720754623413\n",
      "190:Epoch: 2, Train_Loss: 3.9443438053131104, Test_Loss: 4.752371788024902\n",
      "191:Epoch: 2, Train_Loss: 4.008028030395508, Test_Loss: 4.8531646728515625\n",
      "192:Epoch: 2, Train_Loss: 4.246936798095703, Test_Loss: 3.742852210998535 *\n",
      "193:Epoch: 2, Train_Loss: 3.9954402446746826, Test_Loss: 3.7921559810638428\n",
      "194:Epoch: 2, Train_Loss: 3.7357568740844727, Test_Loss: 3.683365821838379 *\n",
      "195:Epoch: 2, Train_Loss: 3.8042306900024414, Test_Loss: 3.6833033561706543 *\n",
      "196:Epoch: 2, Train_Loss: 3.777031660079956, Test_Loss: 3.6855006217956543\n",
      "197:Epoch: 2, Train_Loss: 3.6866614818573, Test_Loss: 3.6900482177734375\n",
      "198:Epoch: 2, Train_Loss: 3.6785385608673096, Test_Loss: 3.716186761856079\n",
      "199:Epoch: 2, Train_Loss: 3.6689345836639404, Test_Loss: 3.688570261001587 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200:Epoch: 2, Train_Loss: 3.6699724197387695, Test_Loss: 3.674901247024536 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 2\n",
      "201:Epoch: 2, Train_Loss: 3.667337417602539, Test_Loss: 3.78981351852417\n",
      "202:Epoch: 2, Train_Loss: 3.674225091934204, Test_Loss: 4.080153942108154\n",
      "203:Epoch: 2, Train_Loss: 3.745617151260376, Test_Loss: 3.8589024543762207 *\n",
      "204:Epoch: 2, Train_Loss: 3.7376577854156494, Test_Loss: 3.718104839324951 *\n",
      "205:Epoch: 2, Train_Loss: 3.7649714946746826, Test_Loss: 3.6678802967071533 *\n",
      "206:Epoch: 2, Train_Loss: 3.8437986373901367, Test_Loss: 3.6655027866363525 *\n",
      "207:Epoch: 2, Train_Loss: 4.0595011711120605, Test_Loss: 3.663010358810425 *\n",
      "208:Epoch: 2, Train_Loss: 3.662140369415283, Test_Loss: 3.661262273788452 *\n",
      "209:Epoch: 2, Train_Loss: 3.702359914779663, Test_Loss: 3.7895548343658447\n",
      "210:Epoch: 2, Train_Loss: 3.9563043117523193, Test_Loss: 8.840782165527344\n",
      "211:Epoch: 2, Train_Loss: 4.239605903625488, Test_Loss: 3.7529783248901367 *\n",
      "212:Epoch: 2, Train_Loss: 3.710692882537842, Test_Loss: 3.6469061374664307 *\n",
      "213:Epoch: 2, Train_Loss: 3.6353065967559814, Test_Loss: 3.638946533203125 *\n",
      "214:Epoch: 2, Train_Loss: 4.117823600769043, Test_Loss: 3.643245220184326\n",
      "215:Epoch: 2, Train_Loss: 4.375931739807129, Test_Loss: 3.6383931636810303 *\n",
      "216:Epoch: 2, Train_Loss: 3.9622607231140137, Test_Loss: 3.63206148147583 *\n",
      "217:Epoch: 2, Train_Loss: 3.664862632751465, Test_Loss: 3.6278953552246094 *\n",
      "218:Epoch: 2, Train_Loss: 3.6448559761047363, Test_Loss: 3.6247966289520264 *\n",
      "219:Epoch: 2, Train_Loss: 3.8907976150512695, Test_Loss: 3.6223037242889404 *\n",
      "220:Epoch: 2, Train_Loss: 5.280464172363281, Test_Loss: 3.6201605796813965 *\n",
      "221:Epoch: 2, Train_Loss: 4.062632083892822, Test_Loss: 3.619954824447632 *\n",
      "222:Epoch: 2, Train_Loss: 3.6281988620758057, Test_Loss: 3.6339597702026367\n",
      "223:Epoch: 2, Train_Loss: 3.6230714321136475, Test_Loss: 3.6432204246520996\n",
      "224:Epoch: 2, Train_Loss: 3.61116886138916, Test_Loss: 3.6252808570861816 *\n",
      "225:Epoch: 2, Train_Loss: 3.963488817214966, Test_Loss: 3.606175422668457 *\n",
      "226:Epoch: 2, Train_Loss: 3.73934268951416, Test_Loss: 3.6034343242645264 *\n",
      "227:Epoch: 2, Train_Loss: 3.654898166656494, Test_Loss: 3.601223945617676 *\n",
      "228:Epoch: 2, Train_Loss: 3.9176292419433594, Test_Loss: 3.5991415977478027 *\n",
      "229:Epoch: 2, Train_Loss: 3.604604721069336, Test_Loss: 3.5956063270568848 *\n",
      "230:Epoch: 2, Train_Loss: 3.674182891845703, Test_Loss: 3.595197916030884 *\n",
      "231:Epoch: 2, Train_Loss: 3.689490795135498, Test_Loss: 3.5909345149993896 *\n",
      "232:Epoch: 2, Train_Loss: 3.9918248653411865, Test_Loss: 3.5897936820983887 *\n",
      "233:Epoch: 2, Train_Loss: 3.6452083587646484, Test_Loss: 3.587139844894409 *\n",
      "234:Epoch: 2, Train_Loss: 3.686324119567871, Test_Loss: 3.585094690322876 *\n",
      "235:Epoch: 2, Train_Loss: 3.8350670337677, Test_Loss: 3.581930160522461 *\n",
      "236:Epoch: 2, Train_Loss: 3.794982433319092, Test_Loss: 3.5798351764678955 *\n",
      "237:Epoch: 2, Train_Loss: 3.6020050048828125, Test_Loss: 3.5767548084259033 *\n",
      "238:Epoch: 2, Train_Loss: 3.5990376472473145, Test_Loss: 3.5746870040893555 *\n",
      "239:Epoch: 2, Train_Loss: 3.585789680480957, Test_Loss: 3.577449083328247\n",
      "240:Epoch: 2, Train_Loss: 3.623746871948242, Test_Loss: 3.6303460597991943\n",
      "241:Epoch: 2, Train_Loss: 3.720944881439209, Test_Loss: 5.433590412139893\n",
      "242:Epoch: 2, Train_Loss: 4.319646835327148, Test_Loss: 7.12883186340332\n",
      "243:Epoch: 2, Train_Loss: 3.837752103805542, Test_Loss: 3.568157911300659 *\n",
      "244:Epoch: 2, Train_Loss: 4.484308242797852, Test_Loss: 3.560086250305176 *\n",
      "245:Epoch: 2, Train_Loss: 3.972482442855835, Test_Loss: 3.6118955612182617\n",
      "246:Epoch: 2, Train_Loss: 4.025214672088623, Test_Loss: 3.6057991981506348 *\n",
      "247:Epoch: 2, Train_Loss: 3.879363775253296, Test_Loss: 3.61130690574646\n",
      "248:Epoch: 2, Train_Loss: 3.5641982555389404, Test_Loss: 3.5836353302001953 *\n",
      "249:Epoch: 2, Train_Loss: 3.56558895111084, Test_Loss: 3.6695683002471924\n",
      "250:Epoch: 2, Train_Loss: 3.6025750637054443, Test_Loss: 3.5530197620391846 *\n",
      "251:Epoch: 2, Train_Loss: 3.7062103748321533, Test_Loss: 3.5590784549713135\n",
      "252:Epoch: 2, Train_Loss: 4.406654357910156, Test_Loss: 3.5677216053009033\n",
      "253:Epoch: 2, Train_Loss: 3.736907482147217, Test_Loss: 3.5562334060668945 *\n",
      "254:Epoch: 2, Train_Loss: 5.621098518371582, Test_Loss: 3.5457077026367188 *\n",
      "255:Epoch: 2, Train_Loss: 4.125011920928955, Test_Loss: 3.6107707023620605\n",
      "256:Epoch: 2, Train_Loss: 4.586599826812744, Test_Loss: 3.6222496032714844\n",
      "257:Epoch: 2, Train_Loss: 3.6504714488983154, Test_Loss: 3.609398603439331 *\n",
      "258:Epoch: 2, Train_Loss: 3.531296730041504, Test_Loss: 3.6367130279541016\n",
      "259:Epoch: 2, Train_Loss: 3.6999800205230713, Test_Loss: 3.544686794281006 *\n",
      "260:Epoch: 2, Train_Loss: 4.814554691314697, Test_Loss: 3.5571796894073486\n",
      "261:Epoch: 2, Train_Loss: 4.249477386474609, Test_Loss: 3.5355570316314697 *\n",
      "262:Epoch: 2, Train_Loss: 3.618574380874634, Test_Loss: 3.5270721912384033 *\n",
      "263:Epoch: 2, Train_Loss: 3.54569673538208, Test_Loss: 3.52465558052063 *\n",
      "264:Epoch: 2, Train_Loss: 3.5947065353393555, Test_Loss: 3.5246036052703857 *\n",
      "265:Epoch: 2, Train_Loss: 3.9094014167785645, Test_Loss: 3.520442008972168 *\n",
      "266:Epoch: 2, Train_Loss: 3.7032124996185303, Test_Loss: 3.5150041580200195 *\n",
      "267:Epoch: 2, Train_Loss: 4.60882568359375, Test_Loss: 3.521714210510254\n",
      "268:Epoch: 2, Train_Loss: 4.244431018829346, Test_Loss: 3.5160953998565674 *\n",
      "269:Epoch: 2, Train_Loss: 3.5685317516326904, Test_Loss: 3.519184112548828\n",
      "270:Epoch: 2, Train_Loss: 3.506925582885742, Test_Loss: 3.5030148029327393 *\n",
      "271:Epoch: 2, Train_Loss: 3.5108160972595215, Test_Loss: 3.522005081176758\n",
      "272:Epoch: 2, Train_Loss: 3.513082981109619, Test_Loss: 3.565580129623413\n",
      "273:Epoch: 2, Train_Loss: 3.5161619186401367, Test_Loss: 3.504523992538452 *\n",
      "274:Epoch: 2, Train_Loss: 3.5054614543914795, Test_Loss: 3.9948346614837646\n",
      "275:Epoch: 2, Train_Loss: 3.499753952026367, Test_Loss: 4.051316738128662\n",
      "276:Epoch: 2, Train_Loss: 21.056692123413086, Test_Loss: 3.6622965335845947 *\n",
      "277:Epoch: 2, Train_Loss: 3.4907290935516357, Test_Loss: 3.519745349884033 *\n",
      "278:Epoch: 2, Train_Loss: 6.284878730773926, Test_Loss: 3.506190299987793 *\n",
      "279:Epoch: 2, Train_Loss: 4.880858898162842, Test_Loss: 3.5053141117095947 *\n",
      "280:Epoch: 2, Train_Loss: 3.483752965927124, Test_Loss: 3.689051628112793\n",
      "281:Epoch: 2, Train_Loss: 3.5288548469543457, Test_Loss: 4.754998207092285\n",
      "282:Epoch: 2, Train_Loss: 10.098546028137207, Test_Loss: 4.367393970489502 *\n",
      "283:Epoch: 2, Train_Loss: 9.458061218261719, Test_Loss: 3.5261123180389404 *\n",
      "284:Epoch: 2, Train_Loss: 3.495149850845337, Test_Loss: 3.5176658630371094 *\n",
      "285:Epoch: 2, Train_Loss: 3.6120853424072266, Test_Loss: 3.4730732440948486 *\n",
      "286:Epoch: 2, Train_Loss: 9.682313919067383, Test_Loss: 3.4822871685028076\n",
      "287:Epoch: 2, Train_Loss: 3.4792978763580322, Test_Loss: 3.4795122146606445 *\n",
      "288:Epoch: 2, Train_Loss: 3.489412784576416, Test_Loss: 3.522372007369995\n",
      "289:Epoch: 2, Train_Loss: 3.4853696823120117, Test_Loss: 3.602424144744873\n",
      "290:Epoch: 2, Train_Loss: 3.4785947799682617, Test_Loss: 3.4798707962036133 *\n",
      "291:Epoch: 2, Train_Loss: 3.481657028198242, Test_Loss: 3.4904332160949707\n",
      "292:Epoch: 2, Train_Loss: 3.4759764671325684, Test_Loss: 3.5603296756744385\n",
      "293:Epoch: 2, Train_Loss: 3.4704253673553467, Test_Loss: 3.748354911804199\n",
      "294:Epoch: 2, Train_Loss: 3.466906785964966, Test_Loss: 3.5916755199432373 *\n",
      "295:Epoch: 2, Train_Loss: 3.4653918743133545, Test_Loss: 3.4494540691375732 *\n",
      "296:Epoch: 2, Train_Loss: 3.4583327770233154, Test_Loss: 3.441411018371582 *\n",
      "297:Epoch: 2, Train_Loss: 3.4572231769561768, Test_Loss: 3.439276695251465 *\n",
      "298:Epoch: 2, Train_Loss: 3.4474780559539795, Test_Loss: 3.4368996620178223 *\n",
      "299:Epoch: 2, Train_Loss: 3.4943928718566895, Test_Loss: 3.435952663421631 *\n",
      "300:Epoch: 2, Train_Loss: 3.493406295776367, Test_Loss: 4.175316333770752\n",
      "Model saved at location ../Saver/model.ckpt at epoch 2\n",
      "301:Epoch: 2, Train_Loss: 3.446554183959961, Test_Loss: 8.413558959960938\n",
      "302:Epoch: 2, Train_Loss: 3.4382548332214355, Test_Loss: 3.4713480472564697 *\n",
      "303:Epoch: 2, Train_Loss: 3.430720090866089, Test_Loss: 3.4319443702697754 *\n",
      "304:Epoch: 2, Train_Loss: 3.424142599105835, Test_Loss: 3.425807237625122 *\n",
      "305:Epoch: 2, Train_Loss: 3.4221036434173584, Test_Loss: 3.423476219177246 *\n",
      "306:Epoch: 2, Train_Loss: 3.4198567867279053, Test_Loss: 3.424508810043335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307:Epoch: 2, Train_Loss: 3.418048620223999, Test_Loss: 3.421013355255127 *\n",
      "308:Epoch: 2, Train_Loss: 3.417748212814331, Test_Loss: 3.4279730319976807\n",
      "309:Epoch: 2, Train_Loss: 3.4129953384399414, Test_Loss: 3.42258358001709 *\n",
      "310:Epoch: 2, Train_Loss: 3.410890817642212, Test_Loss: 3.4204344749450684 *\n",
      "311:Epoch: 2, Train_Loss: 3.409148931503296, Test_Loss: 3.4159061908721924 *\n",
      "312:Epoch: 2, Train_Loss: 3.4080660343170166, Test_Loss: 3.4176220893859863\n",
      "313:Epoch: 2, Train_Loss: 3.4220712184906006, Test_Loss: 3.4201927185058594\n",
      "314:Epoch: 2, Train_Loss: 3.4283647537231445, Test_Loss: 3.419828414916992 *\n",
      "315:Epoch: 2, Train_Loss: 3.4131484031677246, Test_Loss: 3.4124817848205566 *\n",
      "316:Epoch: 2, Train_Loss: 3.405367612838745, Test_Loss: 3.403529167175293 *\n",
      "317:Epoch: 2, Train_Loss: 7.033933639526367, Test_Loss: 3.400850772857666 *\n",
      "318:Epoch: 2, Train_Loss: 9.151585578918457, Test_Loss: 3.3993613719940186 *\n",
      "319:Epoch: 2, Train_Loss: 3.420506715774536, Test_Loss: 3.3988986015319824 *\n",
      "320:Epoch: 2, Train_Loss: 3.396005392074585, Test_Loss: 3.396939516067505 *\n",
      "321:Epoch: 2, Train_Loss: 3.3973634243011475, Test_Loss: 3.397021532058716\n",
      "322:Epoch: 2, Train_Loss: 3.410281181335449, Test_Loss: 3.392859697341919 *\n",
      "323:Epoch: 2, Train_Loss: 3.4201560020446777, Test_Loss: 3.395768880844116\n",
      "324:Epoch: 2, Train_Loss: 3.3910090923309326, Test_Loss: 3.3920557498931885 *\n",
      "325:Epoch: 2, Train_Loss: 3.4186818599700928, Test_Loss: 3.3909990787506104 *\n",
      "326:Epoch: 2, Train_Loss: 3.617558240890503, Test_Loss: 3.3887248039245605 *\n",
      "327:Epoch: 2, Train_Loss: 3.568406820297241, Test_Loss: 3.3861162662506104 *\n",
      "328:Epoch: 2, Train_Loss: 3.461970329284668, Test_Loss: 3.3822925090789795 *\n",
      "329:Epoch: 2, Train_Loss: 3.4296603202819824, Test_Loss: 3.381866931915283 *\n",
      "330:Epoch: 2, Train_Loss: 3.4921584129333496, Test_Loss: 3.4050514698028564\n",
      "331:Epoch: 2, Train_Loss: 3.482053279876709, Test_Loss: 3.4228296279907227\n",
      "332:Epoch: 2, Train_Loss: 3.517625570297241, Test_Loss: 6.530645370483398\n",
      "333:Epoch: 2, Train_Loss: 3.4886221885681152, Test_Loss: 6.039969444274902 *\n",
      "334:Epoch: 2, Train_Loss: 3.458153009414673, Test_Loss: 3.371276617050171 *\n",
      "335:Epoch: 2, Train_Loss: 3.360544443130493, Test_Loss: 3.365058183670044 *\n",
      "336:Epoch: 2, Train_Loss: 3.4250190258026123, Test_Loss: 3.38879132270813\n",
      "337:Epoch: 2, Train_Loss: 3.397024393081665, Test_Loss: 3.3913071155548096\n",
      "338:Epoch: 2, Train_Loss: 3.3657827377319336, Test_Loss: 3.380094528198242 *\n",
      "339:Epoch: 2, Train_Loss: 3.3537614345550537, Test_Loss: 3.430626153945923\n",
      "340:Epoch: 2, Train_Loss: 3.35192608833313, Test_Loss: 3.4953432083129883\n",
      "341:Epoch: 2, Train_Loss: 3.3543777465820312, Test_Loss: 3.349118947982788 *\n",
      "342:Epoch: 2, Train_Loss: 6.908436298370361, Test_Loss: 3.387545347213745\n",
      "343:Epoch: 2, Train_Loss: 5.599551200866699, Test_Loss: 3.3673603534698486 *\n",
      "344:Epoch: 2, Train_Loss: 3.353654384613037, Test_Loss: 3.355966091156006 *\n",
      "345:Epoch: 2, Train_Loss: 3.3546130657196045, Test_Loss: 3.3536195755004883 *\n",
      "346:Epoch: 2, Train_Loss: 3.359670400619507, Test_Loss: 3.402329683303833\n",
      "347:Epoch: 2, Train_Loss: 3.3527441024780273, Test_Loss: 3.3886773586273193 *\n",
      "348:Epoch: 2, Train_Loss: 3.3424408435821533, Test_Loss: 3.4693105220794678\n",
      "349:Epoch: 2, Train_Loss: 3.339104175567627, Test_Loss: 3.473834991455078\n",
      "350:Epoch: 2, Train_Loss: 3.3474161624908447, Test_Loss: 3.3396944999694824 *\n",
      "351:Epoch: 2, Train_Loss: 3.3602964878082275, Test_Loss: 3.3339414596557617 *\n",
      "352:Epoch: 2, Train_Loss: 3.33590030670166, Test_Loss: 3.327772378921509 *\n",
      "353:Epoch: 2, Train_Loss: 3.3310940265655518, Test_Loss: 3.3240208625793457 *\n",
      "354:Epoch: 2, Train_Loss: 3.328009843826294, Test_Loss: 3.321619749069214 *\n",
      "355:Epoch: 2, Train_Loss: 3.3402798175811768, Test_Loss: 3.319875955581665 *\n",
      "356:Epoch: 2, Train_Loss: 3.3233726024627686, Test_Loss: 3.3177454471588135 *\n",
      "357:Epoch: 2, Train_Loss: 3.317169666290283, Test_Loss: 3.315169095993042 *\n",
      "358:Epoch: 2, Train_Loss: 3.318497657775879, Test_Loss: 3.322251558303833\n",
      "359:Epoch: 2, Train_Loss: 3.377133369445801, Test_Loss: 3.3167004585266113 *\n",
      "360:Epoch: 2, Train_Loss: 3.3640811443328857, Test_Loss: 3.315704107284546 *\n",
      "361:Epoch: 2, Train_Loss: 3.308288097381592, Test_Loss: 3.312952756881714 *\n",
      "362:Epoch: 2, Train_Loss: 3.3093173503875732, Test_Loss: 3.3220877647399902\n",
      "363:Epoch: 2, Train_Loss: 3.335848093032837, Test_Loss: 3.347733736038208\n",
      "364:Epoch: 2, Train_Loss: 3.408773899078369, Test_Loss: 3.345179796218872 *\n",
      "365:Epoch: 2, Train_Loss: 3.372953176498413, Test_Loss: 3.7905147075653076\n",
      "366:Epoch: 2, Train_Loss: 3.382432460784912, Test_Loss: 3.717552661895752 *\n",
      "367:Epoch: 2, Train_Loss: 3.330665111541748, Test_Loss: 3.3989951610565186 *\n",
      "368:Epoch: 2, Train_Loss: 3.369581460952759, Test_Loss: 3.311854362487793 *\n",
      "369:Epoch: 2, Train_Loss: 3.354945421218872, Test_Loss: 3.304868698120117 *\n",
      "370:Epoch: 2, Train_Loss: 3.3400063514709473, Test_Loss: 3.3588764667510986\n",
      "371:Epoch: 2, Train_Loss: 3.3499112129211426, Test_Loss: 3.7186553478240967\n",
      "372:Epoch: 2, Train_Loss: 3.3391928672790527, Test_Loss: 4.7742695808410645\n",
      "373:Epoch: 2, Train_Loss: 3.299527883529663, Test_Loss: 4.019510269165039 *\n",
      "374:Epoch: 2, Train_Loss: 3.2830374240875244, Test_Loss: 3.3494317531585693 *\n",
      "375:Epoch: 2, Train_Loss: 3.279200792312622, Test_Loss: 3.2961697578430176 *\n",
      "376:Epoch: 2, Train_Loss: 3.2775955200195312, Test_Loss: 3.2856597900390625 *\n",
      "377:Epoch: 2, Train_Loss: 3.275826930999756, Test_Loss: 3.2796504497528076 *\n",
      "378:Epoch: 2, Train_Loss: 3.2744150161743164, Test_Loss: 3.2796761989593506\n",
      "379:Epoch: 2, Train_Loss: 6.591365337371826, Test_Loss: 3.308063268661499\n",
      "380:Epoch: 2, Train_Loss: 4.946103096008301, Test_Loss: 3.345345973968506\n",
      "381:Epoch: 2, Train_Loss: 3.26636004447937, Test_Loss: 3.26707124710083 *\n",
      "382:Epoch: 2, Train_Loss: 3.289541482925415, Test_Loss: 3.3149149417877197\n",
      "383:Epoch: 2, Train_Loss: 3.273890256881714, Test_Loss: 3.3824124336242676\n",
      "384:Epoch: 2, Train_Loss: 3.2584335803985596, Test_Loss: 3.5999186038970947\n",
      "385:Epoch: 2, Train_Loss: 3.2594141960144043, Test_Loss: 3.4551939964294434 *\n",
      "386:Epoch: 2, Train_Loss: 3.2554826736450195, Test_Loss: 3.2598986625671387 *\n",
      "387:Epoch: 2, Train_Loss: 3.253652572631836, Test_Loss: 3.253269672393799 *\n",
      "388:Epoch: 2, Train_Loss: 3.2509734630584717, Test_Loss: 3.2513585090637207 *\n",
      "389:Epoch: 2, Train_Loss: 3.2837696075439453, Test_Loss: 3.24936842918396 *\n",
      "390:Epoch: 2, Train_Loss: 3.355879306793213, Test_Loss: 3.255283832550049\n",
      "391:Epoch: 2, Train_Loss: 3.3408029079437256, Test_Loss: 5.04177188873291\n",
      "392:Epoch: 2, Train_Loss: 3.3595917224884033, Test_Loss: 6.978954315185547\n",
      "393:Epoch: 2, Train_Loss: 3.26701021194458, Test_Loss: 3.251434803009033 *\n",
      "394:Epoch: 2, Train_Loss: 3.2505860328674316, Test_Loss: 3.2406249046325684 *\n",
      "395:Epoch: 2, Train_Loss: 3.427314281463623, Test_Loss: 3.237553834915161 *\n",
      "396:Epoch: 2, Train_Loss: 3.4346892833709717, Test_Loss: 3.240563154220581\n",
      "397:Epoch: 2, Train_Loss: 3.421658515930176, Test_Loss: 3.2350568771362305 *\n",
      "398:Epoch: 2, Train_Loss: 3.2858798503875732, Test_Loss: 3.234642744064331 *\n",
      "399:Epoch: 2, Train_Loss: 3.2277944087982178, Test_Loss: 3.23282790184021 *\n",
      "400:Epoch: 2, Train_Loss: 3.227790117263794, Test_Loss: 3.230363130569458 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 2\n",
      "401:Epoch: 2, Train_Loss: 3.2266080379486084, Test_Loss: 3.230210781097412 *\n",
      "402:Epoch: 2, Train_Loss: 3.2279722690582275, Test_Loss: 3.224362850189209 *\n",
      "403:Epoch: 2, Train_Loss: 3.2272377014160156, Test_Loss: 3.228081464767456\n",
      "404:Epoch: 2, Train_Loss: 3.2214231491088867, Test_Loss: 3.242382764816284\n",
      "405:Epoch: 2, Train_Loss: 3.2184810638427734, Test_Loss: 3.237497329711914 *\n",
      "406:Epoch: 2, Train_Loss: 3.2147011756896973, Test_Loss: 3.222379446029663 *\n",
      "407:Epoch: 2, Train_Loss: 3.218445062637329, Test_Loss: 3.2134783267974854 *\n",
      "408:Epoch: 2, Train_Loss: 3.291843891143799, Test_Loss: 3.213545799255371\n",
      "409:Epoch: 2, Train_Loss: 3.3939316272735596, Test_Loss: 3.2101891040802 *\n",
      "410:Epoch: 2, Train_Loss: 3.379058837890625, Test_Loss: 3.2087790966033936 *\n",
      "411:Epoch: 2, Train_Loss: 3.300598621368408, Test_Loss: 3.208890914916992\n",
      "412:Epoch: 2, Train_Loss: 3.336456298828125, Test_Loss: 3.2045629024505615 *\n",
      "413:Epoch: 2, Train_Loss: 3.3750574588775635, Test_Loss: 3.2036426067352295 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414:Epoch: 2, Train_Loss: 3.213930606842041, Test_Loss: 3.2049691677093506\n",
      "415:Epoch: 2, Train_Loss: 3.3655056953430176, Test_Loss: 3.199686288833618 *\n",
      "416:Epoch: 2, Train_Loss: 3.3226139545440674, Test_Loss: 3.1986284255981445 *\n",
      "417:Epoch: 2, Train_Loss: 3.4739999771118164, Test_Loss: 3.195345401763916 *\n",
      "418:Epoch: 2, Train_Loss: 3.2035958766937256, Test_Loss: 3.1942250728607178 *\n",
      "419:Epoch: 2, Train_Loss: 3.725696563720703, Test_Loss: 3.191591501235962 *\n",
      "420:Epoch: 2, Train_Loss: 5.903409004211426, Test_Loss: 3.1888787746429443 *\n",
      "421:Epoch: 2, Train_Loss: 3.2339370250701904, Test_Loss: 3.2314822673797607\n",
      "422:Epoch: 2, Train_Loss: 3.2280802726745605, Test_Loss: 3.216862440109253 *\n",
      "423:Epoch: 2, Train_Loss: 3.226101875305176, Test_Loss: 7.288804054260254\n",
      "424:Epoch: 2, Train_Loss: 3.2283051013946533, Test_Loss: 4.665893077850342 *\n",
      "425:Epoch: 2, Train_Loss: 3.17883563041687, Test_Loss: 3.1796274185180664 *\n",
      "426:Epoch: 2, Train_Loss: 3.1796786785125732, Test_Loss: 3.1853203773498535\n",
      "427:Epoch: 2, Train_Loss: 3.2857351303100586, Test_Loss: 3.2218337059020996\n",
      "428:Epoch: 2, Train_Loss: 3.295711040496826, Test_Loss: 3.23085618019104\n",
      "429:Epoch: 2, Train_Loss: 3.2859811782836914, Test_Loss: 3.186868906021118 *\n",
      "430:Epoch: 2, Train_Loss: 3.2514145374298096, Test_Loss: 3.2506368160247803\n",
      "431:Epoch: 2, Train_Loss: 3.2507102489471436, Test_Loss: 3.256774663925171\n",
      "432:Epoch: 2, Train_Loss: 3.1943862438201904, Test_Loss: 3.166195869445801 *\n",
      "433:Epoch: 2, Train_Loss: 3.1963768005371094, Test_Loss: 3.197526216506958\n",
      "434:Epoch: 2, Train_Loss: 3.172274351119995, Test_Loss: 3.1787140369415283 *\n",
      "435:Epoch: 2, Train_Loss: 3.187873601913452, Test_Loss: 3.170842170715332 *\n",
      "436:Epoch: 2, Train_Loss: 3.1678202152252197, Test_Loss: 3.1597330570220947 *\n",
      "437:Epoch: 2, Train_Loss: 3.153707981109619, Test_Loss: 3.2698943614959717\n",
      "438:Epoch: 2, Train_Loss: 3.194779872894287, Test_Loss: 3.198237419128418 *\n",
      "439:Epoch: 2, Train_Loss: 3.2096872329711914, Test_Loss: 3.264465808868408\n",
      "440:Epoch: 2, Train_Loss: 3.1825814247131348, Test_Loss: 3.2229204177856445 *\n",
      "441:Epoch: 2, Train_Loss: 3.146472454071045, Test_Loss: 3.1844170093536377 *\n",
      "442:Epoch: 2, Train_Loss: 3.1444313526153564, Test_Loss: 3.162691831588745 *\n",
      "443:Epoch: 2, Train_Loss: 3.1420605182647705, Test_Loss: 3.160567045211792 *\n",
      "444:Epoch: 2, Train_Loss: 3.1401596069335938, Test_Loss: 3.1559832096099854 *\n",
      "445:Epoch: 2, Train_Loss: 3.139151096343994, Test_Loss: 3.1544909477233887 *\n",
      "446:Epoch: 2, Train_Loss: 3.136683940887451, Test_Loss: 3.1542179584503174 *\n",
      "447:Epoch: 2, Train_Loss: 3.1354050636291504, Test_Loss: 3.150707483291626 *\n",
      "448:Epoch: 2, Train_Loss: 3.1335504055023193, Test_Loss: 3.1449134349823 *\n",
      "449:Epoch: 2, Train_Loss: 3.130680561065674, Test_Loss: 3.159816026687622\n",
      "450:Epoch: 2, Train_Loss: 3.1292078495025635, Test_Loss: 3.15324330329895 *\n",
      "451:Epoch: 2, Train_Loss: 3.1334328651428223, Test_Loss: 3.142921209335327 *\n",
      "452:Epoch: 2, Train_Loss: 3.1364474296569824, Test_Loss: 3.135288715362549 *\n",
      "453:Epoch: 2, Train_Loss: 3.133514165878296, Test_Loss: 3.177685022354126\n",
      "454:Epoch: 2, Train_Loss: 3.1438307762145996, Test_Loss: 3.1724464893341064 *\n",
      "1:Epoch: 3, Train_Loss: 3.124767780303955, Test_Loss: 3.295691728591919 *\n",
      "2:Epoch: 3, Train_Loss: 3.122539520263672, Test_Loss: 3.6975841522216797\n",
      "3:Epoch: 3, Train_Loss: 3.1162757873535156, Test_Loss: 3.5086770057678223 *\n",
      "4:Epoch: 3, Train_Loss: 3.114975690841675, Test_Loss: 3.238553524017334 *\n",
      "5:Epoch: 3, Train_Loss: 3.133326768875122, Test_Loss: 3.1558032035827637 *\n",
      "6:Epoch: 3, Train_Loss: 3.1279444694519043, Test_Loss: 3.123584508895874 *\n",
      "7:Epoch: 3, Train_Loss: 3.1102776527404785, Test_Loss: 3.1788833141326904\n",
      "8:Epoch: 3, Train_Loss: 3.1069631576538086, Test_Loss: 3.6728627681732178\n",
      "9:Epoch: 3, Train_Loss: 3.1103739738464355, Test_Loss: 4.503540515899658\n",
      "10:Epoch: 3, Train_Loss: 3.1758599281311035, Test_Loss: 3.532294273376465 *\n",
      "11:Epoch: 3, Train_Loss: 3.1291310787200928, Test_Loss: 3.2110674381256104 *\n",
      "12:Epoch: 3, Train_Loss: 3.1507582664489746, Test_Loss: 3.1068153381347656 *\n",
      "13:Epoch: 3, Train_Loss: 3.0990149974823, Test_Loss: 3.1035430431365967 *\n",
      "14:Epoch: 3, Train_Loss: 3.1183021068573, Test_Loss: 3.0975937843322754 *\n",
      "15:Epoch: 3, Train_Loss: 3.1427907943725586, Test_Loss: 3.1042425632476807\n",
      "16:Epoch: 3, Train_Loss: 3.0935540199279785, Test_Loss: 3.121077299118042\n",
      "17:Epoch: 3, Train_Loss: 3.1074109077453613, Test_Loss: 3.1377055644989014\n",
      "18:Epoch: 3, Train_Loss: 3.1164357662200928, Test_Loss: 3.092176914215088 *\n",
      "19:Epoch: 3, Train_Loss: 3.1614255905151367, Test_Loss: 3.179647445678711\n",
      "20:Epoch: 3, Train_Loss: 3.1814324855804443, Test_Loss: 3.376725196838379\n",
      "21:Epoch: 3, Train_Loss: 3.158493757247925, Test_Loss: 3.276427745819092 *\n",
      "22:Epoch: 3, Train_Loss: 3.1147572994232178, Test_Loss: 3.289992332458496\n",
      "23:Epoch: 3, Train_Loss: 3.082677125930786, Test_Loss: 3.0866105556488037 *\n",
      "24:Epoch: 3, Train_Loss: 3.102454662322998, Test_Loss: 3.083329916000366 *\n",
      "25:Epoch: 3, Train_Loss: 3.076129198074341, Test_Loss: 3.081430673599243 *\n",
      "26:Epoch: 3, Train_Loss: 3.079312562942505, Test_Loss: 3.0802628993988037 *\n",
      "27:Epoch: 3, Train_Loss: 3.0859897136688232, Test_Loss: 3.0950233936309814\n",
      "28:Epoch: 3, Train_Loss: 3.090459108352661, Test_Loss: 6.185755729675293\n",
      "29:Epoch: 3, Train_Loss: 3.1576943397521973, Test_Loss: 5.373518943786621 *\n",
      "30:Epoch: 3, Train_Loss: 3.0883779525756836, Test_Loss: 3.0765645503997803 *\n",
      "31:Epoch: 3, Train_Loss: 3.1345067024230957, Test_Loss: 3.067786693572998 *\n",
      "32:Epoch: 3, Train_Loss: 3.071636915206909, Test_Loss: 3.0657849311828613 *\n",
      "33:Epoch: 3, Train_Loss: 3.0978238582611084, Test_Loss: 3.0722897052764893\n",
      "34:Epoch: 3, Train_Loss: 3.0698206424713135, Test_Loss: 3.061230182647705 *\n",
      "35:Epoch: 3, Train_Loss: 3.300598382949829, Test_Loss: 3.0616092681884766\n",
      "36:Epoch: 3, Train_Loss: 3.1487910747528076, Test_Loss: 3.056460380554199 *\n",
      "37:Epoch: 3, Train_Loss: 3.0713491439819336, Test_Loss: 3.0565030574798584\n",
      "38:Epoch: 3, Train_Loss: 3.0703577995300293, Test_Loss: 3.055318593978882 *\n",
      "39:Epoch: 3, Train_Loss: 3.04997181892395, Test_Loss: 3.0511577129364014 *\n",
      "40:Epoch: 3, Train_Loss: 3.0488176345825195, Test_Loss: 3.0574235916137695\n",
      "41:Epoch: 3, Train_Loss: 3.046914577484131, Test_Loss: 3.0793018341064453\n",
      "42:Epoch: 3, Train_Loss: 3.050429582595825, Test_Loss: 3.068925142288208 *\n",
      "43:Epoch: 3, Train_Loss: 3.0585789680480957, Test_Loss: 3.0450809001922607 *\n",
      "44:Epoch: 3, Train_Loss: 3.0685627460479736, Test_Loss: 3.0407557487487793 *\n",
      "45:Epoch: 3, Train_Loss: 3.053903341293335, Test_Loss: 3.0408499240875244\n",
      "46:Epoch: 3, Train_Loss: 3.0520853996276855, Test_Loss: 3.0377416610717773 *\n",
      "47:Epoch: 3, Train_Loss: 3.0561795234680176, Test_Loss: 3.035428047180176 *\n",
      "48:Epoch: 3, Train_Loss: 3.0353140830993652, Test_Loss: 3.0358948707580566\n",
      "49:Epoch: 3, Train_Loss: 3.032536745071411, Test_Loss: 3.0313560962677 *\n",
      "50:Epoch: 3, Train_Loss: 3.0333099365234375, Test_Loss: 3.0306060314178467 *\n",
      "51:Epoch: 3, Train_Loss: 3.059921979904175, Test_Loss: 3.031392812728882\n",
      "52:Epoch: 3, Train_Loss: 3.061617851257324, Test_Loss: 3.027357339859009 *\n",
      "53:Epoch: 3, Train_Loss: 3.0289735794067383, Test_Loss: 3.025935411453247 *\n",
      "54:Epoch: 3, Train_Loss: 3.052945613861084, Test_Loss: 3.0231480598449707 *\n",
      "55:Epoch: 3, Train_Loss: 3.080704927444458, Test_Loss: 3.0220868587493896 *\n",
      "56:Epoch: 3, Train_Loss: 3.081756353378296, Test_Loss: 3.0200371742248535 *\n",
      "57:Epoch: 3, Train_Loss: 3.017620325088501, Test_Loss: 3.0186703205108643 *\n",
      "58:Epoch: 3, Train_Loss: 3.044396162033081, Test_Loss: 3.057842254638672\n",
      "59:Epoch: 3, Train_Loss: 3.0222220420837402, Test_Loss: 3.0461485385894775 *\n",
      "60:Epoch: 3, Train_Loss: 3.0320727825164795, Test_Loss: 8.249397277832031\n",
      "61:Epoch: 3, Train_Loss: 3.012786626815796, Test_Loss: 3.3350560665130615 *\n",
      "62:Epoch: 3, Train_Loss: 3.036564350128174, Test_Loss: 3.0094048976898193 *\n",
      "63:Epoch: 3, Train_Loss: 3.0731334686279297, Test_Loss: 3.028839588165283\n",
      "64:Epoch: 3, Train_Loss: 5.345261573791504, Test_Loss: 3.0616111755371094\n",
      "65:Epoch: 3, Train_Loss: 6.09468412399292, Test_Loss: 3.0666985511779785\n",
      "66:Epoch: 3, Train_Loss: 3.022111415863037, Test_Loss: 3.008272647857666 *\n",
      "67:Epoch: 3, Train_Loss: 3.0000689029693604, Test_Loss: 3.103402614593506\n",
      "68:Epoch: 3, Train_Loss: 3.094909906387329, Test_Loss: 3.0645911693573 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69:Epoch: 3, Train_Loss: 3.1453421115875244, Test_Loss: 2.99706768989563 *\n",
      "70:Epoch: 3, Train_Loss: 3.018073558807373, Test_Loss: 3.035757541656494\n",
      "71:Epoch: 3, Train_Loss: 2.9950904846191406, Test_Loss: 3.0088255405426025 *\n",
      "72:Epoch: 3, Train_Loss: 3.0214133262634277, Test_Loss: 3.001194953918457 *\n",
      "73:Epoch: 3, Train_Loss: 3.039322853088379, Test_Loss: 3.0018625259399414\n",
      "74:Epoch: 3, Train_Loss: 3.0017993450164795, Test_Loss: 3.1258511543273926\n",
      "75:Epoch: 3, Train_Loss: 2.999856948852539, Test_Loss: 3.0202419757843018 *\n",
      "76:Epoch: 3, Train_Loss: 4.163471698760986, Test_Loss: 3.1022796630859375\n",
      "77:Epoch: 3, Train_Loss: 4.402400016784668, Test_Loss: 3.037834882736206 *\n",
      "78:Epoch: 3, Train_Loss: 3.2602007389068604, Test_Loss: 3.0241098403930664 *\n",
      "79:Epoch: 3, Train_Loss: 3.0647642612457275, Test_Loss: 3.0011746883392334 *\n",
      "80:Epoch: 3, Train_Loss: 4.426328659057617, Test_Loss: 2.9981606006622314 *\n",
      "81:Epoch: 3, Train_Loss: 5.003405570983887, Test_Loss: 2.9946866035461426 *\n",
      "82:Epoch: 3, Train_Loss: 3.0574233531951904, Test_Loss: 2.991281509399414 *\n",
      "83:Epoch: 3, Train_Loss: 3.019146680831909, Test_Loss: 2.989643096923828 *\n",
      "84:Epoch: 3, Train_Loss: 3.099287986755371, Test_Loss: 2.986614942550659 *\n",
      "85:Epoch: 3, Train_Loss: 4.663303375244141, Test_Loss: 2.9820480346679688 *\n",
      "86:Epoch: 3, Train_Loss: 4.338484764099121, Test_Loss: 2.9951820373535156\n",
      "87:Epoch: 3, Train_Loss: 2.9802486896514893, Test_Loss: 2.9924468994140625 *\n",
      "88:Epoch: 3, Train_Loss: 2.990178346633911, Test_Loss: 2.971717596054077 *\n",
      "89:Epoch: 3, Train_Loss: 2.995969295501709, Test_Loss: 2.9737637042999268\n",
      "90:Epoch: 3, Train_Loss: 3.698107957839966, Test_Loss: 3.029691219329834\n",
      "91:Epoch: 3, Train_Loss: 2.9927475452423096, Test_Loss: 2.9871506690979004 *\n",
      "92:Epoch: 3, Train_Loss: 3.016871452331543, Test_Loss: 3.236752510070801\n",
      "93:Epoch: 3, Train_Loss: 2.9779224395751953, Test_Loss: 3.565932273864746\n",
      "94:Epoch: 3, Train_Loss: 3.133157730102539, Test_Loss: 3.2560884952545166 *\n",
      "95:Epoch: 3, Train_Loss: 3.1098499298095703, Test_Loss: 3.062042474746704 *\n",
      "96:Epoch: 3, Train_Loss: 3.211848735809326, Test_Loss: 2.9988512992858887 *\n",
      "97:Epoch: 3, Train_Loss: 3.2190632820129395, Test_Loss: 2.9530577659606934 *\n",
      "98:Epoch: 3, Train_Loss: 3.007240056991577, Test_Loss: 3.028541088104248\n",
      "99:Epoch: 3, Train_Loss: 3.108656406402588, Test_Loss: 3.6734347343444824\n",
      "100:Epoch: 3, Train_Loss: 3.186742067337036, Test_Loss: 4.281124114990234\n",
      "Model saved at location ../Saver/model.ckpt at epoch 3\n",
      "101:Epoch: 3, Train_Loss: 3.446275472640991, Test_Loss: 3.1474010944366455 *\n",
      "102:Epoch: 3, Train_Loss: 3.319225549697876, Test_Loss: 3.055898904800415 *\n",
      "103:Epoch: 3, Train_Loss: 2.9634456634521484, Test_Loss: 2.938061237335205 *\n",
      "104:Epoch: 3, Train_Loss: 3.057868719100952, Test_Loss: 2.9415223598480225\n",
      "105:Epoch: 3, Train_Loss: 3.0654139518737793, Test_Loss: 2.9363701343536377 *\n",
      "106:Epoch: 3, Train_Loss: 2.941908597946167, Test_Loss: 2.9473531246185303\n",
      "107:Epoch: 3, Train_Loss: 2.9428741931915283, Test_Loss: 2.9533820152282715\n",
      "108:Epoch: 3, Train_Loss: 2.9277901649475098, Test_Loss: 2.962824583053589\n",
      "109:Epoch: 3, Train_Loss: 2.92726469039917, Test_Loss: 2.9316160678863525 *\n",
      "110:Epoch: 3, Train_Loss: 2.9290497303009033, Test_Loss: 3.045025110244751\n",
      "111:Epoch: 3, Train_Loss: 2.9277563095092773, Test_Loss: 3.311110734939575\n",
      "112:Epoch: 3, Train_Loss: 3.0029897689819336, Test_Loss: 3.0334837436676025 *\n",
      "113:Epoch: 3, Train_Loss: 2.9860005378723145, Test_Loss: 3.1207971572875977\n",
      "114:Epoch: 3, Train_Loss: 3.031806468963623, Test_Loss: 2.9314732551574707 *\n",
      "115:Epoch: 3, Train_Loss: 3.032886266708374, Test_Loss: 2.9296607971191406 *\n",
      "116:Epoch: 3, Train_Loss: 3.404801607131958, Test_Loss: 2.9276819229125977 *\n",
      "117:Epoch: 3, Train_Loss: 2.919837474822998, Test_Loss: 2.926827907562256 *\n",
      "118:Epoch: 3, Train_Loss: 2.942434310913086, Test_Loss: 2.946404457092285\n",
      "119:Epoch: 3, Train_Loss: 3.1415224075317383, Test_Loss: 7.307699203491211\n",
      "120:Epoch: 3, Train_Loss: 3.4383392333984375, Test_Loss: 3.861499547958374 *\n",
      "121:Epoch: 3, Train_Loss: 3.1206724643707275, Test_Loss: 2.916203498840332 *\n",
      "122:Epoch: 3, Train_Loss: 2.906137228012085, Test_Loss: 2.9080355167388916 *\n",
      "123:Epoch: 3, Train_Loss: 3.207155704498291, Test_Loss: 2.908674955368042\n",
      "124:Epoch: 3, Train_Loss: 3.5619895458221436, Test_Loss: 2.9142322540283203\n",
      "125:Epoch: 3, Train_Loss: 3.422229528427124, Test_Loss: 2.903076410293579 *\n",
      "126:Epoch: 3, Train_Loss: 2.9469945430755615, Test_Loss: 2.9022109508514404 *\n",
      "127:Epoch: 3, Train_Loss: 2.9210004806518555, Test_Loss: 2.897474527359009 *\n",
      "128:Epoch: 3, Train_Loss: 2.9408481121063232, Test_Loss: 2.8973939418792725 *\n",
      "129:Epoch: 3, Train_Loss: 4.288506507873535, Test_Loss: 2.8968143463134766 *\n",
      "130:Epoch: 3, Train_Loss: 3.6811728477478027, Test_Loss: 2.8973288536071777\n",
      "131:Epoch: 3, Train_Loss: 2.8930630683898926, Test_Loss: 2.8990442752838135\n",
      "132:Epoch: 3, Train_Loss: 2.9119603633880615, Test_Loss: 2.921165943145752\n",
      "133:Epoch: 3, Train_Loss: 2.889047145843506, Test_Loss: 2.9046571254730225 *\n",
      "134:Epoch: 3, Train_Loss: 3.0388011932373047, Test_Loss: 2.885791301727295 *\n",
      "135:Epoch: 3, Train_Loss: 3.2154276371002197, Test_Loss: 2.8841969966888428 *\n",
      "136:Epoch: 3, Train_Loss: 2.9165008068084717, Test_Loss: 2.8832483291625977 *\n",
      "137:Epoch: 3, Train_Loss: 3.190805673599243, Test_Loss: 2.8812646865844727 *\n",
      "138:Epoch: 3, Train_Loss: 2.8997461795806885, Test_Loss: 2.877955913543701 *\n",
      "139:Epoch: 3, Train_Loss: 2.9266929626464844, Test_Loss: 2.8781683444976807\n",
      "140:Epoch: 3, Train_Loss: 2.9726765155792236, Test_Loss: 2.875277519226074 *\n",
      "141:Epoch: 3, Train_Loss: 3.2088334560394287, Test_Loss: 2.8759915828704834\n",
      "142:Epoch: 3, Train_Loss: 2.9947965145111084, Test_Loss: 2.874129056930542 *\n",
      "143:Epoch: 3, Train_Loss: 2.9117445945739746, Test_Loss: 2.873079299926758 *\n",
      "144:Epoch: 3, Train_Loss: 3.0590100288391113, Test_Loss: 2.8694305419921875 *\n",
      "145:Epoch: 3, Train_Loss: 3.1076719760894775, Test_Loss: 2.867103338241577 *\n",
      "146:Epoch: 3, Train_Loss: 2.878774642944336, Test_Loss: 2.8668324947357178 *\n",
      "147:Epoch: 3, Train_Loss: 2.9069974422454834, Test_Loss: 2.8642091751098633 *\n",
      "148:Epoch: 3, Train_Loss: 2.874833822250366, Test_Loss: 2.8641748428344727 *\n",
      "149:Epoch: 3, Train_Loss: 2.9121382236480713, Test_Loss: 2.9186205863952637\n",
      "150:Epoch: 3, Train_Loss: 2.897887945175171, Test_Loss: 3.0438287258148193\n",
      "151:Epoch: 3, Train_Loss: 3.538985252380371, Test_Loss: 8.144691467285156\n",
      "152:Epoch: 3, Train_Loss: 3.02801775932312, Test_Loss: 2.867525815963745 *\n",
      "153:Epoch: 3, Train_Loss: 3.667668581008911, Test_Loss: 2.858022928237915 *\n",
      "154:Epoch: 3, Train_Loss: 3.4660744667053223, Test_Loss: 2.890631914138794\n",
      "155:Epoch: 3, Train_Loss: 3.1438260078430176, Test_Loss: 2.8915157318115234\n",
      "156:Epoch: 3, Train_Loss: 3.237833261489868, Test_Loss: 2.899721622467041\n",
      "157:Epoch: 3, Train_Loss: 2.888784885406494, Test_Loss: 2.8528518676757812 *\n",
      "158:Epoch: 3, Train_Loss: 2.855731964111328, Test_Loss: 2.9563887119293213\n",
      "159:Epoch: 3, Train_Loss: 2.8653957843780518, Test_Loss: 2.8802192211151123 *\n",
      "160:Epoch: 3, Train_Loss: 2.997744083404541, Test_Loss: 2.8479487895965576 *\n",
      "161:Epoch: 3, Train_Loss: 3.356612205505371, Test_Loss: 2.863132953643799\n",
      "162:Epoch: 3, Train_Loss: 3.324049711227417, Test_Loss: 2.8763699531555176\n",
      "163:Epoch: 3, Train_Loss: 4.318717956542969, Test_Loss: 2.8401951789855957 *\n",
      "164:Epoch: 3, Train_Loss: 3.8479385375976562, Test_Loss: 2.888920307159424\n",
      "165:Epoch: 3, Train_Loss: 3.75886607170105, Test_Loss: 2.908804178237915\n",
      "166:Epoch: 3, Train_Loss: 3.141367197036743, Test_Loss: 2.897428274154663 *\n",
      "167:Epoch: 3, Train_Loss: 2.8380892276763916, Test_Loss: 2.8950493335723877 *\n",
      "168:Epoch: 3, Train_Loss: 2.8922688961029053, Test_Loss: 2.862069606781006 *\n",
      "169:Epoch: 3, Train_Loss: 3.771604061126709, Test_Loss: 2.882650136947632\n",
      "170:Epoch: 3, Train_Loss: 3.9343295097351074, Test_Loss: 2.8360891342163086 *\n",
      "171:Epoch: 3, Train_Loss: 2.8792123794555664, Test_Loss: 2.8269622325897217 *\n",
      "172:Epoch: 3, Train_Loss: 2.886425256729126, Test_Loss: 2.8290603160858154\n",
      "173:Epoch: 3, Train_Loss: 2.860823631286621, Test_Loss: 2.8288402557373047 *\n",
      "174:Epoch: 3, Train_Loss: 3.1472713947296143, Test_Loss: 2.8277151584625244 *\n",
      "175:Epoch: 3, Train_Loss: 2.994039535522461, Test_Loss: 2.82182240486145 *\n",
      "176:Epoch: 3, Train_Loss: 3.6675360202789307, Test_Loss: 2.8194503784179688 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177:Epoch: 3, Train_Loss: 3.62490177154541, Test_Loss: 2.8175923824310303 *\n",
      "178:Epoch: 3, Train_Loss: 3.056535482406616, Test_Loss: 2.8165078163146973 *\n",
      "179:Epoch: 3, Train_Loss: 2.819833755493164, Test_Loss: 2.8352091312408447\n",
      "180:Epoch: 3, Train_Loss: 2.813953399658203, Test_Loss: 2.820277690887451 *\n",
      "181:Epoch: 3, Train_Loss: 2.813448667526245, Test_Loss: 2.8264284133911133\n",
      "182:Epoch: 3, Train_Loss: 2.8455145359039307, Test_Loss: 2.8320071697235107\n",
      "183:Epoch: 3, Train_Loss: 2.817532777786255, Test_Loss: 3.08422589302063\n",
      "184:Epoch: 3, Train_Loss: 2.825840711593628, Test_Loss: 3.306994676589966\n",
      "185:Epoch: 3, Train_Loss: 20.06403923034668, Test_Loss: 3.0409352779388428 *\n",
      "186:Epoch: 3, Train_Loss: 2.8063952922821045, Test_Loss: 2.8764595985412598 *\n",
      "187:Epoch: 3, Train_Loss: 4.784481525421143, Test_Loss: 2.838062286376953 *\n",
      "188:Epoch: 3, Train_Loss: 4.841809272766113, Test_Loss: 2.803462266921997 *\n",
      "189:Epoch: 3, Train_Loss: 2.8028578758239746, Test_Loss: 2.842987298965454\n",
      "190:Epoch: 3, Train_Loss: 2.834557294845581, Test_Loss: 3.446530818939209\n",
      "191:Epoch: 3, Train_Loss: 6.713836193084717, Test_Loss: 3.788789749145508\n",
      "192:Epoch: 3, Train_Loss: 11.304319381713867, Test_Loss: 2.8729324340820312 *\n",
      "193:Epoch: 3, Train_Loss: 2.865216016769409, Test_Loss: 2.845259189605713 *\n",
      "194:Epoch: 3, Train_Loss: 2.8142659664154053, Test_Loss: 2.8015472888946533 *\n",
      "195:Epoch: 3, Train_Loss: 8.874168395996094, Test_Loss: 2.8288111686706543\n",
      "196:Epoch: 3, Train_Loss: 2.829606294631958, Test_Loss: 2.814990282058716 *\n",
      "197:Epoch: 3, Train_Loss: 2.827291965484619, Test_Loss: 2.84854793548584\n",
      "198:Epoch: 3, Train_Loss: 2.811925172805786, Test_Loss: 2.8947832584381104\n",
      "199:Epoch: 3, Train_Loss: 2.8014633655548096, Test_Loss: 2.85201096534729 *\n",
      "200:Epoch: 3, Train_Loss: 2.8098435401916504, Test_Loss: 2.8252313137054443 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 3\n",
      "201:Epoch: 3, Train_Loss: 2.798445463180542, Test_Loss: 2.823695421218872 *\n",
      "202:Epoch: 3, Train_Loss: 2.800546646118164, Test_Loss: 3.1192541122436523\n",
      "203:Epoch: 3, Train_Loss: 2.79752516746521, Test_Loss: 2.8285436630249023 *\n",
      "204:Epoch: 3, Train_Loss: 2.7972421646118164, Test_Loss: 2.8639891147613525\n",
      "205:Epoch: 3, Train_Loss: 2.8028974533081055, Test_Loss: 2.777223587036133 *\n",
      "206:Epoch: 3, Train_Loss: 2.7942357063293457, Test_Loss: 2.774332284927368 *\n",
      "207:Epoch: 3, Train_Loss: 2.7864694595336914, Test_Loss: 2.7723236083984375 *\n",
      "208:Epoch: 3, Train_Loss: 2.8368871212005615, Test_Loss: 2.770622968673706 *\n",
      "209:Epoch: 3, Train_Loss: 2.8479738235473633, Test_Loss: 2.806107997894287\n",
      "210:Epoch: 3, Train_Loss: 2.7996912002563477, Test_Loss: 8.40097427368164\n",
      "211:Epoch: 3, Train_Loss: 2.7786200046539307, Test_Loss: 3.0636065006256104 *\n",
      "212:Epoch: 3, Train_Loss: 2.7752439975738525, Test_Loss: 2.7746691703796387 *\n",
      "213:Epoch: 3, Train_Loss: 2.764026403427124, Test_Loss: 2.7675669193267822 *\n",
      "214:Epoch: 3, Train_Loss: 2.762066602706909, Test_Loss: 2.7618701457977295 *\n",
      "215:Epoch: 3, Train_Loss: 2.7618463039398193, Test_Loss: 2.765273332595825\n",
      "216:Epoch: 3, Train_Loss: 2.7594943046569824, Test_Loss: 2.760545492172241 *\n",
      "217:Epoch: 3, Train_Loss: 2.760768175125122, Test_Loss: 2.772736072540283\n",
      "218:Epoch: 3, Train_Loss: 2.7560007572174072, Test_Loss: 2.76530385017395 *\n",
      "219:Epoch: 3, Train_Loss: 2.754112720489502, Test_Loss: 2.763976812362671 *\n",
      "220:Epoch: 3, Train_Loss: 2.752406120300293, Test_Loss: 2.765101909637451\n",
      "221:Epoch: 3, Train_Loss: 2.7538628578186035, Test_Loss: 2.7673301696777344\n",
      "222:Epoch: 3, Train_Loss: 2.7697200775146484, Test_Loss: 2.756716728210449 *\n",
      "223:Epoch: 3, Train_Loss: 2.7787599563598633, Test_Loss: 2.7631242275238037\n",
      "224:Epoch: 3, Train_Loss: 2.7583389282226562, Test_Loss: 2.7586183547973633 *\n",
      "225:Epoch: 3, Train_Loss: 2.758946418762207, Test_Loss: 2.7564613819122314 *\n",
      "226:Epoch: 3, Train_Loss: 3.746504306793213, Test_Loss: 2.748687267303467 *\n",
      "227:Epoch: 3, Train_Loss: 11.039209365844727, Test_Loss: 2.7554209232330322\n",
      "228:Epoch: 3, Train_Loss: 2.792933702468872, Test_Loss: 2.7505173683166504 *\n",
      "229:Epoch: 3, Train_Loss: 2.7458672523498535, Test_Loss: 2.749937057495117 *\n",
      "230:Epoch: 3, Train_Loss: 2.7592060565948486, Test_Loss: 2.7612502574920654\n",
      "231:Epoch: 3, Train_Loss: 2.7569658756256104, Test_Loss: 2.7524495124816895 *\n",
      "232:Epoch: 3, Train_Loss: 2.7556352615356445, Test_Loss: 2.765714406967163\n",
      "233:Epoch: 3, Train_Loss: 2.7464969158172607, Test_Loss: 2.7643306255340576 *\n",
      "234:Epoch: 3, Train_Loss: 2.769554376602173, Test_Loss: 2.7500898838043213 *\n",
      "235:Epoch: 3, Train_Loss: 2.96103572845459, Test_Loss: 2.749931812286377 *\n",
      "236:Epoch: 3, Train_Loss: 2.943155527114868, Test_Loss: 2.7531049251556396\n",
      "237:Epoch: 3, Train_Loss: 2.863642454147339, Test_Loss: 2.7451953887939453 *\n",
      "238:Epoch: 3, Train_Loss: 2.7629430294036865, Test_Loss: 2.7472715377807617\n",
      "239:Epoch: 3, Train_Loss: 2.871340751647949, Test_Loss: 2.752936840057373\n",
      "240:Epoch: 3, Train_Loss: 2.842597484588623, Test_Loss: 2.7949631214141846\n",
      "241:Epoch: 3, Train_Loss: 2.8945112228393555, Test_Loss: 4.050241470336914\n",
      "242:Epoch: 3, Train_Loss: 2.867973804473877, Test_Loss: 7.305645942687988\n",
      "243:Epoch: 3, Train_Loss: 2.8496131896972656, Test_Loss: 2.7462894916534424 *\n",
      "244:Epoch: 3, Train_Loss: 2.729750871658325, Test_Loss: 2.726423978805542 *\n",
      "245:Epoch: 3, Train_Loss: 2.7573094367980957, Test_Loss: 2.7403292655944824\n",
      "246:Epoch: 3, Train_Loss: 2.7691495418548584, Test_Loss: 2.743062734603882\n",
      "247:Epoch: 3, Train_Loss: 2.7356879711151123, Test_Loss: 2.7482571601867676\n",
      "248:Epoch: 3, Train_Loss: 2.7191162109375, Test_Loss: 2.7491796016693115\n",
      "249:Epoch: 3, Train_Loss: 2.7200779914855957, Test_Loss: 2.9052937030792236\n",
      "250:Epoch: 3, Train_Loss: 2.7175135612487793, Test_Loss: 2.739863395690918 *\n",
      "251:Epoch: 3, Train_Loss: 4.181798934936523, Test_Loss: 2.7229621410369873 *\n",
      "252:Epoch: 3, Train_Loss: 7.0276384353637695, Test_Loss: 2.763169288635254\n",
      "253:Epoch: 3, Train_Loss: 2.716750144958496, Test_Loss: 2.7137458324432373 *\n",
      "254:Epoch: 3, Train_Loss: 2.72932767868042, Test_Loss: 2.7322189807891846\n",
      "255:Epoch: 3, Train_Loss: 2.7330691814422607, Test_Loss: 2.7447092533111572\n",
      "256:Epoch: 3, Train_Loss: 2.7284932136535645, Test_Loss: 2.768099784851074\n",
      "257:Epoch: 3, Train_Loss: 2.7179253101348877, Test_Loss: 2.8149969577789307\n",
      "258:Epoch: 3, Train_Loss: 2.7119264602661133, Test_Loss: 2.8762855529785156\n",
      "259:Epoch: 3, Train_Loss: 2.7107555866241455, Test_Loss: 2.7364158630371094 *\n",
      "260:Epoch: 3, Train_Loss: 2.726604461669922, Test_Loss: 2.709196090698242 *\n",
      "261:Epoch: 3, Train_Loss: 2.712923288345337, Test_Loss: 2.69869065284729 *\n",
      "262:Epoch: 3, Train_Loss: 2.7062273025512695, Test_Loss: 2.6954565048217773 *\n",
      "263:Epoch: 3, Train_Loss: 2.7046992778778076, Test_Loss: 2.6942577362060547 *\n",
      "264:Epoch: 3, Train_Loss: 2.699507713317871, Test_Loss: 2.6930766105651855 *\n",
      "265:Epoch: 3, Train_Loss: 2.7186360359191895, Test_Loss: 2.6919474601745605 *\n",
      "266:Epoch: 3, Train_Loss: 2.696415424346924, Test_Loss: 2.6907265186309814 *\n",
      "267:Epoch: 3, Train_Loss: 2.692592144012451, Test_Loss: 2.696240186691284\n",
      "268:Epoch: 3, Train_Loss: 2.7489449977874756, Test_Loss: 2.6907753944396973 *\n",
      "269:Epoch: 3, Train_Loss: 2.7540123462677, Test_Loss: 2.6935875415802\n",
      "270:Epoch: 3, Train_Loss: 2.6973042488098145, Test_Loss: 2.692391872406006 *\n",
      "271:Epoch: 3, Train_Loss: 2.6891398429870605, Test_Loss: 2.689985990524292 *\n",
      "272:Epoch: 3, Train_Loss: 2.6953189373016357, Test_Loss: 2.7282309532165527\n",
      "273:Epoch: 3, Train_Loss: 2.7736990451812744, Test_Loss: 2.6895551681518555 *\n",
      "274:Epoch: 3, Train_Loss: 2.7468056678771973, Test_Loss: 3.0533320903778076\n",
      "275:Epoch: 3, Train_Loss: 2.7604823112487793, Test_Loss: 3.159245491027832\n",
      "276:Epoch: 3, Train_Loss: 2.716524362564087, Test_Loss: 2.8183188438415527 *\n",
      "277:Epoch: 3, Train_Loss: 2.742133140563965, Test_Loss: 2.693366527557373 *\n",
      "278:Epoch: 3, Train_Loss: 2.741879463195801, Test_Loss: 2.6907424926757812 *\n",
      "279:Epoch: 3, Train_Loss: 2.746577024459839, Test_Loss: 2.7029061317443848\n",
      "280:Epoch: 3, Train_Loss: 2.6833014488220215, Test_Loss: 2.8590381145477295\n",
      "281:Epoch: 3, Train_Loss: 2.7843871116638184, Test_Loss: 3.908891201019287\n",
      "282:Epoch: 3, Train_Loss: 2.689617872238159, Test_Loss: 3.822699546813965 *\n",
      "283:Epoch: 3, Train_Loss: 2.671372175216675, Test_Loss: 2.7127301692962646 *\n",
      "284:Epoch: 3, Train_Loss: 2.671602249145508, Test_Loss: 2.718583106994629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285:Epoch: 3, Train_Loss: 2.6693239212036133, Test_Loss: 2.6690175533294678 *\n",
      "286:Epoch: 3, Train_Loss: 2.667701244354248, Test_Loss: 2.6764636039733887\n",
      "287:Epoch: 3, Train_Loss: 2.665843963623047, Test_Loss: 2.6710474491119385 *\n",
      "288:Epoch: 3, Train_Loss: 4.207372188568115, Test_Loss: 2.6904613971710205\n",
      "289:Epoch: 3, Train_Loss: 6.075211524963379, Test_Loss: 2.7314133644104004\n",
      "290:Epoch: 3, Train_Loss: 2.657797336578369, Test_Loss: 2.669186592102051 *\n",
      "291:Epoch: 3, Train_Loss: 2.674699544906616, Test_Loss: 2.6703503131866455\n",
      "292:Epoch: 3, Train_Loss: 2.6692850589752197, Test_Loss: 2.7583982944488525\n",
      "293:Epoch: 3, Train_Loss: 2.6514103412628174, Test_Loss: 3.046435832977295\n",
      "294:Epoch: 3, Train_Loss: 2.6508326530456543, Test_Loss: 2.8716888427734375 *\n",
      "295:Epoch: 3, Train_Loss: 2.6493868827819824, Test_Loss: 2.6659023761749268 *\n",
      "296:Epoch: 3, Train_Loss: 2.6486971378326416, Test_Loss: 2.6532506942749023 *\n",
      "297:Epoch: 3, Train_Loss: 2.646303415298462, Test_Loss: 2.6524102687835693 *\n",
      "298:Epoch: 3, Train_Loss: 2.652076482772827, Test_Loss: 2.6514718532562256 *\n",
      "299:Epoch: 3, Train_Loss: 2.7376914024353027, Test_Loss: 2.6517622470855713\n",
      "300:Epoch: 3, Train_Loss: 2.7161083221435547, Test_Loss: 2.963200092315674\n",
      "Model saved at location ../Saver/model.ckpt at epoch 3\n",
      "301:Epoch: 3, Train_Loss: 2.7372751235961914, Test_Loss: 7.69720458984375\n",
      "302:Epoch: 3, Train_Loss: 2.6838254928588867, Test_Loss: 2.707641124725342 *\n",
      "303:Epoch: 3, Train_Loss: 2.638615608215332, Test_Loss: 2.6475045680999756 *\n",
      "304:Epoch: 3, Train_Loss: 2.820277690887451, Test_Loss: 2.640594959259033 *\n",
      "305:Epoch: 3, Train_Loss: 2.8650965690612793, Test_Loss: 2.646857738494873\n",
      "306:Epoch: 3, Train_Loss: 2.8469226360321045, Test_Loss: 2.6417577266693115 *\n",
      "307:Epoch: 3, Train_Loss: 2.7458510398864746, Test_Loss: 2.636533260345459 *\n",
      "308:Epoch: 3, Train_Loss: 2.632159471511841, Test_Loss: 2.637369155883789\n",
      "309:Epoch: 3, Train_Loss: 2.6302685737609863, Test_Loss: 2.6330063343048096 *\n",
      "310:Epoch: 3, Train_Loss: 2.6313390731811523, Test_Loss: 2.633697271347046\n",
      "311:Epoch: 3, Train_Loss: 2.6370463371276855, Test_Loss: 2.6321165561676025 *\n",
      "312:Epoch: 3, Train_Loss: 2.638260841369629, Test_Loss: 2.636047840118408\n",
      "313:Epoch: 3, Train_Loss: 2.6317508220672607, Test_Loss: 2.6371266841888428\n",
      "314:Epoch: 3, Train_Loss: 2.624673843383789, Test_Loss: 2.6363515853881836 *\n",
      "315:Epoch: 3, Train_Loss: 2.6224188804626465, Test_Loss: 2.635803461074829 *\n",
      "316:Epoch: 3, Train_Loss: 2.627751350402832, Test_Loss: 2.62483811378479 *\n",
      "317:Epoch: 3, Train_Loss: 2.664205551147461, Test_Loss: 2.621497631072998 *\n",
      "318:Epoch: 3, Train_Loss: 2.7850019931793213, Test_Loss: 2.6238744258880615\n",
      "319:Epoch: 3, Train_Loss: 2.7673499584198, Test_Loss: 2.622507095336914 *\n",
      "320:Epoch: 3, Train_Loss: 2.745452642440796, Test_Loss: 2.620871067047119 *\n",
      "321:Epoch: 3, Train_Loss: 2.7141709327697754, Test_Loss: 2.627761125564575\n",
      "322:Epoch: 3, Train_Loss: 2.7714390754699707, Test_Loss: 2.6235272884368896 *\n",
      "323:Epoch: 3, Train_Loss: 2.6834487915039062, Test_Loss: 2.637023687362671\n",
      "324:Epoch: 3, Train_Loss: 2.7614758014678955, Test_Loss: 2.6269896030426025 *\n",
      "325:Epoch: 3, Train_Loss: 2.77597975730896, Test_Loss: 2.6183483600616455 *\n",
      "326:Epoch: 3, Train_Loss: 2.9403233528137207, Test_Loss: 2.6157491207122803 *\n",
      "327:Epoch: 3, Train_Loss: 2.6170482635498047, Test_Loss: 2.6150479316711426 *\n",
      "328:Epoch: 3, Train_Loss: 2.6400034427642822, Test_Loss: 2.609510660171509 *\n",
      "329:Epoch: 3, Train_Loss: 5.628744125366211, Test_Loss: 2.6091859340667725 *\n",
      "330:Epoch: 3, Train_Loss: 2.8578951358795166, Test_Loss: 2.617982864379883\n",
      "331:Epoch: 3, Train_Loss: 2.6422696113586426, Test_Loss: 2.662442922592163\n",
      "332:Epoch: 3, Train_Loss: 2.6533849239349365, Test_Loss: 5.016834259033203\n",
      "333:Epoch: 3, Train_Loss: 2.6534976959228516, Test_Loss: 5.711676597595215\n",
      "334:Epoch: 3, Train_Loss: 2.6110122203826904, Test_Loss: 2.6025984287261963 *\n",
      "335:Epoch: 3, Train_Loss: 2.5994956493377686, Test_Loss: 2.596950054168701 *\n",
      "336:Epoch: 3, Train_Loss: 2.670541286468506, Test_Loss: 2.6434013843536377\n",
      "337:Epoch: 3, Train_Loss: 2.7272093296051025, Test_Loss: 2.644439697265625\n",
      "338:Epoch: 3, Train_Loss: 2.7008774280548096, Test_Loss: 2.643468141555786 *\n",
      "339:Epoch: 3, Train_Loss: 2.6698179244995117, Test_Loss: 2.637075901031494 *\n",
      "340:Epoch: 3, Train_Loss: 2.676968812942505, Test_Loss: 2.708752393722534\n",
      "341:Epoch: 3, Train_Loss: 2.615342617034912, Test_Loss: 2.592855215072632 *\n",
      "342:Epoch: 3, Train_Loss: 2.6223537921905518, Test_Loss: 2.6079161167144775\n",
      "343:Epoch: 3, Train_Loss: 2.592715263366699, Test_Loss: 2.6066296100616455 *\n",
      "344:Epoch: 3, Train_Loss: 2.62337589263916, Test_Loss: 2.6042869091033936 *\n",
      "345:Epoch: 3, Train_Loss: 2.6065356731414795, Test_Loss: 2.5907063484191895 *\n",
      "346:Epoch: 3, Train_Loss: 2.5821874141693115, Test_Loss: 2.6746654510498047\n",
      "347:Epoch: 3, Train_Loss: 2.5994679927825928, Test_Loss: 2.6779727935791016\n",
      "348:Epoch: 3, Train_Loss: 2.6301350593566895, Test_Loss: 2.651808023452759 *\n",
      "349:Epoch: 3, Train_Loss: 2.6171910762786865, Test_Loss: 2.6705431938171387\n",
      "350:Epoch: 3, Train_Loss: 2.577925682067871, Test_Loss: 2.601041793823242 *\n",
      "351:Epoch: 3, Train_Loss: 2.5755836963653564, Test_Loss: 2.618086814880371\n",
      "352:Epoch: 3, Train_Loss: 2.574301242828369, Test_Loss: 2.600576162338257 *\n",
      "353:Epoch: 3, Train_Loss: 2.5725557804107666, Test_Loss: 2.598684310913086 *\n",
      "354:Epoch: 3, Train_Loss: 2.57196044921875, Test_Loss: 2.5996265411376953\n",
      "355:Epoch: 3, Train_Loss: 2.571779489517212, Test_Loss: 2.6004106998443604\n",
      "356:Epoch: 3, Train_Loss: 2.570122718811035, Test_Loss: 2.5981690883636475 *\n",
      "357:Epoch: 3, Train_Loss: 2.5685901641845703, Test_Loss: 2.594850540161133 *\n",
      "358:Epoch: 3, Train_Loss: 2.5678319931030273, Test_Loss: 2.605429172515869\n",
      "359:Epoch: 3, Train_Loss: 2.566009998321533, Test_Loss: 2.5969207286834717 *\n",
      "360:Epoch: 3, Train_Loss: 2.5675418376922607, Test_Loss: 2.5926218032836914 *\n",
      "361:Epoch: 3, Train_Loss: 2.5764472484588623, Test_Loss: 2.5665669441223145 *\n",
      "362:Epoch: 3, Train_Loss: 2.576319694519043, Test_Loss: 2.6018238067626953\n",
      "363:Epoch: 3, Train_Loss: 2.578836679458618, Test_Loss: 2.6437594890594482\n",
      "364:Epoch: 3, Train_Loss: 2.5713837146759033, Test_Loss: 2.5863139629364014 *\n",
      "365:Epoch: 3, Train_Loss: 2.5669808387756348, Test_Loss: 3.1389153003692627\n",
      "366:Epoch: 3, Train_Loss: 2.55680775642395, Test_Loss: 3.1078264713287354 *\n",
      "367:Epoch: 3, Train_Loss: 2.5563101768493652, Test_Loss: 2.7221319675445557 *\n",
      "368:Epoch: 3, Train_Loss: 2.571491241455078, Test_Loss: 2.5962491035461426 *\n",
      "369:Epoch: 3, Train_Loss: 2.57682204246521, Test_Loss: 2.5773918628692627 *\n",
      "370:Epoch: 3, Train_Loss: 2.5517160892486572, Test_Loss: 2.581648111343384\n",
      "371:Epoch: 3, Train_Loss: 2.550847291946411, Test_Loss: 2.830889940261841\n",
      "372:Epoch: 3, Train_Loss: 2.5490763187408447, Test_Loss: 3.907487154006958\n",
      "373:Epoch: 3, Train_Loss: 2.612877607345581, Test_Loss: 3.3306961059570312 *\n",
      "374:Epoch: 3, Train_Loss: 2.585881471633911, Test_Loss: 2.6202199459075928 *\n",
      "375:Epoch: 3, Train_Loss: 2.5937113761901855, Test_Loss: 2.5885400772094727 *\n",
      "376:Epoch: 3, Train_Loss: 2.5555639266967773, Test_Loss: 2.5479207038879395 *\n",
      "377:Epoch: 3, Train_Loss: 2.5463614463806152, Test_Loss: 2.5451345443725586 *\n",
      "378:Epoch: 3, Train_Loss: 2.6009035110473633, Test_Loss: 2.5496163368225098\n",
      "379:Epoch: 3, Train_Loss: 2.5417160987854004, Test_Loss: 2.5667576789855957\n",
      "380:Epoch: 3, Train_Loss: 2.5543570518493652, Test_Loss: 2.606801986694336\n",
      "381:Epoch: 3, Train_Loss: 2.5720252990722656, Test_Loss: 2.5384230613708496 *\n",
      "382:Epoch: 3, Train_Loss: 2.559121608734131, Test_Loss: 2.5778520107269287\n",
      "383:Epoch: 3, Train_Loss: 2.6826882362365723, Test_Loss: 2.6641881465911865\n",
      "384:Epoch: 3, Train_Loss: 2.6171140670776367, Test_Loss: 2.8959174156188965\n",
      "385:Epoch: 3, Train_Loss: 2.5718348026275635, Test_Loss: 2.7466623783111572 *\n",
      "386:Epoch: 3, Train_Loss: 2.540522336959839, Test_Loss: 2.5413732528686523 *\n",
      "387:Epoch: 3, Train_Loss: 2.5568230152130127, Test_Loss: 2.533968210220337 *\n",
      "388:Epoch: 3, Train_Loss: 2.5316152572631836, Test_Loss: 2.532761335372925 *\n",
      "389:Epoch: 3, Train_Loss: 2.5338521003723145, Test_Loss: 2.5314483642578125 *\n",
      "390:Epoch: 3, Train_Loss: 2.5377414226531982, Test_Loss: 2.5358452796936035\n",
      "391:Epoch: 3, Train_Loss: 2.5413191318511963, Test_Loss: 3.6526780128479004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392:Epoch: 3, Train_Loss: 2.57226824760437, Test_Loss: 6.882898807525635\n",
      "393:Epoch: 3, Train_Loss: 2.585327625274658, Test_Loss: 2.538328170776367 *\n",
      "394:Epoch: 3, Train_Loss: 2.561751127243042, Test_Loss: 2.525355100631714 *\n",
      "395:Epoch: 3, Train_Loss: 2.5663371086120605, Test_Loss: 2.5235509872436523 *\n",
      "396:Epoch: 3, Train_Loss: 2.547530174255371, Test_Loss: 2.527188777923584\n",
      "397:Epoch: 3, Train_Loss: 2.5303750038146973, Test_Loss: 2.521353006362915 *\n",
      "398:Epoch: 3, Train_Loss: 2.638497829437256, Test_Loss: 2.5199501514434814 *\n",
      "399:Epoch: 3, Train_Loss: 2.7235491275787354, Test_Loss: 2.518550157546997 *\n",
      "400:Epoch: 3, Train_Loss: 2.5152907371520996, Test_Loss: 2.515953540802002 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 3\n",
      "401:Epoch: 3, Train_Loss: 2.5460190773010254, Test_Loss: 2.5156607627868652 *\n",
      "402:Epoch: 3, Train_Loss: 2.511951446533203, Test_Loss: 2.5124824047088623 *\n",
      "403:Epoch: 3, Train_Loss: 2.5102179050445557, Test_Loss: 2.5159153938293457\n",
      "404:Epoch: 3, Train_Loss: 2.5103962421417236, Test_Loss: 2.535654067993164\n",
      "405:Epoch: 3, Train_Loss: 2.509364128112793, Test_Loss: 2.5332844257354736 *\n",
      "406:Epoch: 3, Train_Loss: 2.5249855518341064, Test_Loss: 2.5157809257507324 *\n",
      "407:Epoch: 3, Train_Loss: 2.5333850383758545, Test_Loss: 2.5061357021331787 *\n",
      "408:Epoch: 3, Train_Loss: 2.5205602645874023, Test_Loss: 2.505425214767456 *\n",
      "409:Epoch: 3, Train_Loss: 2.5200064182281494, Test_Loss: 2.5038695335388184 *\n",
      "410:Epoch: 3, Train_Loss: 2.530217409133911, Test_Loss: 2.503031015396118 *\n",
      "411:Epoch: 3, Train_Loss: 2.50219464302063, Test_Loss: 2.501645565032959 *\n",
      "412:Epoch: 3, Train_Loss: 2.502009391784668, Test_Loss: 2.500826597213745 *\n",
      "413:Epoch: 3, Train_Loss: 2.498065710067749, Test_Loss: 2.4983110427856445 *\n",
      "414:Epoch: 3, Train_Loss: 2.5237460136413574, Test_Loss: 2.5000972747802734\n",
      "415:Epoch: 3, Train_Loss: 2.524522304534912, Test_Loss: 2.4967782497406006 *\n",
      "416:Epoch: 3, Train_Loss: 2.5080313682556152, Test_Loss: 2.495497226715088 *\n",
      "417:Epoch: 3, Train_Loss: 2.5081896781921387, Test_Loss: 2.4941258430480957 *\n",
      "418:Epoch: 3, Train_Loss: 2.5579731464385986, Test_Loss: 2.493511438369751 *\n",
      "419:Epoch: 3, Train_Loss: 2.5471131801605225, Test_Loss: 2.4916391372680664 *\n",
      "420:Epoch: 3, Train_Loss: 2.509005069732666, Test_Loss: 2.4906625747680664 *\n",
      "421:Epoch: 3, Train_Loss: 2.503255605697632, Test_Loss: 2.5171236991882324\n",
      "422:Epoch: 3, Train_Loss: 2.505462646484375, Test_Loss: 2.530881404876709\n",
      "423:Epoch: 3, Train_Loss: 2.496333599090576, Test_Loss: 5.87993049621582\n",
      "424:Epoch: 3, Train_Loss: 2.4962267875671387, Test_Loss: 4.608865737915039 *\n",
      "425:Epoch: 3, Train_Loss: 2.510702133178711, Test_Loss: 2.484736442565918 *\n",
      "426:Epoch: 3, Train_Loss: 2.538405418395996, Test_Loss: 2.4853086471557617\n",
      "427:Epoch: 3, Train_Loss: 4.689495086669922, Test_Loss: 2.5410614013671875\n",
      "428:Epoch: 3, Train_Loss: 5.721305847167969, Test_Loss: 2.5417110919952393\n",
      "429:Epoch: 3, Train_Loss: 2.4897618293762207, Test_Loss: 2.5085649490356445 *\n",
      "430:Epoch: 3, Train_Loss: 2.4849281311035156, Test_Loss: 2.5461018085479736\n",
      "431:Epoch: 3, Train_Loss: 2.5273513793945312, Test_Loss: 2.578695774078369\n",
      "432:Epoch: 3, Train_Loss: 2.656747341156006, Test_Loss: 2.4765801429748535 *\n",
      "433:Epoch: 3, Train_Loss: 2.501523494720459, Test_Loss: 2.504157304763794\n",
      "434:Epoch: 3, Train_Loss: 2.478644609451294, Test_Loss: 2.490917444229126 *\n",
      "435:Epoch: 3, Train_Loss: 2.4783551692962646, Test_Loss: 2.4855356216430664 *\n",
      "436:Epoch: 3, Train_Loss: 2.5404953956604004, Test_Loss: 2.474574565887451 *\n",
      "437:Epoch: 3, Train_Loss: 2.476679801940918, Test_Loss: 2.578890085220337\n",
      "438:Epoch: 3, Train_Loss: 2.4837982654571533, Test_Loss: 2.5259413719177246 *\n",
      "439:Epoch: 3, Train_Loss: 3.3219246864318848, Test_Loss: 2.5649147033691406\n",
      "440:Epoch: 3, Train_Loss: 3.8511292934417725, Test_Loss: 2.545042037963867 *\n",
      "441:Epoch: 3, Train_Loss: 3.0735483169555664, Test_Loss: 2.5001747608184814 *\n",
      "442:Epoch: 3, Train_Loss: 2.5862479209899902, Test_Loss: 2.493471622467041 *\n",
      "443:Epoch: 3, Train_Loss: 3.3009142875671387, Test_Loss: 2.4880764484405518 *\n",
      "444:Epoch: 3, Train_Loss: 4.803948402404785, Test_Loss: 2.48386287689209 *\n",
      "445:Epoch: 3, Train_Loss: 2.840522289276123, Test_Loss: 2.4861278533935547\n",
      "446:Epoch: 3, Train_Loss: 2.512478828430176, Test_Loss: 2.4833643436431885 *\n",
      "447:Epoch: 3, Train_Loss: 2.48093843460083, Test_Loss: 2.479628324508667 *\n",
      "448:Epoch: 3, Train_Loss: 3.9171533584594727, Test_Loss: 2.4723665714263916 *\n",
      "449:Epoch: 3, Train_Loss: 3.990279197692871, Test_Loss: 2.4861912727355957\n",
      "450:Epoch: 3, Train_Loss: 2.609154224395752, Test_Loss: 2.4774436950683594 *\n",
      "451:Epoch: 3, Train_Loss: 2.4735188484191895, Test_Loss: 2.4647223949432373 *\n",
      "452:Epoch: 3, Train_Loss: 2.4521236419677734, Test_Loss: 2.457724094390869 *\n",
      "453:Epoch: 3, Train_Loss: 3.1025948524475098, Test_Loss: 2.484809398651123\n",
      "454:Epoch: 3, Train_Loss: 2.580807685852051, Test_Loss: 2.495651960372925\n",
      "1:Epoch: 4, Train_Loss: 2.480443000793457, Test_Loss: 2.540869951248169 *\n",
      "2:Epoch: 4, Train_Loss: 2.4779770374298096, Test_Loss: 3.0351791381835938\n",
      "3:Epoch: 4, Train_Loss: 2.603072166442871, Test_Loss: 2.8980891704559326 *\n",
      "4:Epoch: 4, Train_Loss: 2.6069717407226562, Test_Loss: 2.5782268047332764 *\n",
      "5:Epoch: 4, Train_Loss: 2.589576005935669, Test_Loss: 2.4732038974761963 *\n",
      "6:Epoch: 4, Train_Loss: 2.8124582767486572, Test_Loss: 2.4599578380584717 *\n",
      "7:Epoch: 4, Train_Loss: 2.5243752002716064, Test_Loss: 2.491793394088745\n",
      "8:Epoch: 4, Train_Loss: 2.5241713523864746, Test_Loss: 2.877051591873169\n",
      "9:Epoch: 4, Train_Loss: 2.6922476291656494, Test_Loss: 3.729501962661743\n",
      "10:Epoch: 4, Train_Loss: 2.844674825668335, Test_Loss: 2.979731321334839 *\n",
      "11:Epoch: 4, Train_Loss: 2.8408162593841553, Test_Loss: 2.518301248550415 *\n",
      "12:Epoch: 4, Train_Loss: 2.4927659034729004, Test_Loss: 2.446837902069092 *\n",
      "13:Epoch: 4, Train_Loss: 2.560657262802124, Test_Loss: 2.4425621032714844 *\n",
      "14:Epoch: 4, Train_Loss: 2.5925776958465576, Test_Loss: 2.4375617504119873 *\n",
      "15:Epoch: 4, Train_Loss: 2.467343330383301, Test_Loss: 2.4435503482818604\n",
      "16:Epoch: 4, Train_Loss: 2.451390266418457, Test_Loss: 2.447856903076172\n",
      "17:Epoch: 4, Train_Loss: 2.429440975189209, Test_Loss: 2.4830288887023926\n",
      "18:Epoch: 4, Train_Loss: 2.4305036067962646, Test_Loss: 2.430576801300049 *\n",
      "19:Epoch: 4, Train_Loss: 2.429891347885132, Test_Loss: 2.4930996894836426\n",
      "20:Epoch: 4, Train_Loss: 2.430673837661743, Test_Loss: 2.599134683609009\n",
      "21:Epoch: 4, Train_Loss: 2.5104572772979736, Test_Loss: 2.736177921295166\n",
      "22:Epoch: 4, Train_Loss: 2.497819423675537, Test_Loss: 2.656578779220581 *\n",
      "23:Epoch: 4, Train_Loss: 2.5239996910095215, Test_Loss: 2.4267311096191406 *\n",
      "24:Epoch: 4, Train_Loss: 2.5247161388397217, Test_Loss: 2.4230756759643555 *\n",
      "25:Epoch: 4, Train_Loss: 2.9222655296325684, Test_Loss: 2.4218087196350098 *\n",
      "26:Epoch: 4, Train_Loss: 2.422588586807251, Test_Loss: 2.420783758163452 *\n",
      "27:Epoch: 4, Train_Loss: 2.4447543621063232, Test_Loss: 2.438133955001831\n",
      "28:Epoch: 4, Train_Loss: 2.5860986709594727, Test_Loss: 4.719272136688232\n",
      "29:Epoch: 4, Train_Loss: 2.918120861053467, Test_Loss: 5.489153861999512\n",
      "30:Epoch: 4, Train_Loss: 2.7555127143859863, Test_Loss: 2.421603202819824 *\n",
      "31:Epoch: 4, Train_Loss: 2.4144539833068848, Test_Loss: 2.4143693447113037 *\n",
      "32:Epoch: 4, Train_Loss: 2.4893274307250977, Test_Loss: 2.411832809448242 *\n",
      "33:Epoch: 4, Train_Loss: 3.055617570877075, Test_Loss: 2.4182353019714355\n",
      "34:Epoch: 4, Train_Loss: 2.982008218765259, Test_Loss: 2.4118237495422363 *\n",
      "35:Epoch: 4, Train_Loss: 2.4853479862213135, Test_Loss: 2.413815498352051\n",
      "36:Epoch: 4, Train_Loss: 2.4335198402404785, Test_Loss: 2.4096333980560303 *\n",
      "37:Epoch: 4, Train_Loss: 2.412372350692749, Test_Loss: 2.412543296813965\n",
      "38:Epoch: 4, Train_Loss: 3.4351344108581543, Test_Loss: 2.4112138748168945 *\n",
      "39:Epoch: 4, Train_Loss: 3.5088658332824707, Test_Loss: 2.4084668159484863 *\n",
      "40:Epoch: 4, Train_Loss: 2.42182993888855, Test_Loss: 2.413259267807007\n",
      "41:Epoch: 4, Train_Loss: 2.4256949424743652, Test_Loss: 2.415999412536621\n",
      "42:Epoch: 4, Train_Loss: 2.4001545906066895, Test_Loss: 2.4078571796417236 *\n",
      "43:Epoch: 4, Train_Loss: 2.4206936359405518, Test_Loss: 2.4063854217529297 *\n",
      "44:Epoch: 4, Train_Loss: 2.846754312515259, Test_Loss: 2.3980019092559814 *\n",
      "45:Epoch: 4, Train_Loss: 2.418820381164551, Test_Loss: 2.400730848312378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46:Epoch: 4, Train_Loss: 2.632443428039551, Test_Loss: 2.397346019744873 *\n",
      "47:Epoch: 4, Train_Loss: 2.5187008380889893, Test_Loss: 2.3976051807403564\n",
      "48:Epoch: 4, Train_Loss: 2.416443347930908, Test_Loss: 2.396422863006592 *\n",
      "49:Epoch: 4, Train_Loss: 2.446329355239868, Test_Loss: 2.395629405975342 *\n",
      "50:Epoch: 4, Train_Loss: 2.6005094051361084, Test_Loss: 2.3984899520874023\n",
      "51:Epoch: 4, Train_Loss: 2.583216667175293, Test_Loss: 2.4095041751861572\n",
      "52:Epoch: 4, Train_Loss: 2.400545120239258, Test_Loss: 2.397493839263916 *\n",
      "53:Epoch: 4, Train_Loss: 2.5174803733825684, Test_Loss: 2.3972058296203613 *\n",
      "54:Epoch: 4, Train_Loss: 2.6039657592773438, Test_Loss: 2.389768362045288 *\n",
      "55:Epoch: 4, Train_Loss: 2.4324240684509277, Test_Loss: 2.391400098800659\n",
      "56:Epoch: 4, Train_Loss: 2.4513535499572754, Test_Loss: 2.3909432888031006 *\n",
      "57:Epoch: 4, Train_Loss: 2.39328670501709, Test_Loss: 2.385070562362671 *\n",
      "58:Epoch: 4, Train_Loss: 2.422637939453125, Test_Loss: 2.449876546859741\n",
      "59:Epoch: 4, Train_Loss: 2.394139051437378, Test_Loss: 2.4317054748535156 *\n",
      "60:Epoch: 4, Train_Loss: 2.9303786754608154, Test_Loss: 7.184630393981934\n",
      "61:Epoch: 4, Train_Loss: 2.6203649044036865, Test_Loss: 3.219566583633423 *\n",
      "62:Epoch: 4, Train_Loss: 2.986473798751831, Test_Loss: 2.390131711959839 *\n",
      "63:Epoch: 4, Train_Loss: 3.0238852500915527, Test_Loss: 2.4179561138153076\n",
      "64:Epoch: 4, Train_Loss: 2.547459363937378, Test_Loss: 2.414485454559326 *\n",
      "65:Epoch: 4, Train_Loss: 2.7855823040008545, Test_Loss: 2.401230812072754 *\n",
      "66:Epoch: 4, Train_Loss: 2.4439785480499268, Test_Loss: 2.381406784057617 *\n",
      "67:Epoch: 4, Train_Loss: 2.378782033920288, Test_Loss: 2.441575765609741\n",
      "68:Epoch: 4, Train_Loss: 2.37701416015625, Test_Loss: 2.416734218597412 *\n",
      "69:Epoch: 4, Train_Loss: 2.4633069038391113, Test_Loss: 2.3969151973724365 *\n",
      "70:Epoch: 4, Train_Loss: 2.6307742595672607, Test_Loss: 2.3863131999969482 *\n",
      "71:Epoch: 4, Train_Loss: 2.9820427894592285, Test_Loss: 2.4605016708374023\n",
      "72:Epoch: 4, Train_Loss: 3.11004638671875, Test_Loss: 2.4029483795166016 *\n",
      "73:Epoch: 4, Train_Loss: 3.900735855102539, Test_Loss: 2.4082765579223633\n",
      "74:Epoch: 4, Train_Loss: 2.9595346450805664, Test_Loss: 2.412336826324463\n",
      "75:Epoch: 4, Train_Loss: 2.794196844100952, Test_Loss: 2.412700653076172\n",
      "76:Epoch: 4, Train_Loss: 2.396702527999878, Test_Loss: 2.4006097316741943 *\n",
      "77:Epoch: 4, Train_Loss: 2.3742878437042236, Test_Loss: 2.3842029571533203 *\n",
      "78:Epoch: 4, Train_Loss: 2.946092128753662, Test_Loss: 2.523736000061035\n",
      "79:Epoch: 4, Train_Loss: 3.7294998168945312, Test_Loss: 2.3823649883270264 *\n",
      "80:Epoch: 4, Train_Loss: 2.452439546585083, Test_Loss: 2.3674986362457275 *\n",
      "81:Epoch: 4, Train_Loss: 2.4854490756988525, Test_Loss: 2.3700742721557617\n",
      "82:Epoch: 4, Train_Loss: 2.3937063217163086, Test_Loss: 2.3715577125549316\n",
      "83:Epoch: 4, Train_Loss: 2.4729905128479004, Test_Loss: 2.3730568885803223\n",
      "84:Epoch: 4, Train_Loss: 2.640019178390503, Test_Loss: 2.3724205493927 *\n",
      "85:Epoch: 4, Train_Loss: 2.8591787815093994, Test_Loss: 2.3825759887695312\n",
      "86:Epoch: 4, Train_Loss: 3.1781530380249023, Test_Loss: 2.3646936416625977 *\n",
      "87:Epoch: 4, Train_Loss: 2.644913911819458, Test_Loss: 2.35798716545105 *\n",
      "88:Epoch: 4, Train_Loss: 2.364727258682251, Test_Loss: 2.3724424839019775\n",
      "89:Epoch: 4, Train_Loss: 2.353468894958496, Test_Loss: 2.3933870792388916\n",
      "90:Epoch: 4, Train_Loss: 2.3502988815307617, Test_Loss: 2.3502089977264404 *\n",
      "91:Epoch: 4, Train_Loss: 2.3958373069763184, Test_Loss: 2.367713212966919\n",
      "92:Epoch: 4, Train_Loss: 2.346285581588745, Test_Loss: 2.4852287769317627\n",
      "93:Epoch: 4, Train_Loss: 2.370307683944702, Test_Loss: 2.7053816318511963\n",
      "94:Epoch: 4, Train_Loss: 18.50574493408203, Test_Loss: 2.6297786235809326 *\n",
      "95:Epoch: 4, Train_Loss: 3.3850741386413574, Test_Loss: 2.462423324584961 *\n",
      "96:Epoch: 4, Train_Loss: 3.600919246673584, Test_Loss: 2.3517634868621826 *\n",
      "97:Epoch: 4, Train_Loss: 4.911017894744873, Test_Loss: 2.3676321506500244\n",
      "98:Epoch: 4, Train_Loss: 2.3608763217926025, Test_Loss: 2.3558363914489746 *\n",
      "99:Epoch: 4, Train_Loss: 2.3649702072143555, Test_Loss: 2.679983615875244\n",
      "100:Epoch: 4, Train_Loss: 3.9079596996307373, Test_Loss: 3.1030843257904053\n",
      "Model saved at location ../Saver/model.ckpt at epoch 4\n",
      "101:Epoch: 4, Train_Loss: 12.808093070983887, Test_Loss: 2.5629491806030273 *\n",
      "102:Epoch: 4, Train_Loss: 2.759608268737793, Test_Loss: 2.4057912826538086 *\n",
      "103:Epoch: 4, Train_Loss: 2.35990309715271, Test_Loss: 2.3649938106536865 *\n",
      "104:Epoch: 4, Train_Loss: 7.499070167541504, Test_Loss: 2.4039223194122314\n",
      "105:Epoch: 4, Train_Loss: 3.084580183029175, Test_Loss: 2.4092745780944824\n",
      "106:Epoch: 4, Train_Loss: 2.3956704139709473, Test_Loss: 2.399955987930298 *\n",
      "107:Epoch: 4, Train_Loss: 2.382215738296509, Test_Loss: 2.451404094696045\n",
      "108:Epoch: 4, Train_Loss: 2.378235340118408, Test_Loss: 2.4841055870056152\n",
      "109:Epoch: 4, Train_Loss: 2.360369920730591, Test_Loss: 2.3850653171539307 *\n",
      "110:Epoch: 4, Train_Loss: 2.3523874282836914, Test_Loss: 2.369717836380005 *\n",
      "111:Epoch: 4, Train_Loss: 2.3476035594940186, Test_Loss: 2.645944595336914\n",
      "112:Epoch: 4, Train_Loss: 2.337895631790161, Test_Loss: 2.3725578784942627 *\n",
      "113:Epoch: 4, Train_Loss: 2.3330612182617188, Test_Loss: 2.4517030715942383\n",
      "114:Epoch: 4, Train_Loss: 2.3378894329071045, Test_Loss: 2.328237533569336 *\n",
      "115:Epoch: 4, Train_Loss: 2.3359386920928955, Test_Loss: 2.324969530105591 *\n",
      "116:Epoch: 4, Train_Loss: 2.3239917755126953, Test_Loss: 2.321495532989502 *\n",
      "117:Epoch: 4, Train_Loss: 2.356572151184082, Test_Loss: 2.319242477416992 *\n",
      "118:Epoch: 4, Train_Loss: 2.364057779312134, Test_Loss: 2.324021100997925\n",
      "119:Epoch: 4, Train_Loss: 2.3433780670166016, Test_Loss: 6.2731475830078125\n",
      "120:Epoch: 4, Train_Loss: 2.318812847137451, Test_Loss: 4.130927085876465 *\n",
      "121:Epoch: 4, Train_Loss: 2.316376209259033, Test_Loss: 2.321345806121826 *\n",
      "122:Epoch: 4, Train_Loss: 2.3143670558929443, Test_Loss: 2.317131519317627 *\n",
      "123:Epoch: 4, Train_Loss: 2.313966989517212, Test_Loss: 2.3132777214050293 *\n",
      "124:Epoch: 4, Train_Loss: 2.311119318008423, Test_Loss: 2.3157246112823486\n",
      "125:Epoch: 4, Train_Loss: 2.309675931930542, Test_Loss: 2.314598798751831 *\n",
      "126:Epoch: 4, Train_Loss: 2.3086910247802734, Test_Loss: 2.320296049118042\n",
      "127:Epoch: 4, Train_Loss: 2.3077046871185303, Test_Loss: 2.313220739364624 *\n",
      "128:Epoch: 4, Train_Loss: 2.3071107864379883, Test_Loss: 2.3167123794555664\n",
      "129:Epoch: 4, Train_Loss: 2.305725336074829, Test_Loss: 2.317223310470581\n",
      "130:Epoch: 4, Train_Loss: 2.3062119483947754, Test_Loss: 2.31341290473938 *\n",
      "131:Epoch: 4, Train_Loss: 2.316283702850342, Test_Loss: 2.313673734664917\n",
      "132:Epoch: 4, Train_Loss: 2.3377773761749268, Test_Loss: 2.319674491882324\n",
      "133:Epoch: 4, Train_Loss: 2.318485975265503, Test_Loss: 2.312448740005493 *\n",
      "134:Epoch: 4, Train_Loss: 2.313387155532837, Test_Loss: 2.312547206878662\n",
      "135:Epoch: 4, Train_Loss: 2.315192699432373, Test_Loss: 2.304025173187256 *\n",
      "136:Epoch: 4, Train_Loss: 11.319973945617676, Test_Loss: 2.309666633605957\n",
      "137:Epoch: 4, Train_Loss: 2.4679203033447266, Test_Loss: 2.304527997970581 *\n",
      "138:Epoch: 4, Train_Loss: 2.3024866580963135, Test_Loss: 2.3101253509521484\n",
      "139:Epoch: 4, Train_Loss: 2.312289237976074, Test_Loss: 2.322199821472168\n",
      "140:Epoch: 4, Train_Loss: 2.3158364295959473, Test_Loss: 2.3117666244506836 *\n",
      "141:Epoch: 4, Train_Loss: 2.311457633972168, Test_Loss: 2.31182861328125\n",
      "142:Epoch: 4, Train_Loss: 2.3118221759796143, Test_Loss: 2.3234951496124268\n",
      "143:Epoch: 4, Train_Loss: 2.3124876022338867, Test_Loss: 2.3172860145568848 *\n",
      "144:Epoch: 4, Train_Loss: 2.4930927753448486, Test_Loss: 2.317683458328247\n",
      "145:Epoch: 4, Train_Loss: 2.5438592433929443, Test_Loss: 2.310776948928833 *\n",
      "146:Epoch: 4, Train_Loss: 2.49222731590271, Test_Loss: 2.3110053539276123\n",
      "147:Epoch: 4, Train_Loss: 2.3126797676086426, Test_Loss: 2.3091726303100586 *\n",
      "148:Epoch: 4, Train_Loss: 2.426703929901123, Test_Loss: 2.3093173503875732\n",
      "149:Epoch: 4, Train_Loss: 2.4035096168518066, Test_Loss: 2.3599088191986084\n",
      "150:Epoch: 4, Train_Loss: 2.456120729446411, Test_Loss: 2.337965965270996 *\n",
      "151:Epoch: 4, Train_Loss: 2.4368319511413574, Test_Loss: 8.073265075683594\n",
      "152:Epoch: 4, Train_Loss: 2.419644355773926, Test_Loss: 2.408317804336548 *\n",
      "153:Epoch: 4, Train_Loss: 2.317744255065918, Test_Loss: 2.2868380546569824 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154:Epoch: 4, Train_Loss: 2.306450366973877, Test_Loss: 2.300152540206909\n",
      "155:Epoch: 4, Train_Loss: 2.350856304168701, Test_Loss: 2.31657338142395\n",
      "156:Epoch: 4, Train_Loss: 2.305250644683838, Test_Loss: 2.3171072006225586\n",
      "157:Epoch: 4, Train_Loss: 2.284602403640747, Test_Loss: 2.284658432006836 *\n",
      "158:Epoch: 4, Train_Loss: 2.2835326194763184, Test_Loss: 2.432243585586548\n",
      "159:Epoch: 4, Train_Loss: 2.281831741333008, Test_Loss: 2.3562653064727783 *\n",
      "160:Epoch: 4, Train_Loss: 2.509719133377075, Test_Loss: 2.2772231101989746 *\n",
      "161:Epoch: 4, Train_Loss: 7.703246116638184, Test_Loss: 2.3445425033569336\n",
      "162:Epoch: 4, Train_Loss: 2.2885916233062744, Test_Loss: 2.2781155109405518 *\n",
      "163:Epoch: 4, Train_Loss: 2.308889865875244, Test_Loss: 2.3006157875061035\n",
      "164:Epoch: 4, Train_Loss: 2.3281567096710205, Test_Loss: 2.2895421981811523 *\n",
      "165:Epoch: 4, Train_Loss: 2.3122334480285645, Test_Loss: 2.318686008453369\n",
      "166:Epoch: 4, Train_Loss: 2.2937378883361816, Test_Loss: 2.3437089920043945\n",
      "167:Epoch: 4, Train_Loss: 2.2835965156555176, Test_Loss: 2.448235511779785\n",
      "168:Epoch: 4, Train_Loss: 2.290330171585083, Test_Loss: 2.3558459281921387 *\n",
      "169:Epoch: 4, Train_Loss: 2.284336805343628, Test_Loss: 2.2792956829071045 *\n",
      "170:Epoch: 4, Train_Loss: 2.2792913913726807, Test_Loss: 2.2676241397857666 *\n",
      "171:Epoch: 4, Train_Loss: 2.2939603328704834, Test_Loss: 2.2651429176330566 *\n",
      "172:Epoch: 4, Train_Loss: 2.280193328857422, Test_Loss: 2.2643978595733643 *\n",
      "173:Epoch: 4, Train_Loss: 2.273803234100342, Test_Loss: 2.2639355659484863 *\n",
      "174:Epoch: 4, Train_Loss: 2.293177366256714, Test_Loss: 2.262968063354492 *\n",
      "175:Epoch: 4, Train_Loss: 2.2679429054260254, Test_Loss: 2.262519598007202 *\n",
      "176:Epoch: 4, Train_Loss: 2.265204668045044, Test_Loss: 2.261646032333374 *\n",
      "177:Epoch: 4, Train_Loss: 2.307013511657715, Test_Loss: 2.266078472137451\n",
      "178:Epoch: 4, Train_Loss: 2.3320889472961426, Test_Loss: 2.264136552810669 *\n",
      "179:Epoch: 4, Train_Loss: 2.2856574058532715, Test_Loss: 2.267814874649048\n",
      "180:Epoch: 4, Train_Loss: 2.263313055038452, Test_Loss: 2.2645418643951416 *\n",
      "181:Epoch: 4, Train_Loss: 2.263979911804199, Test_Loss: 2.292173147201538\n",
      "182:Epoch: 4, Train_Loss: 2.3282968997955322, Test_Loss: 2.274164915084839 *\n",
      "183:Epoch: 4, Train_Loss: 2.331479549407959, Test_Loss: 2.492055892944336\n",
      "184:Epoch: 4, Train_Loss: 2.331690549850464, Test_Loss: 2.752203941345215\n",
      "185:Epoch: 4, Train_Loss: 2.2793946266174316, Test_Loss: 2.4485507011413574 *\n",
      "186:Epoch: 4, Train_Loss: 2.301103115081787, Test_Loss: 2.304314374923706 *\n",
      "187:Epoch: 4, Train_Loss: 2.3348147869110107, Test_Loss: 2.265805959701538 *\n",
      "188:Epoch: 4, Train_Loss: 2.3360276222229004, Test_Loss: 2.2710843086242676\n",
      "189:Epoch: 4, Train_Loss: 2.262913703918457, Test_Loss: 2.3873724937438965\n",
      "190:Epoch: 4, Train_Loss: 2.379750967025757, Test_Loss: 3.1677725315093994\n",
      "191:Epoch: 4, Train_Loss: 2.258352279663086, Test_Loss: 3.7340197563171387\n",
      "192:Epoch: 4, Train_Loss: 2.257535934448242, Test_Loss: 2.420402765274048 *\n",
      "193:Epoch: 4, Train_Loss: 2.2503623962402344, Test_Loss: 2.3148550987243652 *\n",
      "194:Epoch: 4, Train_Loss: 2.2491602897644043, Test_Loss: 2.2464632987976074 *\n",
      "195:Epoch: 4, Train_Loss: 2.248140811920166, Test_Loss: 2.260895013809204\n",
      "196:Epoch: 4, Train_Loss: 2.2475922107696533, Test_Loss: 2.2516884803771973 *\n",
      "197:Epoch: 4, Train_Loss: 2.50309681892395, Test_Loss: 2.2658028602600098\n",
      "198:Epoch: 4, Train_Loss: 6.954311370849609, Test_Loss: 2.2999393939971924\n",
      "199:Epoch: 4, Train_Loss: 2.277174472808838, Test_Loss: 2.29038667678833 *\n",
      "200:Epoch: 4, Train_Loss: 2.2580554485321045, Test_Loss: 2.249180555343628 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 4\n",
      "201:Epoch: 4, Train_Loss: 2.2614779472351074, Test_Loss: 2.329815149307251\n",
      "202:Epoch: 4, Train_Loss: 2.2367300987243652, Test_Loss: 2.6220343112945557\n",
      "203:Epoch: 4, Train_Loss: 2.234757423400879, Test_Loss: 2.3106870651245117 *\n",
      "204:Epoch: 4, Train_Loss: 2.235264778137207, Test_Loss: 2.377180337905884\n",
      "205:Epoch: 4, Train_Loss: 2.23402738571167, Test_Loss: 2.235511302947998 *\n",
      "206:Epoch: 4, Train_Loss: 2.2320749759674072, Test_Loss: 2.2346365451812744 *\n",
      "207:Epoch: 4, Train_Loss: 2.2309048175811768, Test_Loss: 2.2340688705444336 *\n",
      "208:Epoch: 4, Train_Loss: 2.3208260536193848, Test_Loss: 2.2336673736572266 *\n",
      "209:Epoch: 4, Train_Loss: 2.314718008041382, Test_Loss: 2.247985601425171\n",
      "210:Epoch: 4, Train_Loss: 2.3336071968078613, Test_Loss: 7.223049163818359\n",
      "211:Epoch: 4, Train_Loss: 2.2976186275482178, Test_Loss: 2.7780673503875732 *\n",
      "212:Epoch: 4, Train_Loss: 2.225644588470459, Test_Loss: 2.2339909076690674 *\n",
      "213:Epoch: 4, Train_Loss: 2.3443312644958496, Test_Loss: 2.226325511932373 *\n",
      "214:Epoch: 4, Train_Loss: 2.425732135772705, Test_Loss: 2.2270405292510986\n",
      "215:Epoch: 4, Train_Loss: 2.424617290496826, Test_Loss: 2.2315139770507812\n",
      "216:Epoch: 4, Train_Loss: 2.3793447017669678, Test_Loss: 2.224330425262451 *\n",
      "217:Epoch: 4, Train_Loss: 2.223679780960083, Test_Loss: 2.2279555797576904\n",
      "218:Epoch: 4, Train_Loss: 2.2195510864257812, Test_Loss: 2.2232415676116943 *\n",
      "219:Epoch: 4, Train_Loss: 2.219402313232422, Test_Loss: 2.2237119674682617\n",
      "220:Epoch: 4, Train_Loss: 2.2244348526000977, Test_Loss: 2.22487211227417\n",
      "221:Epoch: 4, Train_Loss: 2.227186679840088, Test_Loss: 2.2271077632904053\n",
      "222:Epoch: 4, Train_Loss: 2.222444534301758, Test_Loss: 2.2237548828125 *\n",
      "223:Epoch: 4, Train_Loss: 2.217385768890381, Test_Loss: 2.2365729808807373\n",
      "224:Epoch: 4, Train_Loss: 2.214751958847046, Test_Loss: 2.226454973220825 *\n",
      "225:Epoch: 4, Train_Loss: 2.217424154281616, Test_Loss: 2.219419002532959 *\n",
      "226:Epoch: 4, Train_Loss: 2.228739023208618, Test_Loss: 2.2128331661224365 *\n",
      "227:Epoch: 4, Train_Loss: 2.3741796016693115, Test_Loss: 2.2159640789031982\n",
      "228:Epoch: 4, Train_Loss: 2.3817434310913086, Test_Loss: 2.212205171585083 *\n",
      "229:Epoch: 4, Train_Loss: 2.3930156230926514, Test_Loss: 2.2113609313964844 *\n",
      "230:Epoch: 4, Train_Loss: 2.2618319988250732, Test_Loss: 2.219447135925293\n",
      "231:Epoch: 4, Train_Loss: 2.360280752182007, Test_Loss: 2.211538314819336 *\n",
      "232:Epoch: 4, Train_Loss: 2.3178012371063232, Test_Loss: 2.21573805809021\n",
      "233:Epoch: 4, Train_Loss: 2.3032960891723633, Test_Loss: 2.2180306911468506\n",
      "234:Epoch: 4, Train_Loss: 2.3456075191497803, Test_Loss: 2.2101292610168457 *\n",
      "235:Epoch: 4, Train_Loss: 2.5295815467834473, Test_Loss: 2.2076311111450195 *\n",
      "236:Epoch: 4, Train_Loss: 2.2191498279571533, Test_Loss: 2.2071242332458496 *\n",
      "237:Epoch: 4, Train_Loss: 2.216801404953003, Test_Loss: 2.2056326866149902 *\n",
      "238:Epoch: 4, Train_Loss: 4.75504207611084, Test_Loss: 2.205698251724243\n",
      "239:Epoch: 4, Train_Loss: 2.9166641235351562, Test_Loss: 2.2068707942962646\n",
      "240:Epoch: 4, Train_Loss: 2.232370138168335, Test_Loss: 2.2606523036956787\n",
      "241:Epoch: 4, Train_Loss: 2.24934458732605, Test_Loss: 2.7749128341674805\n",
      "242:Epoch: 4, Train_Loss: 2.255610704421997, Test_Loss: 7.147055625915527\n",
      "243:Epoch: 4, Train_Loss: 2.2269976139068604, Test_Loss: 2.2056033611297607 *\n",
      "244:Epoch: 4, Train_Loss: 2.197387218475342, Test_Loss: 2.195830821990967 *\n",
      "245:Epoch: 4, Train_Loss: 2.2390596866607666, Test_Loss: 2.231363534927368\n",
      "246:Epoch: 4, Train_Loss: 2.334690809249878, Test_Loss: 2.2530620098114014\n",
      "247:Epoch: 4, Train_Loss: 2.3024370670318604, Test_Loss: 2.2561147212982178\n",
      "248:Epoch: 4, Train_Loss: 2.2818169593811035, Test_Loss: 2.1985697746276855 *\n",
      "249:Epoch: 4, Train_Loss: 2.289536952972412, Test_Loss: 2.3186798095703125\n",
      "250:Epoch: 4, Train_Loss: 2.2346043586730957, Test_Loss: 2.221560478210449 *\n",
      "251:Epoch: 4, Train_Loss: 2.226224660873413, Test_Loss: 2.1915459632873535 *\n",
      "252:Epoch: 4, Train_Loss: 2.199392318725586, Test_Loss: 2.227731466293335\n",
      "253:Epoch: 4, Train_Loss: 2.2220070362091064, Test_Loss: 2.2036499977111816 *\n",
      "254:Epoch: 4, Train_Loss: 2.211402654647827, Test_Loss: 2.1949944496154785 *\n",
      "255:Epoch: 4, Train_Loss: 2.1865687370300293, Test_Loss: 2.242093563079834\n",
      "256:Epoch: 4, Train_Loss: 2.1937036514282227, Test_Loss: 2.3082687854766846\n",
      "257:Epoch: 4, Train_Loss: 2.239271879196167, Test_Loss: 2.2399325370788574 *\n",
      "258:Epoch: 4, Train_Loss: 2.2408814430236816, Test_Loss: 2.2876367568969727\n",
      "259:Epoch: 4, Train_Loss: 2.186781167984009, Test_Loss: 2.2157413959503174 *\n",
      "260:Epoch: 4, Train_Loss: 2.1801018714904785, Test_Loss: 2.224012851715088\n",
      "261:Epoch: 4, Train_Loss: 2.179170608520508, Test_Loss: 2.203782081604004 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262:Epoch: 4, Train_Loss: 2.1779754161834717, Test_Loss: 2.1982431411743164 *\n",
      "263:Epoch: 4, Train_Loss: 2.1774048805236816, Test_Loss: 2.199251890182495\n",
      "264:Epoch: 4, Train_Loss: 2.1770167350769043, Test_Loss: 2.1996171474456787\n",
      "265:Epoch: 4, Train_Loss: 2.1757915019989014, Test_Loss: 2.1989238262176514 *\n",
      "266:Epoch: 4, Train_Loss: 2.1755197048187256, Test_Loss: 2.1983184814453125 *\n",
      "267:Epoch: 4, Train_Loss: 2.174459457397461, Test_Loss: 2.2045884132385254\n",
      "268:Epoch: 4, Train_Loss: 2.1729633808135986, Test_Loss: 2.20161771774292 *\n",
      "269:Epoch: 4, Train_Loss: 2.1732585430145264, Test_Loss: 2.2043440341949463\n",
      "270:Epoch: 4, Train_Loss: 2.183316946029663, Test_Loss: 2.1756582260131836 *\n",
      "271:Epoch: 4, Train_Loss: 2.18064546585083, Test_Loss: 2.193479299545288\n",
      "272:Epoch: 4, Train_Loss: 2.1802384853363037, Test_Loss: 2.254669189453125\n",
      "273:Epoch: 4, Train_Loss: 2.186903715133667, Test_Loss: 2.1833715438842773 *\n",
      "274:Epoch: 4, Train_Loss: 2.175309419631958, Test_Loss: 2.587284803390503\n",
      "275:Epoch: 4, Train_Loss: 2.1667258739471436, Test_Loss: 2.7885172367095947\n",
      "276:Epoch: 4, Train_Loss: 2.1663498878479004, Test_Loss: 2.3907978534698486 *\n",
      "277:Epoch: 4, Train_Loss: 2.176243543624878, Test_Loss: 2.2184317111968994 *\n",
      "278:Epoch: 4, Train_Loss: 2.190674066543579, Test_Loss: 2.2012133598327637 *\n",
      "279:Epoch: 4, Train_Loss: 2.164619207382202, Test_Loss: 2.1746857166290283 *\n",
      "280:Epoch: 4, Train_Loss: 2.164088249206543, Test_Loss: 2.2861993312835693\n",
      "281:Epoch: 4, Train_Loss: 2.1615586280822754, Test_Loss: 3.223052501678467\n",
      "282:Epoch: 4, Train_Loss: 2.1952078342437744, Test_Loss: 3.373734951019287\n",
      "283:Epoch: 4, Train_Loss: 2.216688394546509, Test_Loss: 2.2193171977996826 *\n",
      "284:Epoch: 4, Train_Loss: 2.2154388427734375, Test_Loss: 2.25960111618042\n",
      "285:Epoch: 4, Train_Loss: 2.180514335632324, Test_Loss: 2.1584150791168213 *\n",
      "286:Epoch: 4, Train_Loss: 2.1561639308929443, Test_Loss: 2.1610543727874756\n",
      "287:Epoch: 4, Train_Loss: 2.2181873321533203, Test_Loss: 2.1640713214874268\n",
      "288:Epoch: 4, Train_Loss: 2.1672747135162354, Test_Loss: 2.1706762313842773\n",
      "289:Epoch: 4, Train_Loss: 2.160266160964966, Test_Loss: 2.1964478492736816\n",
      "290:Epoch: 4, Train_Loss: 2.1821177005767822, Test_Loss: 2.1739096641540527 *\n",
      "291:Epoch: 4, Train_Loss: 2.1678037643432617, Test_Loss: 2.1589488983154297 *\n",
      "292:Epoch: 4, Train_Loss: 2.284437894821167, Test_Loss: 2.2733051776885986\n",
      "293:Epoch: 4, Train_Loss: 2.2438504695892334, Test_Loss: 2.5712733268737793\n",
      "294:Epoch: 4, Train_Loss: 2.1969516277313232, Test_Loss: 2.3278002738952637 *\n",
      "295:Epoch: 4, Train_Loss: 2.162170171737671, Test_Loss: 2.2196481227874756 *\n",
      "296:Epoch: 4, Train_Loss: 2.1594505310058594, Test_Loss: 2.156639814376831 *\n",
      "297:Epoch: 4, Train_Loss: 2.162942886352539, Test_Loss: 2.15566086769104 *\n",
      "298:Epoch: 4, Train_Loss: 2.1484017372131348, Test_Loss: 2.1550471782684326 *\n",
      "299:Epoch: 4, Train_Loss: 2.1543869972229004, Test_Loss: 2.154785633087158 *\n",
      "300:Epoch: 4, Train_Loss: 2.1648900508880615, Test_Loss: 2.258347988128662\n",
      "Model saved at location ../Saver/model.ckpt at epoch 4\n",
      "301:Epoch: 4, Train_Loss: 2.1696043014526367, Test_Loss: 7.376974105834961\n",
      "302:Epoch: 4, Train_Loss: 2.2368335723876953, Test_Loss: 2.2714874744415283 *\n",
      "303:Epoch: 4, Train_Loss: 2.153804063796997, Test_Loss: 2.1505661010742188 *\n",
      "304:Epoch: 4, Train_Loss: 2.2128424644470215, Test_Loss: 2.1437952518463135 *\n",
      "305:Epoch: 4, Train_Loss: 2.1592397689819336, Test_Loss: 2.148197650909424\n",
      "306:Epoch: 4, Train_Loss: 2.164849042892456, Test_Loss: 2.1473166942596436 *\n",
      "307:Epoch: 4, Train_Loss: 2.2398488521575928, Test_Loss: 2.141840696334839 *\n",
      "308:Epoch: 4, Train_Loss: 2.3657236099243164, Test_Loss: 2.140493154525757 *\n",
      "309:Epoch: 4, Train_Loss: 2.1467478275299072, Test_Loss: 2.1383414268493652 *\n",
      "310:Epoch: 4, Train_Loss: 2.1711788177490234, Test_Loss: 2.1372146606445312 *\n",
      "311:Epoch: 4, Train_Loss: 2.1341378688812256, Test_Loss: 2.137014150619507 *\n",
      "312:Epoch: 4, Train_Loss: 2.133681297302246, Test_Loss: 2.138389825820923\n",
      "313:Epoch: 4, Train_Loss: 2.13356351852417, Test_Loss: 2.15089750289917\n",
      "314:Epoch: 4, Train_Loss: 2.131298303604126, Test_Loss: 2.161623001098633\n",
      "315:Epoch: 4, Train_Loss: 2.1496455669403076, Test_Loss: 2.146306276321411 *\n",
      "316:Epoch: 4, Train_Loss: 2.1492059230804443, Test_Loss: 2.131385564804077 *\n",
      "317:Epoch: 4, Train_Loss: 2.1474521160125732, Test_Loss: 2.128939628601074 *\n",
      "318:Epoch: 4, Train_Loss: 2.144355058670044, Test_Loss: 2.1291415691375732\n",
      "319:Epoch: 4, Train_Loss: 2.1492276191711426, Test_Loss: 2.1278696060180664 *\n",
      "320:Epoch: 4, Train_Loss: 2.134403705596924, Test_Loss: 2.1261088848114014 *\n",
      "321:Epoch: 4, Train_Loss: 2.127586603164673, Test_Loss: 2.128838300704956\n",
      "322:Epoch: 4, Train_Loss: 2.123924732208252, Test_Loss: 2.1246049404144287 *\n",
      "323:Epoch: 4, Train_Loss: 2.14866042137146, Test_Loss: 2.1257145404815674\n",
      "324:Epoch: 4, Train_Loss: 2.1542298793792725, Test_Loss: 2.1250195503234863 *\n",
      "325:Epoch: 4, Train_Loss: 2.147998571395874, Test_Loss: 2.122833490371704 *\n",
      "326:Epoch: 4, Train_Loss: 2.1238009929656982, Test_Loss: 2.1215336322784424 *\n",
      "327:Epoch: 4, Train_Loss: 2.181234359741211, Test_Loss: 2.1214301586151123 *\n",
      "328:Epoch: 4, Train_Loss: 2.1741247177124023, Test_Loss: 2.119384765625 *\n",
      "329:Epoch: 4, Train_Loss: 2.1529674530029297, Test_Loss: 2.1193387508392334 *\n",
      "330:Epoch: 4, Train_Loss: 2.124628782272339, Test_Loss: 2.123236894607544\n",
      "331:Epoch: 4, Train_Loss: 2.145906925201416, Test_Loss: 2.1765003204345703\n",
      "332:Epoch: 4, Train_Loss: 2.118705987930298, Test_Loss: 3.8602285385131836\n",
      "333:Epoch: 4, Train_Loss: 2.1337928771972656, Test_Loss: 5.847318649291992\n",
      "334:Epoch: 4, Train_Loss: 2.1293272972106934, Test_Loss: 2.119499683380127 *\n",
      "335:Epoch: 4, Train_Loss: 2.1478872299194336, Test_Loss: 2.1128578186035156 *\n",
      "336:Epoch: 4, Train_Loss: 3.697847366333008, Test_Loss: 2.163761854171753\n",
      "337:Epoch: 4, Train_Loss: 5.93603515625, Test_Loss: 2.16801118850708\n",
      "338:Epoch: 4, Train_Loss: 2.1980316638946533, Test_Loss: 2.1687872409820557\n",
      "339:Epoch: 4, Train_Loss: 2.1256062984466553, Test_Loss: 2.1409828662872314 *\n",
      "340:Epoch: 4, Train_Loss: 2.125626802444458, Test_Loss: 2.235151767730713\n",
      "341:Epoch: 4, Train_Loss: 2.298734426498413, Test_Loss: 2.1159119606018066 *\n",
      "342:Epoch: 4, Train_Loss: 2.160104513168335, Test_Loss: 2.1211371421813965\n",
      "343:Epoch: 4, Train_Loss: 2.118155002593994, Test_Loss: 2.137916088104248\n",
      "344:Epoch: 4, Train_Loss: 2.1051650047302246, Test_Loss: 2.1188578605651855 *\n",
      "345:Epoch: 4, Train_Loss: 2.1760802268981934, Test_Loss: 2.1150667667388916 *\n",
      "346:Epoch: 4, Train_Loss: 2.1178271770477295, Test_Loss: 2.1778078079223633\n",
      "347:Epoch: 4, Train_Loss: 2.1220102310180664, Test_Loss: 2.206124782562256\n",
      "348:Epoch: 4, Train_Loss: 2.5817670822143555, Test_Loss: 2.1762101650238037 *\n",
      "349:Epoch: 4, Train_Loss: 3.5221590995788574, Test_Loss: 2.215707778930664\n",
      "350:Epoch: 4, Train_Loss: 3.0762362480163574, Test_Loss: 2.119987964630127 *\n",
      "351:Epoch: 4, Train_Loss: 2.2315800189971924, Test_Loss: 2.1393685340881348\n",
      "352:Epoch: 4, Train_Loss: 2.3890111446380615, Test_Loss: 2.1231160163879395 *\n",
      "353:Epoch: 4, Train_Loss: 4.507013320922852, Test_Loss: 2.1189982891082764 *\n",
      "354:Epoch: 4, Train_Loss: 2.956662178039551, Test_Loss: 2.120112419128418\n",
      "355:Epoch: 4, Train_Loss: 2.1520142555236816, Test_Loss: 2.1200430393218994 *\n",
      "356:Epoch: 4, Train_Loss: 2.128638505935669, Test_Loss: 2.1177010536193848 *\n",
      "357:Epoch: 4, Train_Loss: 3.0568900108337402, Test_Loss: 2.116637706756592 *\n",
      "358:Epoch: 4, Train_Loss: 3.7199783325195312, Test_Loss: 2.1267504692077637\n",
      "359:Epoch: 4, Train_Loss: 2.7000200748443604, Test_Loss: 2.1175031661987305 *\n",
      "360:Epoch: 4, Train_Loss: 2.1126997470855713, Test_Loss: 2.1143639087677 *\n",
      "361:Epoch: 4, Train_Loss: 2.100881814956665, Test_Loss: 2.092684268951416 *\n",
      "362:Epoch: 4, Train_Loss: 2.5487864017486572, Test_Loss: 2.11820125579834\n",
      "363:Epoch: 4, Train_Loss: 2.401885509490967, Test_Loss: 2.1645236015319824\n",
      "364:Epoch: 4, Train_Loss: 2.1017463207244873, Test_Loss: 2.0979065895080566 *\n",
      "365:Epoch: 4, Train_Loss: 2.1377274990081787, Test_Loss: 2.6084158420562744\n",
      "366:Epoch: 4, Train_Loss: 2.2200028896331787, Test_Loss: 2.667625904083252\n",
      "367:Epoch: 4, Train_Loss: 2.2465310096740723, Test_Loss: 2.2738137245178223 *\n",
      "368:Epoch: 4, Train_Loss: 2.164950370788574, Test_Loss: 2.1219301223754883 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369:Epoch: 4, Train_Loss: 2.4995620250701904, Test_Loss: 2.1109442710876465 *\n",
      "370:Epoch: 4, Train_Loss: 2.206753969192505, Test_Loss: 2.100398302078247 *\n",
      "371:Epoch: 4, Train_Loss: 2.1538188457489014, Test_Loss: 2.275759220123291\n",
      "372:Epoch: 4, Train_Loss: 2.3566861152648926, Test_Loss: 3.30747652053833\n",
      "373:Epoch: 4, Train_Loss: 2.4431252479553223, Test_Loss: 2.972508668899536 *\n",
      "374:Epoch: 4, Train_Loss: 2.5502991676330566, Test_Loss: 2.135115623474121 *\n",
      "375:Epoch: 4, Train_Loss: 2.2683358192443848, Test_Loss: 2.134382724761963 *\n",
      "376:Epoch: 4, Train_Loss: 2.159172773361206, Test_Loss: 2.079812526702881 *\n",
      "377:Epoch: 4, Train_Loss: 2.2038841247558594, Test_Loss: 2.0795934200286865 *\n",
      "378:Epoch: 4, Train_Loss: 2.1378912925720215, Test_Loss: 2.0849757194519043\n",
      "379:Epoch: 4, Train_Loss: 2.090916633605957, Test_Loss: 2.0913546085357666\n",
      "380:Epoch: 4, Train_Loss: 2.0733799934387207, Test_Loss: 2.129613161087036\n",
      "381:Epoch: 4, Train_Loss: 2.0737855434417725, Test_Loss: 2.0761971473693848 *\n",
      "382:Epoch: 4, Train_Loss: 2.072589635848999, Test_Loss: 2.096156120300293\n",
      "383:Epoch: 4, Train_Loss: 2.0752625465393066, Test_Loss: 2.196539878845215\n",
      "384:Epoch: 4, Train_Loss: 2.114348888397217, Test_Loss: 2.4625773429870605\n",
      "385:Epoch: 4, Train_Loss: 2.158886432647705, Test_Loss: 2.329116106033325 *\n",
      "386:Epoch: 4, Train_Loss: 2.163346290588379, Test_Loss: 2.0839366912841797 *\n",
      "387:Epoch: 4, Train_Loss: 2.1852962970733643, Test_Loss: 2.073521137237549 *\n",
      "388:Epoch: 4, Train_Loss: 2.4862897396087646, Test_Loss: 2.072106122970581 *\n",
      "389:Epoch: 4, Train_Loss: 2.172102212905884, Test_Loss: 2.07085919380188 *\n",
      "390:Epoch: 4, Train_Loss: 2.0849552154541016, Test_Loss: 2.075716733932495\n",
      "391:Epoch: 4, Train_Loss: 2.1757736206054688, Test_Loss: 2.649022102355957\n",
      "392:Epoch: 4, Train_Loss: 2.5357797145843506, Test_Loss: 6.945133209228516\n",
      "393:Epoch: 4, Train_Loss: 2.5402610301971436, Test_Loss: 2.106839895248413 *\n",
      "394:Epoch: 4, Train_Loss: 2.067203998565674, Test_Loss: 2.067701816558838 *\n",
      "395:Epoch: 4, Train_Loss: 2.0670485496520996, Test_Loss: 2.063087224960327 *\n",
      "396:Epoch: 4, Train_Loss: 2.6160435676574707, Test_Loss: 2.0667123794555664\n",
      "397:Epoch: 4, Train_Loss: 2.670876979827881, Test_Loss: 2.063775062561035 *\n",
      "398:Epoch: 4, Train_Loss: 2.1926252841949463, Test_Loss: 2.0616676807403564 *\n",
      "399:Epoch: 4, Train_Loss: 2.0833585262298584, Test_Loss: 2.074312448501587\n",
      "400:Epoch: 4, Train_Loss: 2.0721771717071533, Test_Loss: 2.0661208629608154 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 4\n",
      "401:Epoch: 4, Train_Loss: 2.7890076637268066, Test_Loss: 2.0760886669158936\n",
      "402:Epoch: 4, Train_Loss: 3.476255178451538, Test_Loss: 2.068045139312744 *\n",
      "403:Epoch: 4, Train_Loss: 2.170254707336426, Test_Loss: 2.079305410385132\n",
      "404:Epoch: 4, Train_Loss: 2.0864388942718506, Test_Loss: 2.0574839115142822 *\n",
      "405:Epoch: 4, Train_Loss: 2.0620949268341064, Test_Loss: 2.0559310913085938 *\n",
      "406:Epoch: 4, Train_Loss: 2.062626838684082, Test_Loss: 2.0710761547088623\n",
      "407:Epoch: 4, Train_Loss: 2.4856152534484863, Test_Loss: 2.0562970638275146 *\n",
      "408:Epoch: 4, Train_Loss: 2.082016944885254, Test_Loss: 2.054257392883301 *\n",
      "409:Epoch: 4, Train_Loss: 2.186356782913208, Test_Loss: 2.0541269779205322 *\n",
      "410:Epoch: 4, Train_Loss: 2.3781001567840576, Test_Loss: 2.056274890899658\n",
      "411:Epoch: 4, Train_Loss: 2.065025568008423, Test_Loss: 2.052309989929199 *\n",
      "412:Epoch: 4, Train_Loss: 2.0854527950286865, Test_Loss: 2.066033124923706\n",
      "413:Epoch: 4, Train_Loss: 2.151224374771118, Test_Loss: 2.0591437816619873 *\n",
      "414:Epoch: 4, Train_Loss: 2.3382160663604736, Test_Loss: 2.068732500076294\n",
      "415:Epoch: 4, Train_Loss: 2.0686190128326416, Test_Loss: 2.060063362121582 *\n",
      "416:Epoch: 4, Train_Loss: 2.1579833030700684, Test_Loss: 2.0551462173461914 *\n",
      "417:Epoch: 4, Train_Loss: 2.247468948364258, Test_Loss: 2.050551176071167 *\n",
      "418:Epoch: 4, Train_Loss: 2.159224033355713, Test_Loss: 2.0529680252075195\n",
      "419:Epoch: 4, Train_Loss: 2.1197009086608887, Test_Loss: 2.050248146057129 *\n",
      "420:Epoch: 4, Train_Loss: 2.04329514503479, Test_Loss: 2.0453248023986816 *\n",
      "421:Epoch: 4, Train_Loss: 2.0655970573425293, Test_Loss: 2.0752625465393066\n",
      "422:Epoch: 4, Train_Loss: 2.0563693046569824, Test_Loss: 2.0991709232330322\n",
      "423:Epoch: 4, Train_Loss: 2.5321409702301025, Test_Loss: 4.957228183746338\n",
      "424:Epoch: 4, Train_Loss: 2.385465145111084, Test_Loss: 4.633281707763672 *\n",
      "425:Epoch: 4, Train_Loss: 2.4649887084960938, Test_Loss: 2.039602279663086 *\n",
      "426:Epoch: 4, Train_Loss: 2.7327451705932617, Test_Loss: 2.043653964996338\n",
      "427:Epoch: 4, Train_Loss: 2.1927037239074707, Test_Loss: 2.094585657119751\n",
      "428:Epoch: 4, Train_Loss: 2.49650239944458, Test_Loss: 2.0513017177581787 *\n",
      "429:Epoch: 4, Train_Loss: 2.1812684535980225, Test_Loss: 2.060469150543213\n",
      "430:Epoch: 4, Train_Loss: 2.049339771270752, Test_Loss: 2.093535900115967\n",
      "431:Epoch: 4, Train_Loss: 2.035238265991211, Test_Loss: 2.0998599529266357\n",
      "432:Epoch: 4, Train_Loss: 2.065495252609253, Test_Loss: 2.0493624210357666 *\n",
      "433:Epoch: 4, Train_Loss: 2.2217087745666504, Test_Loss: 2.055399179458618\n",
      "434:Epoch: 4, Train_Loss: 2.7666592597961426, Test_Loss: 2.0729546546936035\n",
      "435:Epoch: 4, Train_Loss: 2.3741486072540283, Test_Loss: 2.0968809127807617\n",
      "436:Epoch: 4, Train_Loss: 3.9178786277770996, Test_Loss: 2.056809902191162 *\n",
      "437:Epoch: 4, Train_Loss: 2.4802045822143555, Test_Loss: 2.0962812900543213\n",
      "438:Epoch: 4, Train_Loss: 2.7903683185577393, Test_Loss: 2.057448148727417 *\n",
      "439:Epoch: 4, Train_Loss: 2.0952980518341064, Test_Loss: 2.0980403423309326\n",
      "440:Epoch: 4, Train_Loss: 2.036815643310547, Test_Loss: 2.0317416191101074 *\n",
      "441:Epoch: 4, Train_Loss: 2.369641065597534, Test_Loss: 2.141425132751465\n",
      "442:Epoch: 4, Train_Loss: 3.2255377769470215, Test_Loss: 2.077130079269409 *\n",
      "443:Epoch: 4, Train_Loss: 2.345662832260132, Test_Loss: 2.02732253074646 *\n",
      "444:Epoch: 4, Train_Loss: 2.2340869903564453, Test_Loss: 2.0320966243743896\n",
      "445:Epoch: 4, Train_Loss: 2.0476274490356445, Test_Loss: 2.0302000045776367 *\n",
      "446:Epoch: 4, Train_Loss: 2.074157238006592, Test_Loss: 2.0306308269500732\n",
      "447:Epoch: 4, Train_Loss: 2.3256473541259766, Test_Loss: 2.032681465148926\n",
      "448:Epoch: 4, Train_Loss: 2.3714022636413574, Test_Loss: 2.034937858581543\n",
      "449:Epoch: 4, Train_Loss: 2.990217685699463, Test_Loss: 2.0312488079071045 *\n",
      "450:Epoch: 4, Train_Loss: 2.3338959217071533, Test_Loss: 2.0227062702178955 *\n",
      "451:Epoch: 4, Train_Loss: 2.041971445083618, Test_Loss: 2.0221900939941406 *\n",
      "452:Epoch: 4, Train_Loss: 2.0238847732543945, Test_Loss: 2.05961012840271\n",
      "453:Epoch: 4, Train_Loss: 2.0189905166625977, Test_Loss: 2.0193190574645996 *\n",
      "454:Epoch: 4, Train_Loss: 2.0466935634613037, Test_Loss: 2.022294521331787\n",
      "1:Epoch: 5, Train_Loss: 2.0502216815948486, Test_Loss: 2.0629141330718994 *\n",
      "2:Epoch: 5, Train_Loss: 2.0406577587127686, Test_Loss: 2.3261070251464844\n",
      "3:Epoch: 5, Train_Loss: 7.079404830932617, Test_Loss: 2.3417105674743652\n",
      "4:Epoch: 5, Train_Loss: 14.228816032409668, Test_Loss: 2.1023640632629395 *\n",
      "5:Epoch: 5, Train_Loss: 2.5402400493621826, Test_Loss: 2.016594171524048 *\n",
      "6:Epoch: 5, Train_Loss: 5.298455238342285, Test_Loss: 2.0382049083709717\n",
      "7:Epoch: 5, Train_Loss: 2.4961097240448, Test_Loss: 2.0264508724212646 *\n",
      "8:Epoch: 5, Train_Loss: 2.0728938579559326, Test_Loss: 2.2197165489196777\n",
      "9:Epoch: 5, Train_Loss: 2.1114675998687744, Test_Loss: 2.8437726497650146\n",
      "10:Epoch: 5, Train_Loss: 11.611300468444824, Test_Loss: 2.4653892517089844 *\n",
      "11:Epoch: 5, Train_Loss: 4.09601354598999, Test_Loss: 2.072550058364868 *\n",
      "12:Epoch: 5, Train_Loss: 2.03715443611145, Test_Loss: 2.0363709926605225 *\n",
      "13:Epoch: 5, Train_Loss: 4.177104949951172, Test_Loss: 2.0821456909179688\n",
      "14:Epoch: 5, Train_Loss: 5.684950351715088, Test_Loss: 2.131028413772583\n",
      "15:Epoch: 5, Train_Loss: 2.084303855895996, Test_Loss: 2.1340041160583496\n",
      "16:Epoch: 5, Train_Loss: 2.0493006706237793, Test_Loss: 2.089582920074463 *\n",
      "17:Epoch: 5, Train_Loss: 2.055896759033203, Test_Loss: 2.333315134048462\n",
      "18:Epoch: 5, Train_Loss: 2.0240797996520996, Test_Loss: 2.0236308574676514 *\n",
      "19:Epoch: 5, Train_Loss: 2.018864631652832, Test_Loss: 2.0838658809661865\n",
      "20:Epoch: 5, Train_Loss: 2.000452756881714, Test_Loss: 2.2137210369110107\n",
      "21:Epoch: 5, Train_Loss: 2.002673625946045, Test_Loss: 2.193572998046875 *\n",
      "22:Epoch: 5, Train_Loss: 1.99919855594635, Test_Loss: 2.146418571472168 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:Epoch: 5, Train_Loss: 2.0081541538238525, Test_Loss: 2.0058095455169678 *\n",
      "24:Epoch: 5, Train_Loss: 2.0243067741394043, Test_Loss: 2.0118632316589355\n",
      "25:Epoch: 5, Train_Loss: 1.9963829517364502, Test_Loss: 2.0089595317840576 *\n",
      "26:Epoch: 5, Train_Loss: 2.030759334564209, Test_Loss: 2.0039846897125244 *\n",
      "27:Epoch: 5, Train_Loss: 2.0592470169067383, Test_Loss: 1.9967026710510254 *\n",
      "28:Epoch: 5, Train_Loss: 2.043267250061035, Test_Loss: 3.7162764072418213\n",
      "29:Epoch: 5, Train_Loss: 2.0007662773132324, Test_Loss: 6.229696273803711\n",
      "30:Epoch: 5, Train_Loss: 1.994300127029419, Test_Loss: 2.0272390842437744 *\n",
      "31:Epoch: 5, Train_Loss: 1.9918750524520874, Test_Loss: 2.0153257846832275 *\n",
      "32:Epoch: 5, Train_Loss: 1.9873645305633545, Test_Loss: 2.0039780139923096 *\n",
      "33:Epoch: 5, Train_Loss: 1.9871093034744263, Test_Loss: 1.9900556802749634 *\n",
      "34:Epoch: 5, Train_Loss: 1.9849162101745605, Test_Loss: 2.014556646347046\n",
      "35:Epoch: 5, Train_Loss: 1.9843422174453735, Test_Loss: 2.0194480419158936\n",
      "36:Epoch: 5, Train_Loss: 1.9835503101348877, Test_Loss: 2.0244951248168945\n",
      "37:Epoch: 5, Train_Loss: 1.9834650754928589, Test_Loss: 2.008674383163452 *\n",
      "38:Epoch: 5, Train_Loss: 1.9819107055664062, Test_Loss: 2.0254151821136475\n",
      "39:Epoch: 5, Train_Loss: 1.9814794063568115, Test_Loss: 2.009545087814331 *\n",
      "40:Epoch: 5, Train_Loss: 1.9869502782821655, Test_Loss: 2.019252061843872\n",
      "41:Epoch: 5, Train_Loss: 2.000596761703491, Test_Loss: 1.9826692342758179 *\n",
      "42:Epoch: 5, Train_Loss: 2.0255324840545654, Test_Loss: 1.9815431833267212 *\n",
      "43:Epoch: 5, Train_Loss: 1.9891374111175537, Test_Loss: 2.010387659072876\n",
      "44:Epoch: 5, Train_Loss: 1.981162667274475, Test_Loss: 1.9853318929672241 *\n",
      "45:Epoch: 5, Train_Loss: 10.1506929397583, Test_Loss: 1.989944577217102\n",
      "46:Epoch: 5, Train_Loss: 3.0862584114074707, Test_Loss: 1.9836691617965698 *\n",
      "47:Epoch: 5, Train_Loss: 1.9870827198028564, Test_Loss: 2.007709264755249\n",
      "48:Epoch: 5, Train_Loss: 2.0116219520568848, Test_Loss: 2.0031898021698 *\n",
      "49:Epoch: 5, Train_Loss: 2.0283989906311035, Test_Loss: 2.021855592727661\n",
      "50:Epoch: 5, Train_Loss: 1.9774250984191895, Test_Loss: 2.017983913421631 *\n",
      "51:Epoch: 5, Train_Loss: 1.9787945747375488, Test_Loss: 2.039621353149414\n",
      "52:Epoch: 5, Train_Loss: 2.015554189682007, Test_Loss: 2.0194997787475586 *\n",
      "53:Epoch: 5, Train_Loss: 2.1309702396392822, Test_Loss: 2.0146358013153076 *\n",
      "54:Epoch: 5, Train_Loss: 2.185462236404419, Test_Loss: 1.9957245588302612 *\n",
      "55:Epoch: 5, Train_Loss: 2.1399898529052734, Test_Loss: 2.0069923400878906\n",
      "56:Epoch: 5, Train_Loss: 1.9883270263671875, Test_Loss: 2.0032174587249756 *\n",
      "57:Epoch: 5, Train_Loss: 2.056795358657837, Test_Loss: 1.9927071332931519 *\n",
      "58:Epoch: 5, Train_Loss: 2.080749988555908, Test_Loss: 2.0711982250213623\n",
      "59:Epoch: 5, Train_Loss: 2.1252269744873047, Test_Loss: 1.9962958097457886 *\n",
      "60:Epoch: 5, Train_Loss: 2.1072394847869873, Test_Loss: 6.2090606689453125\n",
      "61:Epoch: 5, Train_Loss: 2.0859200954437256, Test_Loss: 3.681840181350708 *\n",
      "62:Epoch: 5, Train_Loss: 2.0293173789978027, Test_Loss: 1.97005033493042 *\n",
      "63:Epoch: 5, Train_Loss: 1.9751877784729004, Test_Loss: 1.9721981287002563\n",
      "64:Epoch: 5, Train_Loss: 2.025740623474121, Test_Loss: 1.994053602218628\n",
      "65:Epoch: 5, Train_Loss: 1.9888224601745605, Test_Loss: 1.986095666885376 *\n",
      "66:Epoch: 5, Train_Loss: 1.9724934101104736, Test_Loss: 1.9726585149765015 *\n",
      "67:Epoch: 5, Train_Loss: 1.9739468097686768, Test_Loss: 2.0603749752044678\n",
      "68:Epoch: 5, Train_Loss: 1.9754576683044434, Test_Loss: 2.0711183547973633\n",
      "69:Epoch: 5, Train_Loss: 2.0031445026397705, Test_Loss: 1.9602792263031006 *\n",
      "70:Epoch: 5, Train_Loss: 7.430658340454102, Test_Loss: 2.00539493560791\n",
      "71:Epoch: 5, Train_Loss: 2.033514976501465, Test_Loss: 1.972624659538269 *\n",
      "72:Epoch: 5, Train_Loss: 1.9759531021118164, Test_Loss: 1.9728496074676514\n",
      "73:Epoch: 5, Train_Loss: 1.9995123147964478, Test_Loss: 1.962169885635376 *\n",
      "74:Epoch: 5, Train_Loss: 1.9785335063934326, Test_Loss: 2.0207390785217285\n",
      "75:Epoch: 5, Train_Loss: 1.9692912101745605, Test_Loss: 1.992194414138794 *\n",
      "76:Epoch: 5, Train_Loss: 1.9624567031860352, Test_Loss: 2.077561140060425\n",
      "77:Epoch: 5, Train_Loss: 1.9689520597457886, Test_Loss: 2.0440125465393066 *\n",
      "78:Epoch: 5, Train_Loss: 1.9741160869598389, Test_Loss: 1.9711538553237915 *\n",
      "79:Epoch: 5, Train_Loss: 1.9643064737319946, Test_Loss: 1.9553663730621338 *\n",
      "80:Epoch: 5, Train_Loss: 1.9832347631454468, Test_Loss: 1.9508222341537476 *\n",
      "81:Epoch: 5, Train_Loss: 1.961424708366394, Test_Loss: 1.9504841566085815 *\n",
      "82:Epoch: 5, Train_Loss: 1.9565801620483398, Test_Loss: 1.950848937034607\n",
      "83:Epoch: 5, Train_Loss: 1.9729059934616089, Test_Loss: 1.9519349336624146\n",
      "84:Epoch: 5, Train_Loss: 1.9502869844436646, Test_Loss: 1.9499924182891846 *\n",
      "85:Epoch: 5, Train_Loss: 1.949952483177185, Test_Loss: 1.946871280670166 *\n",
      "86:Epoch: 5, Train_Loss: 1.9698374271392822, Test_Loss: 1.949982762336731\n",
      "87:Epoch: 5, Train_Loss: 1.9969964027404785, Test_Loss: 1.9453213214874268 *\n",
      "88:Epoch: 5, Train_Loss: 1.9747592210769653, Test_Loss: 1.9502575397491455\n",
      "89:Epoch: 5, Train_Loss: 1.945662021636963, Test_Loss: 1.9656792879104614\n",
      "90:Epoch: 5, Train_Loss: 1.946674108505249, Test_Loss: 1.951999545097351 *\n",
      "91:Epoch: 5, Train_Loss: 1.9999481439590454, Test_Loss: 1.961470365524292\n",
      "92:Epoch: 5, Train_Loss: 2.0063390731811523, Test_Loss: 2.0516631603240967\n",
      "93:Epoch: 5, Train_Loss: 1.9827927350997925, Test_Loss: 2.348875045776367\n",
      "94:Epoch: 5, Train_Loss: 1.963139533996582, Test_Loss: 2.2505080699920654 *\n",
      "95:Epoch: 5, Train_Loss: 1.9938952922821045, Test_Loss: 2.0170044898986816 *\n",
      "96:Epoch: 5, Train_Loss: 2.0177958011627197, Test_Loss: 1.9437836408615112 *\n",
      "97:Epoch: 5, Train_Loss: 2.008909225463867, Test_Loss: 1.9550107717514038\n",
      "98:Epoch: 5, Train_Loss: 1.9788689613342285, Test_Loss: 2.0242674350738525\n",
      "99:Epoch: 5, Train_Loss: 2.081139087677002, Test_Loss: 2.4424972534179688\n",
      "100:Epoch: 5, Train_Loss: 1.9637433290481567, Test_Loss: 3.1640868186950684\n",
      "Model saved at location ../Saver/model.ckpt at epoch 5\n",
      "101:Epoch: 5, Train_Loss: 1.9538823366165161, Test_Loss: 2.3731791973114014 *\n",
      "102:Epoch: 5, Train_Loss: 1.9409270286560059, Test_Loss: 1.9986032247543335 *\n",
      "103:Epoch: 5, Train_Loss: 1.9376342296600342, Test_Loss: 1.9416478872299194 *\n",
      "104:Epoch: 5, Train_Loss: 1.9362564086914062, Test_Loss: 1.9477410316467285\n",
      "105:Epoch: 5, Train_Loss: 1.935941457748413, Test_Loss: 1.9454890489578247 *\n",
      "106:Epoch: 5, Train_Loss: 1.9383677244186401, Test_Loss: 1.947116732597351\n",
      "107:Epoch: 5, Train_Loss: 6.527359962463379, Test_Loss: 1.9664779901504517\n",
      "108:Epoch: 5, Train_Loss: 2.3331711292266846, Test_Loss: 1.9864906072616577\n",
      "109:Epoch: 5, Train_Loss: 1.9336992502212524, Test_Loss: 1.9323025941848755 *\n",
      "110:Epoch: 5, Train_Loss: 1.9451854228973389, Test_Loss: 2.00919508934021\n",
      "111:Epoch: 5, Train_Loss: 1.9306344985961914, Test_Loss: 2.186950445175171\n",
      "112:Epoch: 5, Train_Loss: 1.9253851175308228, Test_Loss: 2.128037691116333 *\n",
      "113:Epoch: 5, Train_Loss: 1.9260849952697754, Test_Loss: 2.11841082572937 *\n",
      "114:Epoch: 5, Train_Loss: 1.9252792596817017, Test_Loss: 1.9282488822937012 *\n",
      "115:Epoch: 5, Train_Loss: 1.9236570596694946, Test_Loss: 1.9271291494369507 *\n",
      "116:Epoch: 5, Train_Loss: 1.9231702089309692, Test_Loss: 1.9266897439956665 *\n",
      "117:Epoch: 5, Train_Loss: 1.9806114435195923, Test_Loss: 1.9264551401138306 *\n",
      "118:Epoch: 5, Train_Loss: 2.0092954635620117, Test_Loss: 1.9405581951141357\n",
      "119:Epoch: 5, Train_Loss: 2.015084743499756, Test_Loss: 4.9181060791015625\n",
      "120:Epoch: 5, Train_Loss: 2.0118165016174316, Test_Loss: 4.433249473571777 *\n",
      "121:Epoch: 5, Train_Loss: 1.919946551322937, Test_Loss: 1.9277130365371704 *\n",
      "122:Epoch: 5, Train_Loss: 1.9814033508300781, Test_Loss: 1.921415090560913 *\n",
      "123:Epoch: 5, Train_Loss: 2.1168100833892822, Test_Loss: 1.919416069984436 *\n",
      "124:Epoch: 5, Train_Loss: 2.108361005783081, Test_Loss: 1.9262361526489258\n",
      "125:Epoch: 5, Train_Loss: 2.1103291511535645, Test_Loss: 1.9188320636749268 *\n",
      "126:Epoch: 5, Train_Loss: 1.9222925901412964, Test_Loss: 1.9231168031692505\n",
      "127:Epoch: 5, Train_Loss: 1.9144091606140137, Test_Loss: 1.9183419942855835 *\n",
      "128:Epoch: 5, Train_Loss: 1.9148797988891602, Test_Loss: 1.9209425449371338\n",
      "129:Epoch: 5, Train_Loss: 1.918252944946289, Test_Loss: 1.9216972589492798\n",
      "130:Epoch: 5, Train_Loss: 1.9203295707702637, Test_Loss: 1.9179911613464355 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131:Epoch: 5, Train_Loss: 1.9181946516036987, Test_Loss: 1.9220788478851318\n",
      "132:Epoch: 5, Train_Loss: 1.9154373407363892, Test_Loss: 1.9331716299057007\n",
      "133:Epoch: 5, Train_Loss: 1.911589503288269, Test_Loss: 1.9243687391281128 *\n",
      "134:Epoch: 5, Train_Loss: 1.9118690490722656, Test_Loss: 1.9161419868469238 *\n",
      "135:Epoch: 5, Train_Loss: 1.9183741807937622, Test_Loss: 1.9092763662338257 *\n",
      "136:Epoch: 5, Train_Loss: 2.042454481124878, Test_Loss: 1.9113153219223022\n",
      "137:Epoch: 5, Train_Loss: 2.081397294998169, Test_Loss: 1.9090044498443604 *\n",
      "138:Epoch: 5, Train_Loss: 2.094770908355713, Test_Loss: 1.9092837572097778\n",
      "139:Epoch: 5, Train_Loss: 1.9557678699493408, Test_Loss: 1.9122374057769775\n",
      "140:Epoch: 5, Train_Loss: 2.061645030975342, Test_Loss: 1.9077492952346802 *\n",
      "141:Epoch: 5, Train_Loss: 2.052785873413086, Test_Loss: 1.908090591430664\n",
      "142:Epoch: 5, Train_Loss: 1.9443044662475586, Test_Loss: 1.913313388824463\n",
      "143:Epoch: 5, Train_Loss: 2.0784213542938232, Test_Loss: 1.9064940214157104 *\n",
      "144:Epoch: 5, Train_Loss: 2.0722808837890625, Test_Loss: 1.9074674844741821\n",
      "145:Epoch: 5, Train_Loss: 2.1139638423919678, Test_Loss: 1.9043023586273193 *\n",
      "146:Epoch: 5, Train_Loss: 1.9190690517425537, Test_Loss: 1.9047799110412598\n",
      "147:Epoch: 5, Train_Loss: 3.529895782470703, Test_Loss: 1.9041879177093506 *\n",
      "148:Epoch: 5, Train_Loss: 3.4939560890197754, Test_Loss: 1.9029719829559326 *\n",
      "149:Epoch: 5, Train_Loss: 1.934013843536377, Test_Loss: 1.9433748722076416\n",
      "150:Epoch: 5, Train_Loss: 1.949115514755249, Test_Loss: 1.9308608770370483 *\n",
      "151:Epoch: 5, Train_Loss: 1.9477639198303223, Test_Loss: 7.08411169052124\n",
      "152:Epoch: 5, Train_Loss: 1.936604619026184, Test_Loss: 2.3092105388641357 *\n",
      "153:Epoch: 5, Train_Loss: 1.8969919681549072, Test_Loss: 1.8966169357299805 *\n",
      "154:Epoch: 5, Train_Loss: 1.9151674509048462, Test_Loss: 1.9160830974578857\n",
      "155:Epoch: 5, Train_Loss: 2.0346691608428955, Test_Loss: 1.9506022930145264\n",
      "156:Epoch: 5, Train_Loss: 1.991039514541626, Test_Loss: 1.9552550315856934\n",
      "157:Epoch: 5, Train_Loss: 1.9868983030319214, Test_Loss: 1.8998959064483643 *\n",
      "158:Epoch: 5, Train_Loss: 1.9795550107955933, Test_Loss: 1.9844061136245728\n",
      "159:Epoch: 5, Train_Loss: 1.9470735788345337, Test_Loss: 1.9548851251602173 *\n",
      "160:Epoch: 5, Train_Loss: 1.9175902605056763, Test_Loss: 1.8929033279418945 *\n",
      "161:Epoch: 5, Train_Loss: 1.909833550453186, Test_Loss: 1.9204734563827515\n",
      "162:Epoch: 5, Train_Loss: 1.910814642906189, Test_Loss: 1.9142056703567505 *\n",
      "163:Epoch: 5, Train_Loss: 1.9224886894226074, Test_Loss: 1.895710825920105 *\n",
      "164:Epoch: 5, Train_Loss: 1.8978301286697388, Test_Loss: 1.9018574953079224\n",
      "165:Epoch: 5, Train_Loss: 1.887894630432129, Test_Loss: 2.02341365814209\n",
      "166:Epoch: 5, Train_Loss: 1.9307830333709717, Test_Loss: 1.9223921298980713 *\n",
      "167:Epoch: 5, Train_Loss: 1.9350851774215698, Test_Loss: 1.9704169034957886\n",
      "168:Epoch: 5, Train_Loss: 1.900667428970337, Test_Loss: 1.927934169769287 *\n",
      "169:Epoch: 5, Train_Loss: 1.8844738006591797, Test_Loss: 1.9384275674819946\n",
      "170:Epoch: 5, Train_Loss: 1.8842312097549438, Test_Loss: 1.9063336849212646 *\n",
      "171:Epoch: 5, Train_Loss: 1.8828887939453125, Test_Loss: 1.9013018608093262 *\n",
      "172:Epoch: 5, Train_Loss: 1.8824304342269897, Test_Loss: 1.904086709022522\n",
      "173:Epoch: 5, Train_Loss: 1.8816200494766235, Test_Loss: 1.9046365022659302\n",
      "174:Epoch: 5, Train_Loss: 1.8826100826263428, Test_Loss: 1.9055778980255127\n",
      "175:Epoch: 5, Train_Loss: 1.8828105926513672, Test_Loss: 1.9026851654052734 *\n",
      "176:Epoch: 5, Train_Loss: 1.8800737857818604, Test_Loss: 1.893420696258545 *\n",
      "177:Epoch: 5, Train_Loss: 1.879581332206726, Test_Loss: 1.9086008071899414\n",
      "178:Epoch: 5, Train_Loss: 1.881233811378479, Test_Loss: 1.902461051940918 *\n",
      "179:Epoch: 5, Train_Loss: 1.8925719261169434, Test_Loss: 1.8859620094299316 *\n",
      "180:Epoch: 5, Train_Loss: 1.8942538499832153, Test_Loss: 1.8872157335281372\n",
      "181:Epoch: 5, Train_Loss: 1.8918700218200684, Test_Loss: 1.9379501342773438\n",
      "182:Epoch: 5, Train_Loss: 1.9031466245651245, Test_Loss: 1.9010541439056396 *\n",
      "183:Epoch: 5, Train_Loss: 1.8850420713424683, Test_Loss: 2.140566349029541\n",
      "184:Epoch: 5, Train_Loss: 1.8809432983398438, Test_Loss: 2.470618724822998\n",
      "185:Epoch: 5, Train_Loss: 1.8739514350891113, Test_Loss: 2.1818795204162598 *\n",
      "186:Epoch: 5, Train_Loss: 1.8787060976028442, Test_Loss: 1.9741796255111694 *\n",
      "187:Epoch: 5, Train_Loss: 1.901515007019043, Test_Loss: 1.913787841796875 *\n",
      "188:Epoch: 5, Train_Loss: 1.880721926689148, Test_Loss: 1.8770079612731934 *\n",
      "189:Epoch: 5, Train_Loss: 1.8727666139602661, Test_Loss: 1.9512220621109009\n",
      "190:Epoch: 5, Train_Loss: 1.8687677383422852, Test_Loss: 2.529874801635742\n",
      "191:Epoch: 5, Train_Loss: 1.8803446292877197, Test_Loss: 3.113834857940674\n",
      "192:Epoch: 5, Train_Loss: 1.9392526149749756, Test_Loss: 2.087810754776001 *\n",
      "193:Epoch: 5, Train_Loss: 1.9090436697006226, Test_Loss: 1.9714821577072144 *\n",
      "194:Epoch: 5, Train_Loss: 1.9036821126937866, Test_Loss: 1.8672785758972168 *\n",
      "195:Epoch: 5, Train_Loss: 1.86483633518219, Test_Loss: 1.8719449043273926\n",
      "196:Epoch: 5, Train_Loss: 1.9158920049667358, Test_Loss: 1.8674370050430298 *\n",
      "197:Epoch: 5, Train_Loss: 1.8862924575805664, Test_Loss: 1.8782254457473755\n",
      "198:Epoch: 5, Train_Loss: 1.8653223514556885, Test_Loss: 1.8951199054718018\n",
      "199:Epoch: 5, Train_Loss: 1.876935601234436, Test_Loss: 1.9031100273132324\n",
      "200:Epoch: 5, Train_Loss: 1.8889100551605225, Test_Loss: 1.866500735282898 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 5\n",
      "201:Epoch: 5, Train_Loss: 1.9689154624938965, Test_Loss: 1.975372076034546\n",
      "202:Epoch: 5, Train_Loss: 1.9512816667556763, Test_Loss: 2.242333173751831\n",
      "203:Epoch: 5, Train_Loss: 1.9227240085601807, Test_Loss: 1.9620623588562012 *\n",
      "204:Epoch: 5, Train_Loss: 1.8848514556884766, Test_Loss: 2.0346882343292236\n",
      "205:Epoch: 5, Train_Loss: 1.8622429370880127, Test_Loss: 1.8641164302825928 *\n",
      "206:Epoch: 5, Train_Loss: 1.8808953762054443, Test_Loss: 1.8631360530853271 *\n",
      "207:Epoch: 5, Train_Loss: 1.8572958707809448, Test_Loss: 1.8624664545059204 *\n",
      "208:Epoch: 5, Train_Loss: 1.8629413843154907, Test_Loss: 1.861785888671875 *\n",
      "209:Epoch: 5, Train_Loss: 1.8705482482910156, Test_Loss: 1.8797428607940674\n",
      "210:Epoch: 5, Train_Loss: 1.873469591140747, Test_Loss: 6.2167158126831055\n",
      "211:Epoch: 5, Train_Loss: 1.9530200958251953, Test_Loss: 2.9702019691467285 *\n",
      "212:Epoch: 5, Train_Loss: 1.8551241159439087, Test_Loss: 1.8614943027496338 *\n",
      "213:Epoch: 5, Train_Loss: 1.924790382385254, Test_Loss: 1.8545455932617188 *\n",
      "214:Epoch: 5, Train_Loss: 1.8608920574188232, Test_Loss: 1.8541343212127686 *\n",
      "215:Epoch: 5, Train_Loss: 1.883461356163025, Test_Loss: 1.8602224588394165\n",
      "216:Epoch: 5, Train_Loss: 1.8921467065811157, Test_Loss: 1.8535735607147217 *\n",
      "217:Epoch: 5, Train_Loss: 2.1312272548675537, Test_Loss: 1.858748197555542\n",
      "218:Epoch: 5, Train_Loss: 1.8669309616088867, Test_Loss: 1.8519798517227173 *\n",
      "219:Epoch: 5, Train_Loss: 1.887574553489685, Test_Loss: 1.8539775609970093\n",
      "220:Epoch: 5, Train_Loss: 1.8472651243209839, Test_Loss: 1.8560490608215332\n",
      "221:Epoch: 5, Train_Loss: 1.847124457359314, Test_Loss: 1.8579262495040894\n",
      "222:Epoch: 5, Train_Loss: 1.8472282886505127, Test_Loss: 1.8536216020584106 *\n",
      "223:Epoch: 5, Train_Loss: 1.845596432685852, Test_Loss: 1.867188811302185\n",
      "224:Epoch: 5, Train_Loss: 1.8589993715286255, Test_Loss: 1.8574929237365723 *\n",
      "225:Epoch: 5, Train_Loss: 1.8581231832504272, Test_Loss: 1.8503048419952393 *\n",
      "226:Epoch: 5, Train_Loss: 1.8618521690368652, Test_Loss: 1.843319296836853 *\n",
      "227:Epoch: 5, Train_Loss: 1.8540209531784058, Test_Loss: 1.8447219133377075\n",
      "228:Epoch: 5, Train_Loss: 1.8588082790374756, Test_Loss: 1.8425943851470947 *\n",
      "229:Epoch: 5, Train_Loss: 1.8580422401428223, Test_Loss: 1.8413418531417847 *\n",
      "230:Epoch: 5, Train_Loss: 1.8434836864471436, Test_Loss: 1.8460619449615479\n",
      "231:Epoch: 5, Train_Loss: 1.839231014251709, Test_Loss: 1.8448457717895508 *\n",
      "232:Epoch: 5, Train_Loss: 1.8565219640731812, Test_Loss: 1.8433741331100464 *\n",
      "233:Epoch: 5, Train_Loss: 1.8613464832305908, Test_Loss: 1.844077706336975\n",
      "234:Epoch: 5, Train_Loss: 1.8657816648483276, Test_Loss: 1.8405777215957642 *\n",
      "235:Epoch: 5, Train_Loss: 1.8367657661437988, Test_Loss: 1.8393124341964722 *\n",
      "236:Epoch: 5, Train_Loss: 1.8850936889648438, Test_Loss: 1.8384582996368408 *\n",
      "237:Epoch: 5, Train_Loss: 1.8970351219177246, Test_Loss: 1.8379770517349243 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238:Epoch: 5, Train_Loss: 1.8766919374465942, Test_Loss: 1.8382092714309692\n",
      "239:Epoch: 5, Train_Loss: 1.8343571424484253, Test_Loss: 1.8386482000350952\n",
      "240:Epoch: 5, Train_Loss: 1.8677973747253418, Test_Loss: 1.8897662162780762\n",
      "241:Epoch: 5, Train_Loss: 1.8347824811935425, Test_Loss: 1.9685674905776978\n",
      "242:Epoch: 5, Train_Loss: 1.8518368005752563, Test_Loss: 7.323266983032227\n",
      "243:Epoch: 5, Train_Loss: 1.8338605165481567, Test_Loss: 1.8525069952011108 *\n",
      "244:Epoch: 5, Train_Loss: 1.8564331531524658, Test_Loss: 1.8309663534164429 *\n",
      "245:Epoch: 5, Train_Loss: 2.0547683238983154, Test_Loss: 1.8628252744674683\n",
      "246:Epoch: 5, Train_Loss: 5.496124267578125, Test_Loss: 1.8847672939300537\n",
      "247:Epoch: 5, Train_Loss: 3.401902675628662, Test_Loss: 1.8813434839248657 *\n",
      "248:Epoch: 5, Train_Loss: 1.848633885383606, Test_Loss: 1.8320677280426025 *\n",
      "249:Epoch: 5, Train_Loss: 1.8301012516021729, Test_Loss: 1.9396992921829224\n",
      "250:Epoch: 5, Train_Loss: 1.980893850326538, Test_Loss: 1.8667116165161133 *\n",
      "251:Epoch: 5, Train_Loss: 1.9258973598480225, Test_Loss: 1.8297078609466553 *\n",
      "252:Epoch: 5, Train_Loss: 1.8408526182174683, Test_Loss: 1.8573135137557983\n",
      "253:Epoch: 5, Train_Loss: 1.8242663145065308, Test_Loss: 1.8485738039016724 *\n",
      "254:Epoch: 5, Train_Loss: 1.8766063451766968, Test_Loss: 1.8283140659332275 *\n",
      "255:Epoch: 5, Train_Loss: 1.8426939249038696, Test_Loss: 1.8665436506271362\n",
      "256:Epoch: 5, Train_Loss: 1.8409219980239868, Test_Loss: 1.9225813150405884\n",
      "257:Epoch: 5, Train_Loss: 2.0046329498291016, Test_Loss: 1.8689508438110352 *\n",
      "258:Epoch: 5, Train_Loss: 3.096187114715576, Test_Loss: 1.890132188796997\n",
      "259:Epoch: 5, Train_Loss: 3.0955584049224854, Test_Loss: 1.8567790985107422 *\n",
      "260:Epoch: 5, Train_Loss: 1.9259278774261475, Test_Loss: 1.876556634902954\n",
      "261:Epoch: 5, Train_Loss: 1.8903621435165405, Test_Loss: 1.8414604663848877 *\n",
      "262:Epoch: 5, Train_Loss: 3.9227428436279297, Test_Loss: 1.8347883224487305 *\n",
      "263:Epoch: 5, Train_Loss: 3.1013686656951904, Test_Loss: 1.8391377925872803\n",
      "264:Epoch: 5, Train_Loss: 1.8723630905151367, Test_Loss: 1.8417972326278687\n",
      "265:Epoch: 5, Train_Loss: 1.85646653175354, Test_Loss: 1.8378950357437134 *\n",
      "266:Epoch: 5, Train_Loss: 2.23624324798584, Test_Loss: 1.8323924541473389 *\n",
      "267:Epoch: 5, Train_Loss: 3.3575353622436523, Test_Loss: 1.8205411434173584 *\n",
      "268:Epoch: 5, Train_Loss: 2.837857246398926, Test_Loss: 1.8230959177017212\n",
      "269:Epoch: 5, Train_Loss: 1.8179755210876465, Test_Loss: 1.8183397054672241 *\n",
      "270:Epoch: 5, Train_Loss: 1.8319021463394165, Test_Loss: 1.82547128200531\n",
      "271:Epoch: 5, Train_Loss: 2.0404882431030273, Test_Loss: 1.8191853761672974 *\n",
      "272:Epoch: 5, Train_Loss: 2.411914825439453, Test_Loss: 1.8341171741485596\n",
      "273:Epoch: 5, Train_Loss: 1.8541483879089355, Test_Loss: 1.8344923257827759\n",
      "274:Epoch: 5, Train_Loss: 1.8976234197616577, Test_Loss: 2.062457799911499\n",
      "275:Epoch: 5, Train_Loss: 1.946040153503418, Test_Loss: 2.2839319705963135\n",
      "276:Epoch: 5, Train_Loss: 1.9150407314300537, Test_Loss: 2.034123182296753 *\n",
      "277:Epoch: 5, Train_Loss: 1.9090138673782349, Test_Loss: 1.8600577116012573 *\n",
      "278:Epoch: 5, Train_Loss: 2.0825257301330566, Test_Loss: 1.8277373313903809 *\n",
      "279:Epoch: 5, Train_Loss: 1.984824299812317, Test_Loss: 1.8127315044403076 *\n",
      "280:Epoch: 5, Train_Loss: 1.8669323921203613, Test_Loss: 1.8815079927444458\n",
      "281:Epoch: 5, Train_Loss: 1.9851728677749634, Test_Loss: 2.4891834259033203\n",
      "282:Epoch: 5, Train_Loss: 1.9923213720321655, Test_Loss: 2.7943570613861084\n",
      "283:Epoch: 5, Train_Loss: 2.1601712703704834, Test_Loss: 1.89821457862854 *\n",
      "284:Epoch: 5, Train_Loss: 2.0237276554107666, Test_Loss: 1.8587934970855713 *\n",
      "285:Epoch: 5, Train_Loss: 1.8371044397354126, Test_Loss: 1.8101553916931152 *\n",
      "286:Epoch: 5, Train_Loss: 1.9387667179107666, Test_Loss: 1.83331298828125\n",
      "287:Epoch: 5, Train_Loss: 1.9176814556121826, Test_Loss: 1.8240340948104858 *\n",
      "288:Epoch: 5, Train_Loss: 1.8172303438186646, Test_Loss: 1.8396224975585938\n",
      "289:Epoch: 5, Train_Loss: 1.8050901889801025, Test_Loss: 1.837592363357544 *\n",
      "290:Epoch: 5, Train_Loss: 1.8022679090499878, Test_Loss: 1.8277842998504639 *\n",
      "291:Epoch: 5, Train_Loss: 1.8006067276000977, Test_Loss: 1.8094136714935303 *\n",
      "292:Epoch: 5, Train_Loss: 1.8054721355438232, Test_Loss: 1.8908109664916992\n",
      "293:Epoch: 5, Train_Loss: 1.8225191831588745, Test_Loss: 2.1730337142944336\n",
      "294:Epoch: 5, Train_Loss: 1.8629724979400635, Test_Loss: 1.9755066633224487 *\n",
      "295:Epoch: 5, Train_Loss: 1.838362455368042, Test_Loss: 1.9885867834091187\n",
      "296:Epoch: 5, Train_Loss: 1.9390865564346313, Test_Loss: 1.7968716621398926 *\n",
      "297:Epoch: 5, Train_Loss: 1.9189471006393433, Test_Loss: 1.795891284942627 *\n",
      "298:Epoch: 5, Train_Loss: 2.1009881496429443, Test_Loss: 1.7952464818954468 *\n",
      "299:Epoch: 5, Train_Loss: 1.8266185522079468, Test_Loss: 1.799911379814148\n",
      "300:Epoch: 5, Train_Loss: 1.829575777053833, Test_Loss: 1.861525058746338\n",
      "Model saved at location ../Saver/model.ckpt at epoch 5\n",
      "301:Epoch: 5, Train_Loss: 2.2176740169525146, Test_Loss: 7.131059646606445\n",
      "302:Epoch: 5, Train_Loss: 2.2733259201049805, Test_Loss: 2.1165082454681396 *\n",
      "303:Epoch: 5, Train_Loss: 1.8557912111282349, Test_Loss: 1.8035885095596313 *\n",
      "304:Epoch: 5, Train_Loss: 1.8054265975952148, Test_Loss: 1.797040343284607 *\n",
      "305:Epoch: 5, Train_Loss: 2.146334409713745, Test_Loss: 1.7926205396652222 *\n",
      "306:Epoch: 5, Train_Loss: 2.2996902465820312, Test_Loss: 1.7996011972427368\n",
      "307:Epoch: 5, Train_Loss: 1.9650664329528809, Test_Loss: 1.8042241334915161\n",
      "308:Epoch: 5, Train_Loss: 1.8033303022384644, Test_Loss: 1.8371165990829468\n",
      "309:Epoch: 5, Train_Loss: 1.820663571357727, Test_Loss: 1.7993441820144653 *\n",
      "310:Epoch: 5, Train_Loss: 2.03545880317688, Test_Loss: 1.8069275617599487\n",
      "311:Epoch: 5, Train_Loss: 3.242978572845459, Test_Loss: 1.8185820579528809\n",
      "312:Epoch: 5, Train_Loss: 2.129655599594116, Test_Loss: 1.8412716388702393\n",
      "313:Epoch: 5, Train_Loss: 1.8147536516189575, Test_Loss: 1.7929260730743408 *\n",
      "314:Epoch: 5, Train_Loss: 1.8014631271362305, Test_Loss: 1.7849385738372803 *\n",
      "315:Epoch: 5, Train_Loss: 1.7902932167053223, Test_Loss: 1.7991769313812256\n",
      "316:Epoch: 5, Train_Loss: 2.076573610305786, Test_Loss: 1.7892290353775024 *\n",
      "317:Epoch: 5, Train_Loss: 1.9089581966400146, Test_Loss: 1.7949129343032837\n",
      "318:Epoch: 5, Train_Loss: 1.7954320907592773, Test_Loss: 1.7968958616256714\n",
      "319:Epoch: 5, Train_Loss: 2.1631276607513428, Test_Loss: 1.7952477931976318 *\n",
      "320:Epoch: 5, Train_Loss: 1.7981979846954346, Test_Loss: 1.7880349159240723 *\n",
      "321:Epoch: 5, Train_Loss: 1.7990529537200928, Test_Loss: 1.7953267097473145\n",
      "322:Epoch: 5, Train_Loss: 1.8529067039489746, Test_Loss: 1.7986903190612793\n",
      "323:Epoch: 5, Train_Loss: 1.9788998365402222, Test_Loss: 1.8212690353393555\n",
      "324:Epoch: 5, Train_Loss: 1.8183298110961914, Test_Loss: 1.8043406009674072 *\n",
      "325:Epoch: 5, Train_Loss: 1.85443913936615, Test_Loss: 1.8016767501831055 *\n",
      "326:Epoch: 5, Train_Loss: 1.8445972204208374, Test_Loss: 1.7890591621398926 *\n",
      "327:Epoch: 5, Train_Loss: 1.9479414224624634, Test_Loss: 1.785812497138977 *\n",
      "328:Epoch: 5, Train_Loss: 1.8307887315750122, Test_Loss: 1.7841492891311646 *\n",
      "329:Epoch: 5, Train_Loss: 1.800788164138794, Test_Loss: 1.7871222496032715\n",
      "330:Epoch: 5, Train_Loss: 1.801405429840088, Test_Loss: 1.7878056764602661\n",
      "331:Epoch: 5, Train_Loss: 1.8173129558563232, Test_Loss: 1.8600448369979858\n",
      "332:Epoch: 5, Train_Loss: 1.9669864177703857, Test_Loss: 2.767756223678589\n",
      "333:Epoch: 5, Train_Loss: 2.143728494644165, Test_Loss: 6.234391212463379\n",
      "334:Epoch: 5, Train_Loss: 2.0461626052856445, Test_Loss: 1.7778286933898926 *\n",
      "335:Epoch: 5, Train_Loss: 2.3428938388824463, Test_Loss: 1.7997020483016968\n",
      "336:Epoch: 5, Train_Loss: 2.074100971221924, Test_Loss: 1.826869010925293\n",
      "337:Epoch: 5, Train_Loss: 2.0380470752716064, Test_Loss: 1.7762629985809326 *\n",
      "338:Epoch: 5, Train_Loss: 1.8954768180847168, Test_Loss: 1.7932560443878174\n",
      "339:Epoch: 5, Train_Loss: 1.803083896636963, Test_Loss: 1.8022105693817139\n",
      "340:Epoch: 5, Train_Loss: 1.7822017669677734, Test_Loss: 1.802619218826294\n",
      "341:Epoch: 5, Train_Loss: 1.7718069553375244, Test_Loss: 1.7960376739501953 *\n",
      "342:Epoch: 5, Train_Loss: 1.8970646858215332, Test_Loss: 1.8068653345108032\n",
      "343:Epoch: 5, Train_Loss: 2.280895233154297, Test_Loss: 1.7960866689682007 *\n",
      "344:Epoch: 5, Train_Loss: 2.0278303623199463, Test_Loss: 1.8692351579666138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345:Epoch: 5, Train_Loss: 3.406492233276367, Test_Loss: 1.793546438217163 *\n",
      "346:Epoch: 5, Train_Loss: 2.4080028533935547, Test_Loss: 1.8423880338668823\n",
      "347:Epoch: 5, Train_Loss: 2.385998249053955, Test_Loss: 1.7718585729599 *\n",
      "348:Epoch: 5, Train_Loss: 1.8974958658218384, Test_Loss: 1.8497084379196167\n",
      "349:Epoch: 5, Train_Loss: 1.780259609222412, Test_Loss: 1.7672898769378662 *\n",
      "350:Epoch: 5, Train_Loss: 1.8988972902297974, Test_Loss: 1.8472932577133179\n",
      "351:Epoch: 5, Train_Loss: 2.7092909812927246, Test_Loss: 1.8781660795211792\n",
      "352:Epoch: 5, Train_Loss: 2.5791168212890625, Test_Loss: 1.7662714719772339 *\n",
      "353:Epoch: 5, Train_Loss: 1.8021512031555176, Test_Loss: 1.7806918621063232\n",
      "354:Epoch: 5, Train_Loss: 1.8227325677871704, Test_Loss: 1.7894067764282227\n",
      "355:Epoch: 5, Train_Loss: 1.8526734113693237, Test_Loss: 1.8026258945465088\n",
      "356:Epoch: 5, Train_Loss: 2.0762221813201904, Test_Loss: 1.7991065979003906 *\n",
      "357:Epoch: 5, Train_Loss: 1.9491788148880005, Test_Loss: 1.7938504219055176 *\n",
      "358:Epoch: 5, Train_Loss: 2.3520960807800293, Test_Loss: 1.784826397895813 *\n",
      "359:Epoch: 5, Train_Loss: 2.038851261138916, Test_Loss: 1.7722477912902832 *\n",
      "360:Epoch: 5, Train_Loss: 1.8582253456115723, Test_Loss: 1.767529010772705 *\n",
      "361:Epoch: 5, Train_Loss: 1.7861371040344238, Test_Loss: 1.8363656997680664\n",
      "362:Epoch: 5, Train_Loss: 1.7763729095458984, Test_Loss: 1.7900044918060303 *\n",
      "363:Epoch: 5, Train_Loss: 1.785210132598877, Test_Loss: 1.7548750638961792 *\n",
      "364:Epoch: 5, Train_Loss: 1.824722170829773, Test_Loss: 1.8428484201431274\n",
      "365:Epoch: 5, Train_Loss: 1.7748315334320068, Test_Loss: 1.9126245975494385\n",
      "366:Epoch: 5, Train_Loss: 1.8012746572494507, Test_Loss: 1.9908382892608643\n",
      "367:Epoch: 5, Train_Loss: 18.64438819885254, Test_Loss: 1.8739523887634277 *\n",
      "368:Epoch: 5, Train_Loss: 1.77679443359375, Test_Loss: 1.7656735181808472 *\n",
      "369:Epoch: 5, Train_Loss: 4.598334312438965, Test_Loss: 1.772282600402832\n",
      "370:Epoch: 5, Train_Loss: 2.9188060760498047, Test_Loss: 1.7543866634368896 *\n",
      "371:Epoch: 5, Train_Loss: 1.7698980569839478, Test_Loss: 1.807395339012146\n",
      "372:Epoch: 5, Train_Loss: 1.8222143650054932, Test_Loss: 2.279200553894043\n",
      "373:Epoch: 5, Train_Loss: 8.069161415100098, Test_Loss: 2.2362542152404785 *\n",
      "374:Epoch: 5, Train_Loss: 6.165955066680908, Test_Loss: 1.8158594369888306 *\n",
      "375:Epoch: 5, Train_Loss: 1.7820814847946167, Test_Loss: 1.7894911766052246 *\n",
      "376:Epoch: 5, Train_Loss: 2.0229849815368652, Test_Loss: 1.8328133821487427\n",
      "377:Epoch: 5, Train_Loss: 6.80628776550293, Test_Loss: 1.9624173641204834\n",
      "378:Epoch: 5, Train_Loss: 1.9746755361557007, Test_Loss: 2.0650341510772705\n",
      "379:Epoch: 5, Train_Loss: 1.8705949783325195, Test_Loss: 1.9283663034439087 *\n",
      "380:Epoch: 5, Train_Loss: 1.8279001712799072, Test_Loss: 2.311072826385498\n",
      "381:Epoch: 5, Train_Loss: 1.801745891571045, Test_Loss: 1.9562742710113525 *\n",
      "382:Epoch: 5, Train_Loss: 1.7787809371948242, Test_Loss: 2.020789384841919\n",
      "383:Epoch: 5, Train_Loss: 1.739039421081543, Test_Loss: 1.9075405597686768 *\n",
      "384:Epoch: 5, Train_Loss: 1.7418429851531982, Test_Loss: 2.071322202682495\n",
      "385:Epoch: 5, Train_Loss: 1.7359538078308105, Test_Loss: 1.8209978342056274 *\n",
      "386:Epoch: 5, Train_Loss: 1.739816427230835, Test_Loss: 1.8780773878097534\n",
      "387:Epoch: 5, Train_Loss: 1.7856141328811646, Test_Loss: 1.888490080833435\n",
      "388:Epoch: 5, Train_Loss: 1.7474931478500366, Test_Loss: 1.838791847229004 *\n",
      "389:Epoch: 5, Train_Loss: 1.7598435878753662, Test_Loss: 1.7903014421463013 *\n",
      "390:Epoch: 5, Train_Loss: 1.7676215171813965, Test_Loss: 1.748060703277588 *\n",
      "391:Epoch: 5, Train_Loss: 1.7691460847854614, Test_Loss: 2.0026350021362305\n",
      "392:Epoch: 5, Train_Loss: 1.735413908958435, Test_Loss: 7.137950897216797\n",
      "393:Epoch: 5, Train_Loss: 1.7353310585021973, Test_Loss: 1.8414239883422852 *\n",
      "394:Epoch: 5, Train_Loss: 1.7324862480163574, Test_Loss: 1.7473407983779907 *\n",
      "395:Epoch: 5, Train_Loss: 1.7325290441513062, Test_Loss: 1.7382766008377075 *\n",
      "396:Epoch: 5, Train_Loss: 1.7308675050735474, Test_Loss: 1.734658122062683 *\n",
      "397:Epoch: 5, Train_Loss: 1.728762149810791, Test_Loss: 1.734799861907959\n",
      "398:Epoch: 5, Train_Loss: 1.728053092956543, Test_Loss: 1.7320421934127808 *\n",
      "399:Epoch: 5, Train_Loss: 1.7271573543548584, Test_Loss: 1.738003134727478\n",
      "400:Epoch: 5, Train_Loss: 1.7273503541946411, Test_Loss: 1.7311651706695557 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 5\n",
      "401:Epoch: 5, Train_Loss: 1.7259587049484253, Test_Loss: 1.7309010028839111 *\n",
      "402:Epoch: 5, Train_Loss: 1.7252047061920166, Test_Loss: 1.7301839590072632 *\n",
      "403:Epoch: 5, Train_Loss: 1.728759527206421, Test_Loss: 1.7336570024490356\n",
      "404:Epoch: 5, Train_Loss: 1.75074303150177, Test_Loss: 1.7415074110031128\n",
      "405:Epoch: 5, Train_Loss: 1.7597159147262573, Test_Loss: 1.7415672540664673\n",
      "406:Epoch: 5, Train_Loss: 1.7347151041030884, Test_Loss: 1.7364611625671387 *\n",
      "407:Epoch: 5, Train_Loss: 1.7261073589324951, Test_Loss: 1.7267285585403442 *\n",
      "408:Epoch: 5, Train_Loss: 6.108620643615723, Test_Loss: 1.724590539932251 *\n",
      "409:Epoch: 5, Train_Loss: 6.796651840209961, Test_Loss: 1.7278097867965698\n",
      "410:Epoch: 5, Train_Loss: 1.7430998086929321, Test_Loss: 1.7298318147659302\n",
      "411:Epoch: 5, Train_Loss: 1.727134346961975, Test_Loss: 1.7289228439331055 *\n",
      "412:Epoch: 5, Train_Loss: 1.7342710494995117, Test_Loss: 1.7362481355667114\n",
      "413:Epoch: 5, Train_Loss: 1.7397228479385376, Test_Loss: 1.7313332557678223 *\n",
      "414:Epoch: 5, Train_Loss: 1.7487071752548218, Test_Loss: 1.7412649393081665\n",
      "415:Epoch: 5, Train_Loss: 1.7316761016845703, Test_Loss: 1.7388643026351929 *\n",
      "416:Epoch: 5, Train_Loss: 1.777069330215454, Test_Loss: 1.7354998588562012 *\n",
      "417:Epoch: 5, Train_Loss: 1.9782819747924805, Test_Loss: 1.7328932285308838 *\n",
      "418:Epoch: 5, Train_Loss: 1.9328336715698242, Test_Loss: 1.734417200088501\n",
      "419:Epoch: 5, Train_Loss: 1.8015170097351074, Test_Loss: 1.7293940782546997 *\n",
      "420:Epoch: 5, Train_Loss: 1.782386302947998, Test_Loss: 1.7315893173217773\n",
      "421:Epoch: 5, Train_Loss: 1.8495584726333618, Test_Loss: 1.7425563335418701\n",
      "422:Epoch: 5, Train_Loss: 1.8325845003128052, Test_Loss: 1.7791287899017334\n",
      "423:Epoch: 5, Train_Loss: 1.856130599975586, Test_Loss: 4.114495277404785\n",
      "424:Epoch: 5, Train_Loss: 1.830761194229126, Test_Loss: 5.057680130004883\n",
      "425:Epoch: 5, Train_Loss: 1.7927706241607666, Test_Loss: 1.7188876867294312 *\n",
      "426:Epoch: 5, Train_Loss: 1.7141761779785156, Test_Loss: 1.7115792036056519 *\n",
      "427:Epoch: 5, Train_Loss: 1.7986706495285034, Test_Loss: 1.758697748184204\n",
      "428:Epoch: 5, Train_Loss: 1.767418384552002, Test_Loss: 1.7532553672790527 *\n",
      "429:Epoch: 5, Train_Loss: 1.7188655138015747, Test_Loss: 1.7549527883529663\n",
      "430:Epoch: 5, Train_Loss: 1.7088959217071533, Test_Loss: 1.7552857398986816\n",
      "431:Epoch: 5, Train_Loss: 1.7083978652954102, Test_Loss: 1.8323320150375366\n",
      "432:Epoch: 5, Train_Loss: 1.7210876941680908, Test_Loss: 1.7121320962905884 *\n",
      "433:Epoch: 5, Train_Loss: 5.798744201660156, Test_Loss: 1.7281925678253174\n",
      "434:Epoch: 5, Train_Loss: 3.576503038406372, Test_Loss: 1.7316298484802246\n",
      "435:Epoch: 5, Train_Loss: 1.712041974067688, Test_Loss: 1.720371961593628 *\n",
      "436:Epoch: 5, Train_Loss: 1.719872236251831, Test_Loss: 1.715900182723999 *\n",
      "437:Epoch: 5, Train_Loss: 1.721961259841919, Test_Loss: 1.7748475074768066\n",
      "438:Epoch: 5, Train_Loss: 1.7169100046157837, Test_Loss: 1.7717418670654297 *\n",
      "439:Epoch: 5, Train_Loss: 1.7081347703933716, Test_Loss: 1.809622883796692\n",
      "440:Epoch: 5, Train_Loss: 1.7095867395401, Test_Loss: 1.8466007709503174\n",
      "441:Epoch: 5, Train_Loss: 1.7264386415481567, Test_Loss: 1.7153576612472534 *\n",
      "442:Epoch: 5, Train_Loss: 1.7407770156860352, Test_Loss: 1.7208768129348755\n",
      "443:Epoch: 5, Train_Loss: 1.7125781774520874, Test_Loss: 1.7092456817626953 *\n",
      "444:Epoch: 5, Train_Loss: 1.7061737775802612, Test_Loss: 1.7051329612731934 *\n",
      "445:Epoch: 5, Train_Loss: 1.7042559385299683, Test_Loss: 1.7054729461669922\n",
      "446:Epoch: 5, Train_Loss: 1.7216601371765137, Test_Loss: 1.7053309679031372 *\n",
      "447:Epoch: 5, Train_Loss: 1.70218026638031, Test_Loss: 1.7042793035507202 *\n",
      "448:Epoch: 5, Train_Loss: 1.700347661972046, Test_Loss: 1.7037290334701538 *\n",
      "449:Epoch: 5, Train_Loss: 1.7055081129074097, Test_Loss: 1.7106927633285522\n",
      "450:Epoch: 5, Train_Loss: 1.7590198516845703, Test_Loss: 1.705573320388794 *\n",
      "451:Epoch: 5, Train_Loss: 1.745316505432129, Test_Loss: 1.7033088207244873 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452:Epoch: 5, Train_Loss: 1.6968368291854858, Test_Loss: 1.7040776014328003\n",
      "453:Epoch: 5, Train_Loss: 1.698372483253479, Test_Loss: 1.7089661359786987\n",
      "454:Epoch: 5, Train_Loss: 1.7356693744659424, Test_Loss: 1.7430129051208496\n",
      "1:Epoch: 6, Train_Loss: 1.8062827587127686, Test_Loss: 1.7117985486984253 *\n",
      "2:Epoch: 6, Train_Loss: 1.7663754224777222, Test_Loss: 2.1650352478027344\n",
      "3:Epoch: 6, Train_Loss: 1.7693419456481934, Test_Loss: 2.1536309719085693 *\n",
      "4:Epoch: 6, Train_Loss: 1.7277565002441406, Test_Loss: 1.8135079145431519 *\n",
      "5:Epoch: 6, Train_Loss: 1.7717441320419312, Test_Loss: 1.7112023830413818 *\n",
      "6:Epoch: 6, Train_Loss: 1.7508397102355957, Test_Loss: 1.707574486732483 *\n",
      "7:Epoch: 6, Train_Loss: 1.734131932258606, Test_Loss: 1.7387771606445312\n",
      "8:Epoch: 6, Train_Loss: 1.775603175163269, Test_Loss: 2.0055179595947266\n",
      "9:Epoch: 6, Train_Loss: 1.7348514795303345, Test_Loss: 3.135166645050049\n",
      "10:Epoch: 6, Train_Loss: 1.704530954360962, Test_Loss: 2.5641403198242188 *\n",
      "11:Epoch: 6, Train_Loss: 1.6906013488769531, Test_Loss: 1.74236261844635 *\n",
      "12:Epoch: 6, Train_Loss: 1.6897534132003784, Test_Loss: 1.7239242792129517 *\n",
      "13:Epoch: 6, Train_Loss: 1.6883147954940796, Test_Loss: 1.6930482387542725 *\n",
      "14:Epoch: 6, Train_Loss: 1.6876734495162964, Test_Loss: 1.6936115026474\n",
      "15:Epoch: 6, Train_Loss: 1.6870667934417725, Test_Loss: 1.6949068307876587\n",
      "16:Epoch: 6, Train_Loss: 5.285040855407715, Test_Loss: 1.7178065776824951\n",
      "17:Epoch: 6, Train_Loss: 3.0589513778686523, Test_Loss: 1.7638603448867798\n",
      "18:Epoch: 6, Train_Loss: 1.684728980064392, Test_Loss: 1.685093641281128 *\n",
      "19:Epoch: 6, Train_Loss: 1.7071584463119507, Test_Loss: 1.7200281620025635\n",
      "20:Epoch: 6, Train_Loss: 1.6915264129638672, Test_Loss: 1.8130948543548584\n",
      "21:Epoch: 6, Train_Loss: 1.6807506084442139, Test_Loss: 2.044969081878662\n",
      "22:Epoch: 6, Train_Loss: 1.6817845106124878, Test_Loss: 1.8948379755020142 *\n",
      "23:Epoch: 6, Train_Loss: 1.6801929473876953, Test_Loss: 1.6892926692962646 *\n",
      "24:Epoch: 6, Train_Loss: 1.6794438362121582, Test_Loss: 1.6827818155288696 *\n",
      "25:Epoch: 6, Train_Loss: 1.6783784627914429, Test_Loss: 1.6824451684951782 *\n",
      "26:Epoch: 6, Train_Loss: 1.7122851610183716, Test_Loss: 1.6823533773422241 *\n",
      "27:Epoch: 6, Train_Loss: 1.780472755432129, Test_Loss: 1.6864032745361328\n",
      "28:Epoch: 6, Train_Loss: 1.773236870765686, Test_Loss: 2.689542293548584\n",
      "29:Epoch: 6, Train_Loss: 1.7833607196807861, Test_Loss: 6.141762733459473\n",
      "30:Epoch: 6, Train_Loss: 1.6956785917282104, Test_Loss: 1.6947474479675293 *\n",
      "31:Epoch: 6, Train_Loss: 1.6947370767593384, Test_Loss: 1.6793793439865112 *\n",
      "32:Epoch: 6, Train_Loss: 1.8789095878601074, Test_Loss: 1.6779730319976807 *\n",
      "33:Epoch: 6, Train_Loss: 1.895324945449829, Test_Loss: 1.6830766201019287\n",
      "34:Epoch: 6, Train_Loss: 1.8828314542770386, Test_Loss: 1.6771392822265625 *\n",
      "35:Epoch: 6, Train_Loss: 1.7220314741134644, Test_Loss: 1.6759177446365356 *\n",
      "36:Epoch: 6, Train_Loss: 1.6713390350341797, Test_Loss: 1.67777419090271\n",
      "37:Epoch: 6, Train_Loss: 1.6719075441360474, Test_Loss: 1.6750317811965942 *\n",
      "38:Epoch: 6, Train_Loss: 1.6745299100875854, Test_Loss: 1.6765378713607788\n",
      "39:Epoch: 6, Train_Loss: 1.6778374910354614, Test_Loss: 1.6734981536865234 *\n",
      "40:Epoch: 6, Train_Loss: 1.6769682168960571, Test_Loss: 1.6780290603637695\n",
      "41:Epoch: 6, Train_Loss: 1.6726030111312866, Test_Loss: 1.6892518997192383\n",
      "42:Epoch: 6, Train_Loss: 1.6704281568527222, Test_Loss: 1.688657283782959 *\n",
      "43:Epoch: 6, Train_Loss: 1.6683158874511719, Test_Loss: 1.678185224533081 *\n",
      "44:Epoch: 6, Train_Loss: 1.6732290983200073, Test_Loss: 1.6688297986984253 *\n",
      "45:Epoch: 6, Train_Loss: 1.754075288772583, Test_Loss: 1.6682426929473877 *\n",
      "46:Epoch: 6, Train_Loss: 1.8523660898208618, Test_Loss: 1.6681184768676758 *\n",
      "47:Epoch: 6, Train_Loss: 1.8387171030044556, Test_Loss: 1.6690678596496582\n",
      "48:Epoch: 6, Train_Loss: 1.7511541843414307, Test_Loss: 1.6678402423858643 *\n",
      "49:Epoch: 6, Train_Loss: 1.8059167861938477, Test_Loss: 1.6679984331130981\n",
      "50:Epoch: 6, Train_Loss: 1.843827486038208, Test_Loss: 1.6671380996704102 *\n",
      "51:Epoch: 6, Train_Loss: 1.6758078336715698, Test_Loss: 1.669327974319458\n",
      "52:Epoch: 6, Train_Loss: 1.837881326675415, Test_Loss: 1.666426420211792 *\n",
      "53:Epoch: 6, Train_Loss: 1.7945809364318848, Test_Loss: 1.6669062376022339\n",
      "54:Epoch: 6, Train_Loss: 1.9414142370224, Test_Loss: 1.6653711795806885 *\n",
      "55:Epoch: 6, Train_Loss: 1.6761474609375, Test_Loss: 1.6656831502914429\n",
      "56:Epoch: 6, Train_Loss: 2.3369626998901367, Test_Loss: 1.6633256673812866 *\n",
      "57:Epoch: 6, Train_Loss: 4.261404037475586, Test_Loss: 1.6636192798614502\n",
      "58:Epoch: 6, Train_Loss: 1.702465534210205, Test_Loss: 1.689107894897461\n",
      "59:Epoch: 6, Train_Loss: 1.704556941986084, Test_Loss: 1.7044329643249512\n",
      "60:Epoch: 6, Train_Loss: 1.7056902647018433, Test_Loss: 4.968774795532227\n",
      "61:Epoch: 6, Train_Loss: 1.707383394241333, Test_Loss: 3.9205880165100098 *\n",
      "62:Epoch: 6, Train_Loss: 1.6582242250442505, Test_Loss: 1.6592799425125122 *\n",
      "63:Epoch: 6, Train_Loss: 1.6621607542037964, Test_Loss: 1.6590381860733032 *\n",
      "64:Epoch: 6, Train_Loss: 1.7830634117126465, Test_Loss: 1.7056995630264282\n",
      "65:Epoch: 6, Train_Loss: 1.7830981016159058, Test_Loss: 1.7087057828903198\n",
      "66:Epoch: 6, Train_Loss: 1.7808372974395752, Test_Loss: 1.683029294013977 *\n",
      "67:Epoch: 6, Train_Loss: 1.746005892753601, Test_Loss: 1.7254019975662231\n",
      "68:Epoch: 6, Train_Loss: 1.7459371089935303, Test_Loss: 1.7627289295196533\n",
      "69:Epoch: 6, Train_Loss: 1.6865472793579102, Test_Loss: 1.6542903184890747 *\n",
      "70:Epoch: 6, Train_Loss: 1.6841315031051636, Test_Loss: 1.6832231283187866\n",
      "71:Epoch: 6, Train_Loss: 1.6638199090957642, Test_Loss: 1.6703722476959229 *\n",
      "72:Epoch: 6, Train_Loss: 1.6785098314285278, Test_Loss: 1.6661125421524048 *\n",
      "73:Epoch: 6, Train_Loss: 1.660185694694519, Test_Loss: 1.655153512954712 *\n",
      "74:Epoch: 6, Train_Loss: 1.6487147808074951, Test_Loss: 1.755018949508667\n",
      "75:Epoch: 6, Train_Loss: 1.6970945596694946, Test_Loss: 1.7113425731658936 *\n",
      "76:Epoch: 6, Train_Loss: 1.7113516330718994, Test_Loss: 1.7511094808578491\n",
      "77:Epoch: 6, Train_Loss: 1.6836705207824707, Test_Loss: 1.7404694557189941 *\n",
      "78:Epoch: 6, Train_Loss: 1.6464918851852417, Test_Loss: 1.6778696775436401 *\n",
      "79:Epoch: 6, Train_Loss: 1.6463450193405151, Test_Loss: 1.6698912382125854 *\n",
      "80:Epoch: 6, Train_Loss: 1.6453624963760376, Test_Loss: 1.6640276908874512 *\n",
      "81:Epoch: 6, Train_Loss: 1.6445995569229126, Test_Loss: 1.6613452434539795 *\n",
      "82:Epoch: 6, Train_Loss: 1.6447513103485107, Test_Loss: 1.664007306098938\n",
      "83:Epoch: 6, Train_Loss: 1.6433078050613403, Test_Loss: 1.6642773151397705\n",
      "84:Epoch: 6, Train_Loss: 1.64328134059906, Test_Loss: 1.6639868021011353 *\n",
      "85:Epoch: 6, Train_Loss: 1.6429301500320435, Test_Loss: 1.6597832441329956 *\n",
      "86:Epoch: 6, Train_Loss: 1.6415374279022217, Test_Loss: 1.671185851097107\n",
      "87:Epoch: 6, Train_Loss: 1.6411967277526855, Test_Loss: 1.6628493070602417 *\n",
      "88:Epoch: 6, Train_Loss: 1.6470288038253784, Test_Loss: 1.6564606428146362 *\n",
      "89:Epoch: 6, Train_Loss: 1.6492468118667603, Test_Loss: 1.6473877429962158 *\n",
      "90:Epoch: 6, Train_Loss: 1.6469535827636719, Test_Loss: 1.6775699853897095\n",
      "91:Epoch: 6, Train_Loss: 1.6580252647399902, Test_Loss: 1.697277307510376\n",
      "92:Epoch: 6, Train_Loss: 1.6416654586791992, Test_Loss: 1.72476327419281\n",
      "93:Epoch: 6, Train_Loss: 1.6409376859664917, Test_Loss: 2.2042131423950195\n",
      "94:Epoch: 6, Train_Loss: 1.6370971202850342, Test_Loss: 2.0730834007263184 *\n",
      "95:Epoch: 6, Train_Loss: 1.6370824575424194, Test_Loss: 1.7553850412368774 *\n",
      "96:Epoch: 6, Train_Loss: 1.6567763090133667, Test_Loss: 1.6676472425460815 *\n",
      "97:Epoch: 6, Train_Loss: 1.6490163803100586, Test_Loss: 1.6531240940093994 *\n",
      "98:Epoch: 6, Train_Loss: 1.6354604959487915, Test_Loss: 1.6938556432724\n",
      "99:Epoch: 6, Train_Loss: 1.6340198516845703, Test_Loss: 2.085716485977173\n",
      "100:Epoch: 6, Train_Loss: 1.6409907341003418, Test_Loss: 3.0009546279907227\n",
      "Model saved at location ../Saver/model.ckpt at epoch 6\n",
      "101:Epoch: 6, Train_Loss: 1.7176029682159424, Test_Loss: 2.209747314453125 *\n",
      "102:Epoch: 6, Train_Loss: 1.6582705974578857, Test_Loss: 1.7243547439575195 *\n",
      "103:Epoch: 6, Train_Loss: 1.6761997938156128, Test_Loss: 1.6483256816864014 *\n",
      "104:Epoch: 6, Train_Loss: 1.6316783428192139, Test_Loss: 1.6393247842788696 *\n",
      "105:Epoch: 6, Train_Loss: 1.6544475555419922, Test_Loss: 1.6336044073104858 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106:Epoch: 6, Train_Loss: 1.669939637184143, Test_Loss: 1.6388112306594849\n",
      "107:Epoch: 6, Train_Loss: 1.6314142942428589, Test_Loss: 1.6653006076812744\n",
      "108:Epoch: 6, Train_Loss: 1.6510443687438965, Test_Loss: 1.694218397140503\n",
      "109:Epoch: 6, Train_Loss: 1.6641671657562256, Test_Loss: 1.6313508749008179 *\n",
      "110:Epoch: 6, Train_Loss: 1.7160636186599731, Test_Loss: 1.693656325340271\n",
      "111:Epoch: 6, Train_Loss: 1.7307673692703247, Test_Loss: 1.7847490310668945\n",
      "112:Epoch: 6, Train_Loss: 1.7166423797607422, Test_Loss: 1.9416308403015137\n",
      "113:Epoch: 6, Train_Loss: 1.667218804359436, Test_Loss: 1.825040340423584 *\n",
      "114:Epoch: 6, Train_Loss: 1.6284456253051758, Test_Loss: 1.6316453218460083 *\n",
      "115:Epoch: 6, Train_Loss: 1.6549642086029053, Test_Loss: 1.6283743381500244 *\n",
      "116:Epoch: 6, Train_Loss: 1.625545620918274, Test_Loss: 1.6277587413787842 *\n",
      "117:Epoch: 6, Train_Loss: 1.633374571800232, Test_Loss: 1.6271287202835083 *\n",
      "118:Epoch: 6, Train_Loss: 1.6346781253814697, Test_Loss: 1.635812520980835\n",
      "119:Epoch: 6, Train_Loss: 1.637182354927063, Test_Loss: 3.8563954830169678\n",
      "120:Epoch: 6, Train_Loss: 1.7054853439331055, Test_Loss: 4.955138206481934\n",
      "121:Epoch: 6, Train_Loss: 1.6347150802612305, Test_Loss: 1.6314117908477783 *\n",
      "122:Epoch: 6, Train_Loss: 1.7078006267547607, Test_Loss: 1.6235166788101196 *\n",
      "123:Epoch: 6, Train_Loss: 1.6279418468475342, Test_Loss: 1.621658444404602 *\n",
      "124:Epoch: 6, Train_Loss: 1.6493149995803833, Test_Loss: 1.6259396076202393\n",
      "125:Epoch: 6, Train_Loss: 1.627175211906433, Test_Loss: 1.6215696334838867 *\n",
      "126:Epoch: 6, Train_Loss: 1.8779772520065308, Test_Loss: 1.6246440410614014\n",
      "127:Epoch: 6, Train_Loss: 1.6868342161178589, Test_Loss: 1.6226298809051514 *\n",
      "128:Epoch: 6, Train_Loss: 1.6385889053344727, Test_Loss: 1.6233503818511963\n",
      "129:Epoch: 6, Train_Loss: 1.6297954320907593, Test_Loss: 1.6240081787109375\n",
      "130:Epoch: 6, Train_Loss: 1.6174278259277344, Test_Loss: 1.6200693845748901 *\n",
      "131:Epoch: 6, Train_Loss: 1.617569923400879, Test_Loss: 1.625006079673767\n",
      "132:Epoch: 6, Train_Loss: 1.616904377937317, Test_Loss: 1.637466311454773\n",
      "133:Epoch: 6, Train_Loss: 1.6255419254302979, Test_Loss: 1.6310687065124512 *\n",
      "134:Epoch: 6, Train_Loss: 1.6348878145217896, Test_Loss: 1.6209008693695068 *\n",
      "135:Epoch: 6, Train_Loss: 1.644020438194275, Test_Loss: 1.6142082214355469 *\n",
      "136:Epoch: 6, Train_Loss: 1.6286691427230835, Test_Loss: 1.6159173250198364\n",
      "137:Epoch: 6, Train_Loss: 1.6308469772338867, Test_Loss: 1.6136455535888672 *\n",
      "138:Epoch: 6, Train_Loss: 1.6380962133407593, Test_Loss: 1.614688515663147\n",
      "139:Epoch: 6, Train_Loss: 1.6151764392852783, Test_Loss: 1.6155496835708618\n",
      "140:Epoch: 6, Train_Loss: 1.6118576526641846, Test_Loss: 1.6121153831481934 *\n",
      "141:Epoch: 6, Train_Loss: 1.6158827543258667, Test_Loss: 1.612756609916687\n",
      "142:Epoch: 6, Train_Loss: 1.6325883865356445, Test_Loss: 1.6166563034057617\n",
      "143:Epoch: 6, Train_Loss: 1.6386455297470093, Test_Loss: 1.6116125583648682 *\n",
      "144:Epoch: 6, Train_Loss: 1.6106047630310059, Test_Loss: 1.6125438213348389\n",
      "145:Epoch: 6, Train_Loss: 1.6453825235366821, Test_Loss: 1.6096279621124268 *\n",
      "146:Epoch: 6, Train_Loss: 1.6741224527359009, Test_Loss: 1.610730528831482\n",
      "147:Epoch: 6, Train_Loss: 1.6755154132843018, Test_Loss: 1.6101378202438354 *\n",
      "148:Epoch: 6, Train_Loss: 1.6059964895248413, Test_Loss: 1.6089214086532593 *\n",
      "149:Epoch: 6, Train_Loss: 1.6294989585876465, Test_Loss: 1.652486801147461\n",
      "150:Epoch: 6, Train_Loss: 1.6111798286437988, Test_Loss: 1.6369645595550537 *\n",
      "151:Epoch: 6, Train_Loss: 1.629284381866455, Test_Loss: 6.129149436950684\n",
      "152:Epoch: 6, Train_Loss: 1.6069202423095703, Test_Loss: 2.658953905105591 *\n",
      "153:Epoch: 6, Train_Loss: 1.6271005868911743, Test_Loss: 1.6038002967834473 *\n",
      "154:Epoch: 6, Train_Loss: 1.6623479127883911, Test_Loss: 1.6165621280670166\n",
      "155:Epoch: 6, Train_Loss: 3.9935379028320312, Test_Loss: 1.6524204015731812\n",
      "156:Epoch: 6, Train_Loss: 4.630979537963867, Test_Loss: 1.6593828201293945\n",
      "157:Epoch: 6, Train_Loss: 1.62204909324646, Test_Loss: 1.6110490560531616 *\n",
      "158:Epoch: 6, Train_Loss: 1.6006765365600586, Test_Loss: 1.696940302848816\n",
      "159:Epoch: 6, Train_Loss: 1.7095131874084473, Test_Loss: 1.6842039823532104 *\n",
      "160:Epoch: 6, Train_Loss: 1.7473849058151245, Test_Loss: 1.6006461381912231 *\n",
      "161:Epoch: 6, Train_Loss: 1.620527744293213, Test_Loss: 1.6366443634033203\n",
      "162:Epoch: 6, Train_Loss: 1.599467396736145, Test_Loss: 1.6150552034378052 *\n",
      "163:Epoch: 6, Train_Loss: 1.6348514556884766, Test_Loss: 1.6088875532150269 *\n",
      "164:Epoch: 6, Train_Loss: 1.640435814857483, Test_Loss: 1.6008599996566772 *\n",
      "165:Epoch: 6, Train_Loss: 1.6100008487701416, Test_Loss: 1.714227557182312\n",
      "166:Epoch: 6, Train_Loss: 1.6239314079284668, Test_Loss: 1.6340484619140625 *\n",
      "167:Epoch: 6, Train_Loss: 2.79622220993042, Test_Loss: 1.7100865840911865\n",
      "168:Epoch: 6, Train_Loss: 3.0384342670440674, Test_Loss: 1.655964732170105 *\n",
      "169:Epoch: 6, Train_Loss: 1.8211662769317627, Test_Loss: 1.639817476272583 *\n",
      "170:Epoch: 6, Train_Loss: 1.6765769720077515, Test_Loss: 1.6136701107025146 *\n",
      "171:Epoch: 6, Train_Loss: 3.1029317378997803, Test_Loss: 1.610862374305725 *\n",
      "172:Epoch: 6, Train_Loss: 3.4824647903442383, Test_Loss: 1.6075857877731323 *\n",
      "173:Epoch: 6, Train_Loss: 1.6594949960708618, Test_Loss: 1.6097904443740845\n",
      "174:Epoch: 6, Train_Loss: 1.64070463180542, Test_Loss: 1.610844612121582\n",
      "175:Epoch: 6, Train_Loss: 1.7545291185379028, Test_Loss: 1.6061086654663086 *\n",
      "176:Epoch: 6, Train_Loss: 3.145559310913086, Test_Loss: 1.5954445600509644 *\n",
      "177:Epoch: 6, Train_Loss: 2.896982431411743, Test_Loss: 1.6096371412277222\n",
      "178:Epoch: 6, Train_Loss: 1.6084121465682983, Test_Loss: 1.5951558351516724 *\n",
      "179:Epoch: 6, Train_Loss: 1.6167861223220825, Test_Loss: 1.5949466228485107 *\n",
      "180:Epoch: 6, Train_Loss: 1.637541651725769, Test_Loss: 1.6079092025756836\n",
      "181:Epoch: 6, Train_Loss: 2.345938205718994, Test_Loss: 1.6070400476455688 *\n",
      "182:Epoch: 6, Train_Loss: 1.6258692741394043, Test_Loss: 1.6074578762054443\n",
      "183:Epoch: 6, Train_Loss: 1.6602683067321777, Test_Loss: 1.7503743171691895\n",
      "184:Epoch: 6, Train_Loss: 1.6338180303573608, Test_Loss: 2.0562009811401367\n",
      "185:Epoch: 6, Train_Loss: 1.730288028717041, Test_Loss: 1.9356580972671509 *\n",
      "186:Epoch: 6, Train_Loss: 1.6880515813827515, Test_Loss: 1.694151759147644 *\n",
      "187:Epoch: 6, Train_Loss: 1.812082290649414, Test_Loss: 1.5932369232177734 *\n",
      "188:Epoch: 6, Train_Loss: 1.8471670150756836, Test_Loss: 1.5992766618728638\n",
      "189:Epoch: 6, Train_Loss: 1.6281620264053345, Test_Loss: 1.6322214603424072\n",
      "190:Epoch: 6, Train_Loss: 1.7213058471679688, Test_Loss: 2.0338327884674072\n",
      "191:Epoch: 6, Train_Loss: 1.7446991205215454, Test_Loss: 2.5427238941192627\n",
      "192:Epoch: 6, Train_Loss: 1.8777776956558228, Test_Loss: 1.8723958730697632 *\n",
      "193:Epoch: 6, Train_Loss: 1.7667914628982544, Test_Loss: 1.636448621749878 *\n",
      "194:Epoch: 6, Train_Loss: 1.6103817224502563, Test_Loss: 1.6086909770965576 *\n",
      "195:Epoch: 6, Train_Loss: 1.7027350664138794, Test_Loss: 1.6421886682510376\n",
      "196:Epoch: 6, Train_Loss: 1.8269550800323486, Test_Loss: 1.6352144479751587 *\n",
      "197:Epoch: 6, Train_Loss: 1.6054677963256836, Test_Loss: 1.651677131652832\n",
      "198:Epoch: 6, Train_Loss: 1.6009825468063354, Test_Loss: 1.598237156867981 *\n",
      "199:Epoch: 6, Train_Loss: 1.5879075527191162, Test_Loss: 1.6381300687789917\n",
      "200:Epoch: 6, Train_Loss: 1.58924400806427, Test_Loss: 1.5940916538238525 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 6\n",
      "201:Epoch: 6, Train_Loss: 1.5878740549087524, Test_Loss: 1.6732999086380005\n",
      "202:Epoch: 6, Train_Loss: 1.5931320190429688, Test_Loss: 1.8949387073516846\n",
      "203:Epoch: 6, Train_Loss: 1.6766011714935303, Test_Loss: 1.749197006225586 *\n",
      "204:Epoch: 6, Train_Loss: 1.6037923097610474, Test_Loss: 1.8420003652572632\n",
      "205:Epoch: 6, Train_Loss: 1.718247890472412, Test_Loss: 1.577579379081726 *\n",
      "206:Epoch: 6, Train_Loss: 1.694724202156067, Test_Loss: 1.5739766359329224 *\n",
      "207:Epoch: 6, Train_Loss: 1.9292569160461426, Test_Loss: 1.5734872817993164 *\n",
      "208:Epoch: 6, Train_Loss: 1.5872304439544678, Test_Loss: 1.5735174417495728\n",
      "209:Epoch: 6, Train_Loss: 1.6105552911758423, Test_Loss: 1.626858115196228\n",
      "210:Epoch: 6, Train_Loss: 1.8411370515823364, Test_Loss: 5.134139537811279\n",
      "211:Epoch: 6, Train_Loss: 2.076732635498047, Test_Loss: 3.4691781997680664 *\n",
      "212:Epoch: 6, Train_Loss: 1.7693390846252441, Test_Loss: 1.5809918642044067 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213:Epoch: 6, Train_Loss: 1.5882583856582642, Test_Loss: 1.5776784420013428 *\n",
      "214:Epoch: 6, Train_Loss: 1.8338260650634766, Test_Loss: 1.576297402381897 *\n",
      "215:Epoch: 6, Train_Loss: 2.115628957748413, Test_Loss: 1.5752490758895874 *\n",
      "216:Epoch: 6, Train_Loss: 1.8757648468017578, Test_Loss: 1.60000479221344\n",
      "217:Epoch: 6, Train_Loss: 1.593514084815979, Test_Loss: 1.6172584295272827\n",
      "218:Epoch: 6, Train_Loss: 1.5879216194152832, Test_Loss: 1.5815293788909912 *\n",
      "219:Epoch: 6, Train_Loss: 1.6166062355041504, Test_Loss: 1.5867959260940552\n",
      "220:Epoch: 6, Train_Loss: 2.868455171585083, Test_Loss: 1.5913258790969849\n",
      "221:Epoch: 6, Train_Loss: 2.2435660362243652, Test_Loss: 1.6032222509384155\n",
      "222:Epoch: 6, Train_Loss: 1.5779998302459717, Test_Loss: 1.609993577003479\n",
      "223:Epoch: 6, Train_Loss: 1.5948697328567505, Test_Loss: 1.5679434537887573 *\n",
      "224:Epoch: 6, Train_Loss: 1.5679056644439697, Test_Loss: 1.5745655298233032\n",
      "225:Epoch: 6, Train_Loss: 1.7184076309204102, Test_Loss: 1.5857311487197876\n",
      "226:Epoch: 6, Train_Loss: 1.8575993776321411, Test_Loss: 1.567978858947754 *\n",
      "227:Epoch: 6, Train_Loss: 1.5756882429122925, Test_Loss: 1.5733977556228638\n",
      "228:Epoch: 6, Train_Loss: 1.9160722494125366, Test_Loss: 1.5693434476852417 *\n",
      "229:Epoch: 6, Train_Loss: 1.5915000438690186, Test_Loss: 1.5753381252288818\n",
      "230:Epoch: 6, Train_Loss: 1.5757113695144653, Test_Loss: 1.5704282522201538 *\n",
      "231:Epoch: 6, Train_Loss: 1.611407995223999, Test_Loss: 1.5827839374542236\n",
      "232:Epoch: 6, Train_Loss: 1.7636749744415283, Test_Loss: 1.6032527685165405\n",
      "233:Epoch: 6, Train_Loss: 1.653894066810608, Test_Loss: 1.603577733039856\n",
      "234:Epoch: 6, Train_Loss: 1.6138818264007568, Test_Loss: 1.589694857597351 *\n",
      "235:Epoch: 6, Train_Loss: 1.610485553741455, Test_Loss: 1.5782299041748047 *\n",
      "236:Epoch: 6, Train_Loss: 1.694016695022583, Test_Loss: 1.564064383506775 *\n",
      "237:Epoch: 6, Train_Loss: 1.593894600868225, Test_Loss: 1.5719672441482544\n",
      "238:Epoch: 6, Train_Loss: 1.5958671569824219, Test_Loss: 1.5741891860961914\n",
      "239:Epoch: 6, Train_Loss: 1.5710369348526, Test_Loss: 1.5635695457458496 *\n",
      "240:Epoch: 6, Train_Loss: 1.5857945680618286, Test_Loss: 1.6375117301940918\n",
      "241:Epoch: 6, Train_Loss: 1.6050491333007812, Test_Loss: 1.6089215278625488 *\n",
      "242:Epoch: 6, Train_Loss: 1.9111891984939575, Test_Loss: 7.262794494628906\n",
      "243:Epoch: 6, Train_Loss: 1.724280834197998, Test_Loss: 1.656905174255371 *\n",
      "244:Epoch: 6, Train_Loss: 2.023635149002075, Test_Loss: 1.5823421478271484 *\n",
      "245:Epoch: 6, Train_Loss: 1.9108150005340576, Test_Loss: 1.5966756343841553\n",
      "246:Epoch: 6, Train_Loss: 1.7455273866653442, Test_Loss: 1.5689337253570557 *\n",
      "247:Epoch: 6, Train_Loss: 1.6933060884475708, Test_Loss: 1.5695921182632446\n",
      "248:Epoch: 6, Train_Loss: 1.6222478151321411, Test_Loss: 1.5749472379684448\n",
      "249:Epoch: 6, Train_Loss: 1.573630928993225, Test_Loss: 1.5945017337799072\n",
      "250:Epoch: 6, Train_Loss: 1.5662226676940918, Test_Loss: 1.5920754671096802 *\n",
      "251:Epoch: 6, Train_Loss: 1.615247130393982, Test_Loss: 1.5932152271270752\n",
      "252:Epoch: 6, Train_Loss: 1.9026069641113281, Test_Loss: 1.5870460271835327 *\n",
      "253:Epoch: 6, Train_Loss: 1.9669115543365479, Test_Loss: 1.7107110023498535\n",
      "254:Epoch: 6, Train_Loss: 2.65647029876709, Test_Loss: 1.6371634006500244 *\n",
      "255:Epoch: 6, Train_Loss: 2.436821699142456, Test_Loss: 1.6356736421585083 *\n",
      "256:Epoch: 6, Train_Loss: 2.003777265548706, Test_Loss: 1.548851490020752 *\n",
      "257:Epoch: 6, Train_Loss: 1.835853934288025, Test_Loss: 1.658573865890503\n",
      "258:Epoch: 6, Train_Loss: 1.588417887687683, Test_Loss: 1.5708123445510864 *\n",
      "259:Epoch: 6, Train_Loss: 1.624510645866394, Test_Loss: 1.642150640487671\n",
      "260:Epoch: 6, Train_Loss: 2.0592215061187744, Test_Loss: 1.753577470779419\n",
      "261:Epoch: 6, Train_Loss: 2.338770866394043, Test_Loss: 1.5789620876312256 *\n",
      "262:Epoch: 6, Train_Loss: 1.6178785562515259, Test_Loss: 1.6080471277236938\n",
      "263:Epoch: 6, Train_Loss: 1.661890983581543, Test_Loss: 1.6183769702911377\n",
      "264:Epoch: 6, Train_Loss: 1.6378854513168335, Test_Loss: 1.6150829792022705 *\n",
      "265:Epoch: 6, Train_Loss: 1.6349971294403076, Test_Loss: 1.6092623472213745 *\n",
      "266:Epoch: 6, Train_Loss: 1.782057762145996, Test_Loss: 1.607943058013916 *\n",
      "267:Epoch: 6, Train_Loss: 2.059566020965576, Test_Loss: 1.639008641242981\n",
      "268:Epoch: 6, Train_Loss: 1.8200502395629883, Test_Loss: 1.5686242580413818 *\n",
      "269:Epoch: 6, Train_Loss: 1.8159455060958862, Test_Loss: 1.5880656242370605\n",
      "270:Epoch: 6, Train_Loss: 1.5855807065963745, Test_Loss: 1.63673996925354\n",
      "271:Epoch: 6, Train_Loss: 1.5804314613342285, Test_Loss: 1.6433823108673096\n",
      "272:Epoch: 6, Train_Loss: 1.5752216577529907, Test_Loss: 1.5542351007461548 *\n",
      "273:Epoch: 6, Train_Loss: 1.6080825328826904, Test_Loss: 1.6241191625595093\n",
      "274:Epoch: 6, Train_Loss: 1.5567784309387207, Test_Loss: 1.6518362760543823\n",
      "275:Epoch: 6, Train_Loss: 1.5580110549926758, Test_Loss: 1.7391642332077026\n",
      "276:Epoch: 6, Train_Loss: 17.976802825927734, Test_Loss: 1.7144840955734253 *\n",
      "277:Epoch: 6, Train_Loss: 1.5417667627334595, Test_Loss: 1.573858618736267 *\n",
      "278:Epoch: 6, Train_Loss: 3.481701612472534, Test_Loss: 1.5470852851867676 *\n",
      "279:Epoch: 6, Train_Loss: 3.14680814743042, Test_Loss: 1.5519492626190186\n",
      "280:Epoch: 6, Train_Loss: 1.5550978183746338, Test_Loss: 1.5763897895812988\n",
      "281:Epoch: 6, Train_Loss: 1.5952976942062378, Test_Loss: 1.8329081535339355\n",
      "282:Epoch: 6, Train_Loss: 4.6959228515625, Test_Loss: 1.9226293563842773\n",
      "283:Epoch: 6, Train_Loss: 8.264766693115234, Test_Loss: 1.6652520895004272 *\n",
      "284:Epoch: 6, Train_Loss: 1.6363389492034912, Test_Loss: 1.5850735902786255 *\n",
      "285:Epoch: 6, Train_Loss: 1.6155906915664673, Test_Loss: 1.626947283744812\n",
      "286:Epoch: 6, Train_Loss: 6.373337268829346, Test_Loss: 1.9608526229858398\n",
      "287:Epoch: 6, Train_Loss: 1.7364833354949951, Test_Loss: 1.9971380233764648\n",
      "288:Epoch: 6, Train_Loss: 1.672001600265503, Test_Loss: 1.9885015487670898 *\n",
      "289:Epoch: 6, Train_Loss: 1.5996837615966797, Test_Loss: 1.8993794918060303 *\n",
      "290:Epoch: 6, Train_Loss: 1.538212537765503, Test_Loss: 1.8964380025863647 *\n",
      "291:Epoch: 6, Train_Loss: 1.594868540763855, Test_Loss: 1.816663384437561 *\n",
      "292:Epoch: 6, Train_Loss: 1.5468820333480835, Test_Loss: 1.721427083015442 *\n",
      "293:Epoch: 6, Train_Loss: 1.5384559631347656, Test_Loss: 2.1338586807250977\n",
      "294:Epoch: 6, Train_Loss: 1.5316640138626099, Test_Loss: 1.639390230178833 *\n",
      "295:Epoch: 6, Train_Loss: 1.527705192565918, Test_Loss: 1.661461353302002\n",
      "296:Epoch: 6, Train_Loss: 1.5422310829162598, Test_Loss: 1.6291215419769287 *\n",
      "297:Epoch: 6, Train_Loss: 1.5543023347854614, Test_Loss: 1.5947152376174927 *\n",
      "298:Epoch: 6, Train_Loss: 1.5348830223083496, Test_Loss: 1.5604050159454346 *\n",
      "299:Epoch: 6, Train_Loss: 1.5367320775985718, Test_Loss: 1.5335685014724731 *\n",
      "300:Epoch: 6, Train_Loss: 1.537921667098999, Test_Loss: 1.5593770742416382\n",
      "Model saved at location ../Saver/model.ckpt at epoch 6\n",
      "301:Epoch: 6, Train_Loss: 1.5236879587173462, Test_Loss: 6.690334796905518\n",
      "302:Epoch: 6, Train_Loss: 1.524962067604065, Test_Loss: 2.247126340866089 *\n",
      "303:Epoch: 6, Train_Loss: 1.523606300354004, Test_Loss: 1.576186180114746 *\n",
      "304:Epoch: 6, Train_Loss: 1.5243847370147705, Test_Loss: 1.53982675075531 *\n",
      "305:Epoch: 6, Train_Loss: 1.5238043069839478, Test_Loss: 1.531483769416809 *\n",
      "306:Epoch: 6, Train_Loss: 1.520049810409546, Test_Loss: 1.528124451637268 *\n",
      "307:Epoch: 6, Train_Loss: 1.519477367401123, Test_Loss: 1.541279673576355\n",
      "308:Epoch: 6, Train_Loss: 1.5175235271453857, Test_Loss: 1.5633872747421265\n",
      "309:Epoch: 6, Train_Loss: 1.5181360244750977, Test_Loss: 1.5213268995285034 *\n",
      "310:Epoch: 6, Train_Loss: 1.5169951915740967, Test_Loss: 1.5233725309371948\n",
      "311:Epoch: 6, Train_Loss: 1.5164059400558472, Test_Loss: 1.5264092683792114\n",
      "312:Epoch: 6, Train_Loss: 1.5198861360549927, Test_Loss: 1.535274624824524\n",
      "313:Epoch: 6, Train_Loss: 1.5393157005310059, Test_Loss: 1.5227844715118408 *\n",
      "314:Epoch: 6, Train_Loss: 1.552939534187317, Test_Loss: 1.5260101556777954\n",
      "315:Epoch: 6, Train_Loss: 1.523664951324463, Test_Loss: 1.5321723222732544\n",
      "316:Epoch: 6, Train_Loss: 1.5233290195465088, Test_Loss: 1.5241050720214844 *\n",
      "317:Epoch: 6, Train_Loss: 2.8299503326416016, Test_Loss: 1.5149461030960083 *\n",
      "318:Epoch: 6, Train_Loss: 9.470787048339844, Test_Loss: 1.5199021100997925\n",
      "319:Epoch: 6, Train_Loss: 1.5564838647842407, Test_Loss: 1.5185596942901611 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320:Epoch: 6, Train_Loss: 1.5271930694580078, Test_Loss: 1.5225111246109009\n",
      "321:Epoch: 6, Train_Loss: 1.554455280303955, Test_Loss: 1.544657588005066\n",
      "322:Epoch: 6, Train_Loss: 1.5343817472457886, Test_Loss: 1.5497164726257324\n",
      "323:Epoch: 6, Train_Loss: 1.5165973901748657, Test_Loss: 1.5633630752563477\n",
      "324:Epoch: 6, Train_Loss: 1.5403858423233032, Test_Loss: 1.554445505142212 *\n",
      "325:Epoch: 6, Train_Loss: 1.5967795848846436, Test_Loss: 1.5493879318237305 *\n",
      "326:Epoch: 6, Train_Loss: 1.7561070919036865, Test_Loss: 1.5315723419189453 *\n",
      "327:Epoch: 6, Train_Loss: 1.7335734367370605, Test_Loss: 1.5333034992218018\n",
      "328:Epoch: 6, Train_Loss: 1.642760992050171, Test_Loss: 1.5325146913528442 *\n",
      "329:Epoch: 6, Train_Loss: 1.5507797002792358, Test_Loss: 1.5323609113693237 *\n",
      "330:Epoch: 6, Train_Loss: 1.6505482196807861, Test_Loss: 1.5338181257247925\n",
      "331:Epoch: 6, Train_Loss: 1.6270920038223267, Test_Loss: 1.5816913843154907\n",
      "332:Epoch: 6, Train_Loss: 1.6737560033798218, Test_Loss: 2.038038969039917\n",
      "333:Epoch: 6, Train_Loss: 1.6511719226837158, Test_Loss: 6.906974792480469\n",
      "334:Epoch: 6, Train_Loss: 1.6253364086151123, Test_Loss: 1.531530737876892 *\n",
      "335:Epoch: 6, Train_Loss: 1.5098614692687988, Test_Loss: 1.509324073791504 *\n",
      "336:Epoch: 6, Train_Loss: 1.5622988939285278, Test_Loss: 1.5321911573410034\n",
      "337:Epoch: 6, Train_Loss: 1.5662978887557983, Test_Loss: 1.5520365238189697\n",
      "338:Epoch: 6, Train_Loss: 1.5236204862594604, Test_Loss: 1.5549694299697876\n",
      "339:Epoch: 6, Train_Loss: 1.5059003829956055, Test_Loss: 1.509944200515747 *\n",
      "340:Epoch: 6, Train_Loss: 1.504948616027832, Test_Loss: 1.641953945159912\n",
      "341:Epoch: 6, Train_Loss: 1.5042976140975952, Test_Loss: 1.5418580770492554 *\n",
      "342:Epoch: 6, Train_Loss: 3.2769925594329834, Test_Loss: 1.502770185470581 *\n",
      "343:Epoch: 6, Train_Loss: 5.526050090789795, Test_Loss: 1.5584876537322998\n",
      "344:Epoch: 6, Train_Loss: 1.5070812702178955, Test_Loss: 1.5078225135803223 *\n",
      "345:Epoch: 6, Train_Loss: 1.5672240257263184, Test_Loss: 1.519303560256958\n",
      "346:Epoch: 6, Train_Loss: 1.529860496520996, Test_Loss: 1.5400656461715698\n",
      "347:Epoch: 6, Train_Loss: 1.5134085416793823, Test_Loss: 1.5788235664367676\n",
      "348:Epoch: 6, Train_Loss: 1.5061146020889282, Test_Loss: 1.5627928972244263 *\n",
      "349:Epoch: 6, Train_Loss: 1.502982497215271, Test_Loss: 1.6531909704208374\n",
      "350:Epoch: 6, Train_Loss: 1.5103628635406494, Test_Loss: 1.5506219863891602 *\n",
      "351:Epoch: 6, Train_Loss: 1.530377745628357, Test_Loss: 1.5201317071914673 *\n",
      "352:Epoch: 6, Train_Loss: 1.5156118869781494, Test_Loss: 1.508911371231079 *\n",
      "353:Epoch: 6, Train_Loss: 1.5021567344665527, Test_Loss: 1.503727674484253 *\n",
      "354:Epoch: 6, Train_Loss: 1.4996981620788574, Test_Loss: 1.5043108463287354\n",
      "355:Epoch: 6, Train_Loss: 1.4989650249481201, Test_Loss: 1.5035580396652222 *\n",
      "356:Epoch: 6, Train_Loss: 1.514170527458191, Test_Loss: 1.503030776977539 *\n",
      "357:Epoch: 6, Train_Loss: 1.497124195098877, Test_Loss: 1.5027894973754883 *\n",
      "358:Epoch: 6, Train_Loss: 1.4960664510726929, Test_Loss: 1.5086851119995117\n",
      "359:Epoch: 6, Train_Loss: 1.5485347509384155, Test_Loss: 1.5070606470108032 *\n",
      "360:Epoch: 6, Train_Loss: 1.5552983283996582, Test_Loss: 1.5106257200241089\n",
      "361:Epoch: 6, Train_Loss: 1.4978233575820923, Test_Loss: 1.496777892112732 *\n",
      "362:Epoch: 6, Train_Loss: 1.4942108392715454, Test_Loss: 1.5047755241394043\n",
      "363:Epoch: 6, Train_Loss: 1.5095343589782715, Test_Loss: 1.5542526245117188\n",
      "364:Epoch: 6, Train_Loss: 1.6103184223175049, Test_Loss: 1.5036207437515259 *\n",
      "365:Epoch: 6, Train_Loss: 1.5773957967758179, Test_Loss: 1.8478281497955322\n",
      "366:Epoch: 6, Train_Loss: 1.5988503694534302, Test_Loss: 2.038329601287842\n",
      "367:Epoch: 6, Train_Loss: 1.5326858758926392, Test_Loss: 1.677833080291748 *\n",
      "368:Epoch: 6, Train_Loss: 1.5540591478347778, Test_Loss: 1.5248756408691406 *\n",
      "369:Epoch: 6, Train_Loss: 1.5522874593734741, Test_Loss: 1.5127145051956177 *\n",
      "370:Epoch: 6, Train_Loss: 1.5501399040222168, Test_Loss: 1.513297200202942\n",
      "371:Epoch: 6, Train_Loss: 1.4982776641845703, Test_Loss: 1.648402452468872\n",
      "372:Epoch: 6, Train_Loss: 1.5930986404418945, Test_Loss: 2.6222383975982666\n",
      "373:Epoch: 6, Train_Loss: 1.5042918920516968, Test_Loss: 2.8336234092712402\n",
      "374:Epoch: 6, Train_Loss: 1.4882359504699707, Test_Loss: 1.5557678937911987 *\n",
      "375:Epoch: 6, Train_Loss: 1.4898295402526855, Test_Loss: 1.5690280199050903\n",
      "376:Epoch: 6, Train_Loss: 1.4890490770339966, Test_Loss: 1.4850012063980103 *\n",
      "377:Epoch: 6, Train_Loss: 1.488403558731079, Test_Loss: 1.4938410520553589\n",
      "378:Epoch: 6, Train_Loss: 1.4878952503204346, Test_Loss: 1.4909603595733643 *\n",
      "379:Epoch: 6, Train_Loss: 3.2927207946777344, Test_Loss: 1.5074944496154785\n",
      "380:Epoch: 6, Train_Loss: 4.638651371002197, Test_Loss: 1.539106845855713\n",
      "381:Epoch: 6, Train_Loss: 1.4830243587493896, Test_Loss: 1.5139284133911133 *\n",
      "382:Epoch: 6, Train_Loss: 1.5045900344848633, Test_Loss: 1.49002206325531 *\n",
      "383:Epoch: 6, Train_Loss: 1.496567726135254, Test_Loss: 1.5935571193695068\n",
      "384:Epoch: 6, Train_Loss: 1.479864239692688, Test_Loss: 1.8913655281066895\n",
      "385:Epoch: 6, Train_Loss: 1.4801244735717773, Test_Loss: 1.6348484754562378 *\n",
      "386:Epoch: 6, Train_Loss: 1.4794692993164062, Test_Loss: 1.5515094995498657 *\n",
      "387:Epoch: 6, Train_Loss: 1.478672981262207, Test_Loss: 1.4854789972305298 *\n",
      "388:Epoch: 6, Train_Loss: 1.4774909019470215, Test_Loss: 1.485356330871582 *\n",
      "389:Epoch: 6, Train_Loss: 1.486185908317566, Test_Loss: 1.4856374263763428\n",
      "390:Epoch: 6, Train_Loss: 1.5833120346069336, Test_Loss: 1.4859123229980469\n",
      "391:Epoch: 6, Train_Loss: 1.5659083127975464, Test_Loss: 1.5653752088546753\n",
      "392:Epoch: 6, Train_Loss: 1.5872585773468018, Test_Loss: 6.7128586769104\n",
      "393:Epoch: 6, Train_Loss: 1.5203585624694824, Test_Loss: 1.6228740215301514 *\n",
      "394:Epoch: 6, Train_Loss: 1.4751783609390259, Test_Loss: 1.4855848550796509 *\n",
      "395:Epoch: 6, Train_Loss: 1.6667943000793457, Test_Loss: 1.479743242263794 *\n",
      "396:Epoch: 6, Train_Loss: 1.706591010093689, Test_Loss: 1.4839866161346436\n",
      "397:Epoch: 6, Train_Loss: 1.6874878406524658, Test_Loss: 1.48324453830719 *\n",
      "398:Epoch: 6, Train_Loss: 1.5809842348098755, Test_Loss: 1.4770610332489014 *\n",
      "399:Epoch: 6, Train_Loss: 1.472470998764038, Test_Loss: 1.476617693901062 *\n",
      "400:Epoch: 6, Train_Loss: 1.471753716468811, Test_Loss: 1.4742845296859741 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 6\n",
      "401:Epoch: 6, Train_Loss: 1.473788857460022, Test_Loss: 1.4742064476013184 *\n",
      "402:Epoch: 6, Train_Loss: 1.4794561862945557, Test_Loss: 1.4742977619171143\n",
      "403:Epoch: 6, Train_Loss: 1.4830576181411743, Test_Loss: 1.477433681488037\n",
      "404:Epoch: 6, Train_Loss: 1.4770797491073608, Test_Loss: 1.4840880632400513\n",
      "405:Epoch: 6, Train_Loss: 1.4698891639709473, Test_Loss: 1.487160325050354\n",
      "406:Epoch: 6, Train_Loss: 1.4684672355651855, Test_Loss: 1.483412504196167 *\n",
      "407:Epoch: 6, Train_Loss: 1.4759210348129272, Test_Loss: 1.4704910516738892 *\n",
      "408:Epoch: 6, Train_Loss: 1.521460771560669, Test_Loss: 1.4682517051696777 *\n",
      "409:Epoch: 6, Train_Loss: 1.6617615222930908, Test_Loss: 1.468168020248413 *\n",
      "410:Epoch: 6, Train_Loss: 1.6434969902038574, Test_Loss: 1.46780526638031 *\n",
      "411:Epoch: 6, Train_Loss: 1.6002402305603027, Test_Loss: 1.4666107892990112 *\n",
      "412:Epoch: 6, Train_Loss: 1.565208077430725, Test_Loss: 1.470588207244873\n",
      "413:Epoch: 6, Train_Loss: 1.6244450807571411, Test_Loss: 1.465980052947998 *\n",
      "414:Epoch: 6, Train_Loss: 1.5183855295181274, Test_Loss: 1.468282699584961\n",
      "415:Epoch: 6, Train_Loss: 1.6033434867858887, Test_Loss: 1.4671555757522583 *\n",
      "416:Epoch: 6, Train_Loss: 1.5850731134414673, Test_Loss: 1.4656707048416138 *\n",
      "417:Epoch: 6, Train_Loss: 1.7543143033981323, Test_Loss: 1.4644684791564941 *\n",
      "418:Epoch: 6, Train_Loss: 1.4727765321731567, Test_Loss: 1.4649900197982788\n",
      "419:Epoch: 6, Train_Loss: 1.5252604484558105, Test_Loss: 1.463487148284912 *\n",
      "420:Epoch: 6, Train_Loss: 4.456609725952148, Test_Loss: 1.463193655014038 *\n",
      "421:Epoch: 6, Train_Loss: 1.6576050519943237, Test_Loss: 1.4669995307922363\n",
      "422:Epoch: 6, Train_Loss: 1.5125230550765991, Test_Loss: 1.5211595296859741\n",
      "423:Epoch: 6, Train_Loss: 1.5264391899108887, Test_Loss: 3.1263129711151123\n",
      "424:Epoch: 6, Train_Loss: 1.5242866277694702, Test_Loss: 5.279143333435059\n",
      "425:Epoch: 6, Train_Loss: 1.4699829816818237, Test_Loss: 1.4658067226409912 *\n",
      "426:Epoch: 6, Train_Loss: 1.461409330368042, Test_Loss: 1.459316611289978 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427:Epoch: 6, Train_Loss: 1.5343681573867798, Test_Loss: 1.511780858039856\n",
      "428:Epoch: 6, Train_Loss: 1.5808838605880737, Test_Loss: 1.5247005224227905\n",
      "429:Epoch: 6, Train_Loss: 1.5688714981079102, Test_Loss: 1.5242547988891602 *\n",
      "430:Epoch: 6, Train_Loss: 1.5392566919326782, Test_Loss: 1.4830946922302246 *\n",
      "431:Epoch: 6, Train_Loss: 1.5546170473098755, Test_Loss: 1.571758508682251\n",
      "432:Epoch: 6, Train_Loss: 1.4875457286834717, Test_Loss: 1.4658979177474976 *\n",
      "433:Epoch: 6, Train_Loss: 1.488853931427002, Test_Loss: 1.4676018953323364\n",
      "434:Epoch: 6, Train_Loss: 1.4665677547454834, Test_Loss: 1.4861074686050415\n",
      "435:Epoch: 6, Train_Loss: 1.4955518245697021, Test_Loss: 1.4713419675827026 *\n",
      "436:Epoch: 6, Train_Loss: 1.4761327505111694, Test_Loss: 1.4630100727081299 *\n",
      "437:Epoch: 6, Train_Loss: 1.4533424377441406, Test_Loss: 1.5404340028762817\n",
      "438:Epoch: 6, Train_Loss: 1.471690058708191, Test_Loss: 1.5838301181793213\n",
      "439:Epoch: 6, Train_Loss: 1.4970166683197021, Test_Loss: 1.5128700733184814 *\n",
      "440:Epoch: 6, Train_Loss: 1.48684823513031, Test_Loss: 1.5552005767822266\n",
      "441:Epoch: 6, Train_Loss: 1.4528254270553589, Test_Loss: 1.4739983081817627 *\n",
      "442:Epoch: 6, Train_Loss: 1.4513132572174072, Test_Loss: 1.5021764039993286\n",
      "443:Epoch: 6, Train_Loss: 1.4505003690719604, Test_Loss: 1.486028790473938 *\n",
      "444:Epoch: 6, Train_Loss: 1.4501086473464966, Test_Loss: 1.4810285568237305 *\n",
      "445:Epoch: 6, Train_Loss: 1.45040762424469, Test_Loss: 1.4825944900512695\n",
      "446:Epoch: 6, Train_Loss: 1.4510785341262817, Test_Loss: 1.4822471141815186 *\n",
      "447:Epoch: 6, Train_Loss: 1.4510246515274048, Test_Loss: 1.4811455011367798 *\n",
      "448:Epoch: 6, Train_Loss: 1.4495651721954346, Test_Loss: 1.4803433418273926 *\n",
      "449:Epoch: 6, Train_Loss: 1.4496567249298096, Test_Loss: 1.495461344718933\n",
      "450:Epoch: 6, Train_Loss: 1.4482576847076416, Test_Loss: 1.483270525932312 *\n",
      "451:Epoch: 6, Train_Loss: 1.4498710632324219, Test_Loss: 1.4835255146026611\n",
      "452:Epoch: 6, Train_Loss: 1.459490418434143, Test_Loss: 1.448910117149353 *\n",
      "453:Epoch: 6, Train_Loss: 1.4619927406311035, Test_Loss: 1.4865529537200928\n",
      "454:Epoch: 6, Train_Loss: 1.4661951065063477, Test_Loss: 1.5494147539138794\n",
      "1:Epoch: 7, Train_Loss: 1.454569697380066, Test_Loss: 1.4566816091537476 *\n",
      "2:Epoch: 7, Train_Loss: 1.450825572013855, Test_Loss: 1.9725754261016846\n",
      "3:Epoch: 7, Train_Loss: 1.4433537721633911, Test_Loss: 2.0369765758514404\n",
      "4:Epoch: 7, Train_Loss: 1.4428306818008423, Test_Loss: 1.6315058469772339 *\n",
      "5:Epoch: 7, Train_Loss: 1.4563716650009155, Test_Loss: 1.4800845384597778 *\n",
      "6:Epoch: 7, Train_Loss: 1.4581081867218018, Test_Loss: 1.4693353176116943 *\n",
      "7:Epoch: 7, Train_Loss: 1.441558599472046, Test_Loss: 1.4656533002853394 *\n",
      "8:Epoch: 7, Train_Loss: 1.4411554336547852, Test_Loss: 1.648743748664856\n",
      "9:Epoch: 7, Train_Loss: 1.4417285919189453, Test_Loss: 2.6984076499938965\n",
      "10:Epoch: 7, Train_Loss: 1.5190625190734863, Test_Loss: 2.399306058883667 *\n",
      "11:Epoch: 7, Train_Loss: 1.4761608839035034, Test_Loss: 1.4916080236434937 *\n",
      "12:Epoch: 7, Train_Loss: 1.4893635511398315, Test_Loss: 1.510506272315979\n",
      "13:Epoch: 7, Train_Loss: 1.4497652053833008, Test_Loss: 1.4409245252609253 *\n",
      "14:Epoch: 7, Train_Loss: 1.4429078102111816, Test_Loss: 1.4422962665557861\n",
      "15:Epoch: 7, Train_Loss: 1.4958008527755737, Test_Loss: 1.446028232574463\n",
      "16:Epoch: 7, Train_Loss: 1.438781976699829, Test_Loss: 1.4630612134933472\n",
      "17:Epoch: 7, Train_Loss: 1.4568750858306885, Test_Loss: 1.5010401010513306\n",
      "18:Epoch: 7, Train_Loss: 1.4762226343154907, Test_Loss: 1.4428318738937378 *\n",
      "19:Epoch: 7, Train_Loss: 1.4691630601882935, Test_Loss: 1.4567371606826782\n",
      "20:Epoch: 7, Train_Loss: 1.5789254903793335, Test_Loss: 1.559167504310608\n",
      "21:Epoch: 7, Train_Loss: 1.5234489440917969, Test_Loss: 1.8236854076385498\n",
      "22:Epoch: 7, Train_Loss: 1.4746818542480469, Test_Loss: 1.649991512298584 *\n",
      "23:Epoch: 7, Train_Loss: 1.4425021409988403, Test_Loss: 1.444846272468567 *\n",
      "24:Epoch: 7, Train_Loss: 1.4610458612442017, Test_Loss: 1.4397374391555786 *\n",
      "25:Epoch: 7, Train_Loss: 1.4350522756576538, Test_Loss: 1.439344882965088 *\n",
      "26:Epoch: 7, Train_Loss: 1.4390736818313599, Test_Loss: 1.4391790628433228 *\n",
      "27:Epoch: 7, Train_Loss: 1.4440547227859497, Test_Loss: 1.4402217864990234\n",
      "28:Epoch: 7, Train_Loss: 1.449955940246582, Test_Loss: 1.9608142375946045\n",
      "29:Epoch: 7, Train_Loss: 1.4889734983444214, Test_Loss: 6.405974388122559\n",
      "30:Epoch: 7, Train_Loss: 1.4846677780151367, Test_Loss: 1.4745303392410278 *\n",
      "31:Epoch: 7, Train_Loss: 1.4811943769454956, Test_Loss: 1.4370250701904297 *\n",
      "32:Epoch: 7, Train_Loss: 1.4728864431381226, Test_Loss: 1.4328283071517944 *\n",
      "33:Epoch: 7, Train_Loss: 1.4596925973892212, Test_Loss: 1.4366706609725952\n",
      "34:Epoch: 7, Train_Loss: 1.4399529695510864, Test_Loss: 1.433768391609192 *\n",
      "35:Epoch: 7, Train_Loss: 1.5699394941329956, Test_Loss: 1.4312522411346436 *\n",
      "36:Epoch: 7, Train_Loss: 1.6178836822509766, Test_Loss: 1.4328986406326294\n",
      "37:Epoch: 7, Train_Loss: 1.4283628463745117, Test_Loss: 1.430199384689331 *\n",
      "38:Epoch: 7, Train_Loss: 1.4592734575271606, Test_Loss: 1.4307407140731812\n",
      "39:Epoch: 7, Train_Loss: 1.4267747402191162, Test_Loss: 1.4284446239471436 *\n",
      "40:Epoch: 7, Train_Loss: 1.4259581565856934, Test_Loss: 1.4317924976348877\n",
      "41:Epoch: 7, Train_Loss: 1.4261246919631958, Test_Loss: 1.4469987154006958\n",
      "42:Epoch: 7, Train_Loss: 1.4267723560333252, Test_Loss: 1.4413402080535889 *\n",
      "43:Epoch: 7, Train_Loss: 1.4445974826812744, Test_Loss: 1.437391757965088 *\n",
      "44:Epoch: 7, Train_Loss: 1.4528453350067139, Test_Loss: 1.425236463546753 *\n",
      "45:Epoch: 7, Train_Loss: 1.4425151348114014, Test_Loss: 1.4240541458129883 *\n",
      "46:Epoch: 7, Train_Loss: 1.4473358392715454, Test_Loss: 1.4239001274108887 *\n",
      "47:Epoch: 7, Train_Loss: 1.4485957622528076, Test_Loss: 1.4227709770202637 *\n",
      "48:Epoch: 7, Train_Loss: 1.4223195314407349, Test_Loss: 1.4224926233291626 *\n",
      "49:Epoch: 7, Train_Loss: 1.4234551191329956, Test_Loss: 1.4238158464431763\n",
      "50:Epoch: 7, Train_Loss: 1.4208509922027588, Test_Loss: 1.42097806930542 *\n",
      "51:Epoch: 7, Train_Loss: 1.451338291168213, Test_Loss: 1.4223037958145142\n",
      "52:Epoch: 7, Train_Loss: 1.4531311988830566, Test_Loss: 1.4208130836486816 *\n",
      "53:Epoch: 7, Train_Loss: 1.4328304529190063, Test_Loss: 1.4202659130096436 *\n",
      "54:Epoch: 7, Train_Loss: 1.435514211654663, Test_Loss: 1.4198453426361084 *\n",
      "55:Epoch: 7, Train_Loss: 1.483237624168396, Test_Loss: 1.4197046756744385 *\n",
      "56:Epoch: 7, Train_Loss: 1.477751612663269, Test_Loss: 1.4186577796936035 *\n",
      "57:Epoch: 7, Train_Loss: 1.4343857765197754, Test_Loss: 1.4185400009155273 *\n",
      "58:Epoch: 7, Train_Loss: 1.4347680807113647, Test_Loss: 1.4278817176818848\n",
      "59:Epoch: 7, Train_Loss: 1.4343137741088867, Test_Loss: 1.47411048412323\n",
      "60:Epoch: 7, Train_Loss: 1.428542971611023, Test_Loss: 4.171383857727051\n",
      "61:Epoch: 7, Train_Loss: 1.4255567789077759, Test_Loss: 4.182438373565674\n",
      "62:Epoch: 7, Train_Loss: 1.4435646533966064, Test_Loss: 1.4175978899002075 *\n",
      "63:Epoch: 7, Train_Loss: 1.4743273258209229, Test_Loss: 1.4151866436004639 *\n",
      "64:Epoch: 7, Train_Loss: 3.686730146408081, Test_Loss: 1.471274733543396\n",
      "65:Epoch: 7, Train_Loss: 4.587767601013184, Test_Loss: 1.4727106094360352\n",
      "66:Epoch: 7, Train_Loss: 1.426405429840088, Test_Loss: 1.4585601091384888 *\n",
      "67:Epoch: 7, Train_Loss: 1.418642282485962, Test_Loss: 1.468994140625\n",
      "68:Epoch: 7, Train_Loss: 1.4706491231918335, Test_Loss: 1.5290052890777588\n",
      "69:Epoch: 7, Train_Loss: 1.593799114227295, Test_Loss: 1.413956642150879 *\n",
      "70:Epoch: 7, Train_Loss: 1.438942551612854, Test_Loss: 1.4381420612335205\n",
      "71:Epoch: 7, Train_Loss: 1.4176844358444214, Test_Loss: 1.4318029880523682 *\n",
      "72:Epoch: 7, Train_Loss: 1.4219465255737305, Test_Loss: 1.4264192581176758 *\n",
      "73:Epoch: 7, Train_Loss: 1.4834390878677368, Test_Loss: 1.4175679683685303 *\n",
      "74:Epoch: 7, Train_Loss: 1.417642593383789, Test_Loss: 1.4997954368591309\n",
      "75:Epoch: 7, Train_Loss: 1.4223053455352783, Test_Loss: 1.4868115186691284 *\n",
      "76:Epoch: 7, Train_Loss: 2.3374850749969482, Test_Loss: 1.5028533935546875\n",
      "77:Epoch: 7, Train_Loss: 2.8374216556549072, Test_Loss: 1.5080187320709229\n",
      "78:Epoch: 7, Train_Loss: 1.9842270612716675, Test_Loss: 1.4283270835876465 *\n",
      "79:Epoch: 7, Train_Loss: 1.5318351984024048, Test_Loss: 1.434648036956787\n",
      "80:Epoch: 7, Train_Loss: 2.354376792907715, Test_Loss: 1.4244760274887085 *\n",
      "81:Epoch: 7, Train_Loss: 3.585873603820801, Test_Loss: 1.4175173044204712 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82:Epoch: 7, Train_Loss: 1.663158655166626, Test_Loss: 1.4153064489364624 *\n",
      "83:Epoch: 7, Train_Loss: 1.4378858804702759, Test_Loss: 1.416571021080017\n",
      "84:Epoch: 7, Train_Loss: 1.4252840280532837, Test_Loss: 1.4146894216537476 *\n",
      "85:Epoch: 7, Train_Loss: 2.749236583709717, Test_Loss: 1.410068154335022 *\n",
      "86:Epoch: 7, Train_Loss: 2.8891615867614746, Test_Loss: 1.4383271932601929\n",
      "87:Epoch: 7, Train_Loss: 1.528983473777771, Test_Loss: 1.440584659576416\n",
      "88:Epoch: 7, Train_Loss: 1.4228132963180542, Test_Loss: 1.4721124172210693\n",
      "89:Epoch: 7, Train_Loss: 1.4197287559509277, Test_Loss: 1.547181487083435\n",
      "90:Epoch: 7, Train_Loss: 1.9753108024597168, Test_Loss: 1.4657868146896362 *\n",
      "91:Epoch: 7, Train_Loss: 1.6086691617965698, Test_Loss: 1.462106466293335 *\n",
      "92:Epoch: 7, Train_Loss: 1.4898930788040161, Test_Loss: 1.6183089017868042\n",
      "93:Epoch: 7, Train_Loss: 1.4616315364837646, Test_Loss: 1.4671611785888672 *\n",
      "94:Epoch: 7, Train_Loss: 1.5508406162261963, Test_Loss: 1.4716832637786865\n",
      "95:Epoch: 7, Train_Loss: 1.4370185136795044, Test_Loss: 1.4259672164916992 *\n",
      "96:Epoch: 7, Train_Loss: 1.6182224750518799, Test_Loss: 1.4492685794830322\n",
      "97:Epoch: 7, Train_Loss: 1.7068986892700195, Test_Loss: 1.447972297668457 *\n",
      "98:Epoch: 7, Train_Loss: 1.4480129480361938, Test_Loss: 1.4296035766601562 *\n",
      "99:Epoch: 7, Train_Loss: 1.4743751287460327, Test_Loss: 1.6407241821289062\n",
      "100:Epoch: 7, Train_Loss: 1.6106598377227783, Test_Loss: 2.406773567199707\n",
      "Model saved at location ../Saver/model.ckpt at epoch 7\n",
      "101:Epoch: 7, Train_Loss: 1.6934031248092651, Test_Loss: 1.920047640800476 *\n",
      "102:Epoch: 7, Train_Loss: 1.6683317422866821, Test_Loss: 1.4797173738479614 *\n",
      "103:Epoch: 7, Train_Loss: 1.4535654783248901, Test_Loss: 1.4235680103302002 *\n",
      "104:Epoch: 7, Train_Loss: 1.4775539636611938, Test_Loss: 1.4097570180892944 *\n",
      "105:Epoch: 7, Train_Loss: 1.513448715209961, Test_Loss: 1.405561089515686 *\n",
      "106:Epoch: 7, Train_Loss: 1.4271091222763062, Test_Loss: 1.426248550415039\n",
      "107:Epoch: 7, Train_Loss: 1.4009130001068115, Test_Loss: 1.4011143445968628 *\n",
      "108:Epoch: 7, Train_Loss: 1.3952964544296265, Test_Loss: 1.4535235166549683\n",
      "109:Epoch: 7, Train_Loss: 1.3941764831542969, Test_Loss: 1.4047352075576782 *\n",
      "110:Epoch: 7, Train_Loss: 1.3921507596969604, Test_Loss: 1.4564762115478516\n",
      "111:Epoch: 7, Train_Loss: 1.3988926410675049, Test_Loss: 1.5219271183013916\n",
      "112:Epoch: 7, Train_Loss: 1.4682672023773193, Test_Loss: 1.7835462093353271\n",
      "113:Epoch: 7, Train_Loss: 1.4467259645462036, Test_Loss: 1.7151901721954346 *\n",
      "114:Epoch: 7, Train_Loss: 1.4838378429412842, Test_Loss: 1.4198493957519531 *\n",
      "115:Epoch: 7, Train_Loss: 1.5325713157653809, Test_Loss: 1.4037193059921265 *\n",
      "116:Epoch: 7, Train_Loss: 1.7495328187942505, Test_Loss: 1.402467966079712 *\n",
      "117:Epoch: 7, Train_Loss: 1.3963932991027832, Test_Loss: 1.4017561674118042 *\n",
      "118:Epoch: 7, Train_Loss: 1.4388030767440796, Test_Loss: 1.425424337387085\n",
      "119:Epoch: 7, Train_Loss: 1.533968448638916, Test_Loss: 2.7904906272888184\n",
      "120:Epoch: 7, Train_Loss: 1.8990426063537598, Test_Loss: 5.328490257263184\n",
      "121:Epoch: 7, Train_Loss: 1.6714956760406494, Test_Loss: 1.4047571420669556 *\n",
      "122:Epoch: 7, Train_Loss: 1.3937753438949585, Test_Loss: 1.3937077522277832 *\n",
      "123:Epoch: 7, Train_Loss: 1.5058633089065552, Test_Loss: 1.3900712728500366 *\n",
      "124:Epoch: 7, Train_Loss: 1.9613473415374756, Test_Loss: 1.395330548286438\n",
      "125:Epoch: 7, Train_Loss: 1.7738654613494873, Test_Loss: 1.4179294109344482\n",
      "126:Epoch: 7, Train_Loss: 1.4461520910263062, Test_Loss: 1.4455448389053345\n",
      "127:Epoch: 7, Train_Loss: 1.404075264930725, Test_Loss: 1.4385356903076172 *\n",
      "128:Epoch: 7, Train_Loss: 1.3985987901687622, Test_Loss: 1.4029600620269775 *\n",
      "129:Epoch: 7, Train_Loss: 2.3386952877044678, Test_Loss: 1.4442464113235474\n",
      "130:Epoch: 7, Train_Loss: 2.3337697982788086, Test_Loss: 1.450790286064148\n",
      "131:Epoch: 7, Train_Loss: 1.4260867834091187, Test_Loss: 1.4954646825790405\n",
      "132:Epoch: 7, Train_Loss: 1.4381214380264282, Test_Loss: 1.4034008979797363 *\n",
      "133:Epoch: 7, Train_Loss: 1.3846368789672852, Test_Loss: 1.426154613494873\n",
      "134:Epoch: 7, Train_Loss: 1.4285508394241333, Test_Loss: 1.4458454847335815\n",
      "135:Epoch: 7, Train_Loss: 1.7421042919158936, Test_Loss: 1.385345458984375 *\n",
      "136:Epoch: 7, Train_Loss: 1.3930158615112305, Test_Loss: 1.396531105041504\n",
      "137:Epoch: 7, Train_Loss: 1.628410816192627, Test_Loss: 1.3880308866500854 *\n",
      "138:Epoch: 7, Train_Loss: 1.4904491901397705, Test_Loss: 1.428767442703247\n",
      "139:Epoch: 7, Train_Loss: 1.4036986827850342, Test_Loss: 1.3930553197860718 *\n",
      "140:Epoch: 7, Train_Loss: 1.4037590026855469, Test_Loss: 1.423189401626587\n",
      "141:Epoch: 7, Train_Loss: 1.5362677574157715, Test_Loss: 1.4217222929000854 *\n",
      "142:Epoch: 7, Train_Loss: 1.5119590759277344, Test_Loss: 1.4762674570083618\n",
      "143:Epoch: 7, Train_Loss: 1.4009226560592651, Test_Loss: 1.4514323472976685 *\n",
      "144:Epoch: 7, Train_Loss: 1.4281576871871948, Test_Loss: 1.4239084720611572 *\n",
      "145:Epoch: 7, Train_Loss: 1.4705902338027954, Test_Loss: 1.3855912685394287 *\n",
      "146:Epoch: 7, Train_Loss: 1.4495750665664673, Test_Loss: 1.4028972387313843\n",
      "147:Epoch: 7, Train_Loss: 1.435906171798706, Test_Loss: 1.4066112041473389\n",
      "148:Epoch: 7, Train_Loss: 1.393908977508545, Test_Loss: 1.3871190547943115 *\n",
      "149:Epoch: 7, Train_Loss: 1.4036715030670166, Test_Loss: 1.4997401237487793\n",
      "150:Epoch: 7, Train_Loss: 1.3959232568740845, Test_Loss: 1.4431862831115723 *\n",
      "151:Epoch: 7, Train_Loss: 1.8143706321716309, Test_Loss: 5.134255409240723\n",
      "152:Epoch: 7, Train_Loss: 1.6268266439437866, Test_Loss: 2.932224750518799 *\n",
      "153:Epoch: 7, Train_Loss: 1.8135993480682373, Test_Loss: 1.3957903385162354 *\n",
      "154:Epoch: 7, Train_Loss: 1.7148442268371582, Test_Loss: 1.405946969985962\n",
      "155:Epoch: 7, Train_Loss: 1.5876905918121338, Test_Loss: 1.398705005645752 *\n",
      "156:Epoch: 7, Train_Loss: 1.6199404001235962, Test_Loss: 1.3819364309310913 *\n",
      "157:Epoch: 7, Train_Loss: 1.4389933347702026, Test_Loss: 1.3922408819198608\n",
      "158:Epoch: 7, Train_Loss: 1.3895680904388428, Test_Loss: 1.4371637105941772\n",
      "159:Epoch: 7, Train_Loss: 1.3960002660751343, Test_Loss: 1.4005119800567627 *\n",
      "160:Epoch: 7, Train_Loss: 1.4178012609481812, Test_Loss: 1.401529312133789\n",
      "161:Epoch: 7, Train_Loss: 1.6521692276000977, Test_Loss: 1.4130592346191406\n",
      "162:Epoch: 7, Train_Loss: 1.8019297122955322, Test_Loss: 1.4805278778076172\n",
      "163:Epoch: 7, Train_Loss: 2.0584843158721924, Test_Loss: 1.4834625720977783\n",
      "164:Epoch: 7, Train_Loss: 2.539374828338623, Test_Loss: 1.4335689544677734 *\n",
      "165:Epoch: 7, Train_Loss: 1.828035831451416, Test_Loss: 1.4035065174102783 *\n",
      "166:Epoch: 7, Train_Loss: 1.747912883758545, Test_Loss: 1.4959208965301514\n",
      "167:Epoch: 7, Train_Loss: 1.4270812273025513, Test_Loss: 1.4023480415344238 *\n",
      "168:Epoch: 7, Train_Loss: 1.4025546312332153, Test_Loss: 1.4236418008804321\n",
      "169:Epoch: 7, Train_Loss: 1.7310609817504883, Test_Loss: 1.5918134450912476\n",
      "170:Epoch: 7, Train_Loss: 2.310277223587036, Test_Loss: 1.4360402822494507 *\n",
      "171:Epoch: 7, Train_Loss: 1.4938472509384155, Test_Loss: 1.4798219203948975\n",
      "172:Epoch: 7, Train_Loss: 1.4793518781661987, Test_Loss: 1.5138150453567505\n",
      "173:Epoch: 7, Train_Loss: 1.4520519971847534, Test_Loss: 1.498177409172058 *\n",
      "174:Epoch: 7, Train_Loss: 1.3969464302062988, Test_Loss: 1.436774492263794 *\n",
      "175:Epoch: 7, Train_Loss: 1.668560266494751, Test_Loss: 1.4926804304122925\n",
      "176:Epoch: 7, Train_Loss: 1.729873776435852, Test_Loss: 1.5654346942901611\n",
      "177:Epoch: 7, Train_Loss: 1.8132766485214233, Test_Loss: 1.4885692596435547 *\n",
      "178:Epoch: 7, Train_Loss: 1.6697916984558105, Test_Loss: 1.52240788936615\n",
      "179:Epoch: 7, Train_Loss: 1.4174370765686035, Test_Loss: 1.4951242208480835 *\n",
      "180:Epoch: 7, Train_Loss: 1.413703441619873, Test_Loss: 1.5693753957748413\n",
      "181:Epoch: 7, Train_Loss: 1.39492928981781, Test_Loss: 1.4583102464675903 *\n",
      "182:Epoch: 7, Train_Loss: 1.4754323959350586, Test_Loss: 1.4483962059020996 *\n",
      "183:Epoch: 7, Train_Loss: 1.379778504371643, Test_Loss: 1.4771959781646729\n",
      "184:Epoch: 7, Train_Loss: 1.4186302423477173, Test_Loss: 1.4650840759277344 *\n",
      "185:Epoch: 7, Train_Loss: 16.73363494873047, Test_Loss: 1.5246940851211548\n",
      "186:Epoch: 7, Train_Loss: 1.8431811332702637, Test_Loss: 1.3996883630752563 *\n",
      "187:Epoch: 7, Train_Loss: 2.6951904296875, Test_Loss: 1.3894051313400269 *\n",
      "188:Epoch: 7, Train_Loss: 3.794497013092041, Test_Loss: 1.4064719676971436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189:Epoch: 7, Train_Loss: 1.3878711462020874, Test_Loss: 1.3781211376190186 *\n",
      "190:Epoch: 7, Train_Loss: 1.4463167190551758, Test_Loss: 1.5465912818908691\n",
      "191:Epoch: 7, Train_Loss: 2.7508304119110107, Test_Loss: 1.732810616493225\n",
      "192:Epoch: 7, Train_Loss: 9.609125137329102, Test_Loss: 1.6000852584838867 *\n",
      "193:Epoch: 7, Train_Loss: 1.6440008878707886, Test_Loss: 1.482086181640625 *\n",
      "194:Epoch: 7, Train_Loss: 1.4443613290786743, Test_Loss: 1.4828368425369263\n",
      "195:Epoch: 7, Train_Loss: 6.054306983947754, Test_Loss: 1.7597743272781372\n",
      "196:Epoch: 7, Train_Loss: 1.826974630355835, Test_Loss: 1.843431830406189\n",
      "197:Epoch: 7, Train_Loss: 1.47841477394104, Test_Loss: 1.9387481212615967\n",
      "198:Epoch: 7, Train_Loss: 1.4162541627883911, Test_Loss: 1.6170591115951538 *\n",
      "199:Epoch: 7, Train_Loss: 1.4252020120620728, Test_Loss: 1.979948878288269\n",
      "200:Epoch: 7, Train_Loss: 1.4456506967544556, Test_Loss: 1.4564470052719116 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 7\n",
      "201:Epoch: 7, Train_Loss: 1.3895503282546997, Test_Loss: 1.6544382572174072\n",
      "202:Epoch: 7, Train_Loss: 1.3882012367248535, Test_Loss: 2.15655517578125\n",
      "203:Epoch: 7, Train_Loss: 1.354404330253601, Test_Loss: 1.6077094078063965 *\n",
      "204:Epoch: 7, Train_Loss: 1.353680968284607, Test_Loss: 1.6390011310577393\n",
      "205:Epoch: 7, Train_Loss: 1.3561121225357056, Test_Loss: 1.6117264032363892 *\n",
      "206:Epoch: 7, Train_Loss: 1.4221432209014893, Test_Loss: 1.702972650527954\n",
      "207:Epoch: 7, Train_Loss: 1.415981650352478, Test_Loss: 1.6684904098510742 *\n",
      "208:Epoch: 7, Train_Loss: 1.3961306810379028, Test_Loss: 1.5947710275650024 *\n",
      "209:Epoch: 7, Train_Loss: 1.3625704050064087, Test_Loss: 1.3993964195251465 *\n",
      "210:Epoch: 7, Train_Loss: 1.3540570735931396, Test_Loss: 4.2622551918029785\n",
      "211:Epoch: 7, Train_Loss: 1.3526633977890015, Test_Loss: 4.431736946105957\n",
      "212:Epoch: 7, Train_Loss: 1.3579497337341309, Test_Loss: 1.4783557653427124 *\n",
      "213:Epoch: 7, Train_Loss: 1.3480371236801147, Test_Loss: 1.4721872806549072 *\n",
      "214:Epoch: 7, Train_Loss: 1.3461514711380005, Test_Loss: 1.4408912658691406 *\n",
      "215:Epoch: 7, Train_Loss: 1.343950629234314, Test_Loss: 1.3540327548980713 *\n",
      "216:Epoch: 7, Train_Loss: 1.3433091640472412, Test_Loss: 1.4370414018630981\n",
      "217:Epoch: 7, Train_Loss: 1.34415602684021, Test_Loss: 1.4164347648620605 *\n",
      "218:Epoch: 7, Train_Loss: 1.3430233001708984, Test_Loss: 1.3520010709762573 *\n",
      "219:Epoch: 7, Train_Loss: 1.3420897722244263, Test_Loss: 1.349618673324585 *\n",
      "220:Epoch: 7, Train_Loss: 1.341545581817627, Test_Loss: 1.3507521152496338\n",
      "221:Epoch: 7, Train_Loss: 1.342834234237671, Test_Loss: 1.3475581407546997 *\n",
      "222:Epoch: 7, Train_Loss: 1.3586727380752563, Test_Loss: 1.3547117710113525\n",
      "223:Epoch: 7, Train_Loss: 1.3840703964233398, Test_Loss: 1.3624346256256104\n",
      "224:Epoch: 7, Train_Loss: 1.3523763418197632, Test_Loss: 1.3522902727127075 *\n",
      "225:Epoch: 7, Train_Loss: 1.3490575551986694, Test_Loss: 1.346637487411499 *\n",
      "226:Epoch: 7, Train_Loss: 1.3911608457565308, Test_Loss: 1.340201735496521 *\n",
      "227:Epoch: 7, Train_Loss: 10.56987190246582, Test_Loss: 1.343762993812561\n",
      "228:Epoch: 7, Train_Loss: 1.5014976263046265, Test_Loss: 1.3427151441574097 *\n",
      "229:Epoch: 7, Train_Loss: 1.3465908765792847, Test_Loss: 1.3437031507492065\n",
      "230:Epoch: 7, Train_Loss: 1.3422642946243286, Test_Loss: 1.3468817472457886\n",
      "231:Epoch: 7, Train_Loss: 1.3532328605651855, Test_Loss: 1.341065526008606 *\n",
      "232:Epoch: 7, Train_Loss: 1.3715096712112427, Test_Loss: 1.3442326784133911\n",
      "233:Epoch: 7, Train_Loss: 1.362281322479248, Test_Loss: 1.3533674478530884\n",
      "234:Epoch: 7, Train_Loss: 1.3481988906860352, Test_Loss: 1.3460770845413208 *\n",
      "235:Epoch: 7, Train_Loss: 1.5105934143066406, Test_Loss: 1.348000168800354\n",
      "236:Epoch: 7, Train_Loss: 1.5306181907653809, Test_Loss: 1.3431941270828247 *\n",
      "237:Epoch: 7, Train_Loss: 1.482272982597351, Test_Loss: 1.344241976737976\n",
      "238:Epoch: 7, Train_Loss: 1.3455561399459839, Test_Loss: 1.3432285785675049 *\n",
      "239:Epoch: 7, Train_Loss: 1.443811058998108, Test_Loss: 1.342891812324524 *\n",
      "240:Epoch: 7, Train_Loss: 1.4249070882797241, Test_Loss: 1.390777826309204\n",
      "241:Epoch: 7, Train_Loss: 1.4682449102401733, Test_Loss: 1.3615407943725586 *\n",
      "242:Epoch: 7, Train_Loss: 1.4434008598327637, Test_Loss: 6.496068477630615\n",
      "243:Epoch: 7, Train_Loss: 1.436753749847412, Test_Loss: 1.8683733940124512 *\n",
      "244:Epoch: 7, Train_Loss: 1.352764368057251, Test_Loss: 1.3339072465896606 *\n",
      "245:Epoch: 7, Train_Loss: 1.370593547821045, Test_Loss: 1.3494833707809448\n",
      "246:Epoch: 7, Train_Loss: 1.4201407432556152, Test_Loss: 1.3780940771102905\n",
      "247:Epoch: 7, Train_Loss: 1.368016004562378, Test_Loss: 1.3862615823745728\n",
      "248:Epoch: 7, Train_Loss: 1.330998420715332, Test_Loss: 1.3370634317398071 *\n",
      "249:Epoch: 7, Train_Loss: 1.3304991722106934, Test_Loss: 1.431477665901184\n",
      "250:Epoch: 7, Train_Loss: 1.3310153484344482, Test_Loss: 1.4114383459091187 *\n",
      "251:Epoch: 7, Train_Loss: 1.718052864074707, Test_Loss: 1.3294758796691895 *\n",
      "252:Epoch: 7, Train_Loss: 6.709365367889404, Test_Loss: 1.3816903829574585\n",
      "253:Epoch: 7, Train_Loss: 1.3333280086517334, Test_Loss: 1.3384343385696411 *\n",
      "254:Epoch: 7, Train_Loss: 1.3791495561599731, Test_Loss: 1.3440802097320557\n",
      "255:Epoch: 7, Train_Loss: 1.412832498550415, Test_Loss: 1.332892656326294 *\n",
      "256:Epoch: 7, Train_Loss: 1.34969961643219, Test_Loss: 1.3879722356796265\n",
      "257:Epoch: 7, Train_Loss: 1.3356666564941406, Test_Loss: 1.3680951595306396 *\n",
      "258:Epoch: 7, Train_Loss: 1.3300726413726807, Test_Loss: 1.4855197668075562\n",
      "259:Epoch: 7, Train_Loss: 1.3350640535354614, Test_Loss: 1.4127198457717896 *\n",
      "260:Epoch: 7, Train_Loss: 1.3593703508377075, Test_Loss: 1.3460456132888794 *\n",
      "261:Epoch: 7, Train_Loss: 1.3469117879867554, Test_Loss: 1.3308767080307007 *\n",
      "262:Epoch: 7, Train_Loss: 1.3441927433013916, Test_Loss: 1.3299458026885986 *\n",
      "263:Epoch: 7, Train_Loss: 1.3306940793991089, Test_Loss: 1.3289425373077393 *\n",
      "264:Epoch: 7, Train_Loss: 1.328138828277588, Test_Loss: 1.3285502195358276 *\n",
      "265:Epoch: 7, Train_Loss: 1.3470027446746826, Test_Loss: 1.3285176753997803 *\n",
      "266:Epoch: 7, Train_Loss: 1.3263272047042847, Test_Loss: 1.3278011083602905 *\n",
      "267:Epoch: 7, Train_Loss: 1.324870228767395, Test_Loss: 1.3265166282653809 *\n",
      "268:Epoch: 7, Train_Loss: 1.3662643432617188, Test_Loss: 1.3360471725463867\n",
      "269:Epoch: 7, Train_Loss: 1.385001540184021, Test_Loss: 1.3339334726333618 *\n",
      "270:Epoch: 7, Train_Loss: 1.340831995010376, Test_Loss: 1.3272271156311035 *\n",
      "271:Epoch: 7, Train_Loss: 1.3233078718185425, Test_Loss: 1.3299137353897095\n",
      "272:Epoch: 7, Train_Loss: 1.3247184753417969, Test_Loss: 1.3654417991638184\n",
      "273:Epoch: 7, Train_Loss: 1.4160816669464111, Test_Loss: 1.341688632965088 *\n",
      "274:Epoch: 7, Train_Loss: 1.4167040586471558, Test_Loss: 1.5412575006484985\n",
      "275:Epoch: 7, Train_Loss: 1.4162254333496094, Test_Loss: 1.8425430059432983\n",
      "276:Epoch: 7, Train_Loss: 1.37432062625885, Test_Loss: 1.5687012672424316 *\n",
      "277:Epoch: 7, Train_Loss: 1.3560092449188232, Test_Loss: 1.3913676738739014 *\n",
      "278:Epoch: 7, Train_Loss: 1.3935962915420532, Test_Loss: 1.3445522785186768 *\n",
      "279:Epoch: 7, Train_Loss: 1.3983339071273804, Test_Loss: 1.3323689699172974 *\n",
      "280:Epoch: 7, Train_Loss: 1.3270173072814941, Test_Loss: 1.4399311542510986\n",
      "281:Epoch: 7, Train_Loss: 1.4312810897827148, Test_Loss: 2.1244289875030518\n",
      "282:Epoch: 7, Train_Loss: 1.3297773599624634, Test_Loss: 2.8266000747680664\n",
      "283:Epoch: 7, Train_Loss: 1.3241991996765137, Test_Loss: 1.6136530637741089 *\n",
      "284:Epoch: 7, Train_Loss: 1.3208389282226562, Test_Loss: 1.4029262065887451 *\n",
      "285:Epoch: 7, Train_Loss: 1.3198448419570923, Test_Loss: 1.3161715269088745 *\n",
      "286:Epoch: 7, Train_Loss: 1.3194990158081055, Test_Loss: 1.3254600763320923\n",
      "287:Epoch: 7, Train_Loss: 1.3201572895050049, Test_Loss: 1.3202663660049438 *\n",
      "288:Epoch: 7, Train_Loss: 1.705604076385498, Test_Loss: 1.329634428024292\n",
      "289:Epoch: 7, Train_Loss: 5.830395698547363, Test_Loss: 1.3638560771942139\n",
      "290:Epoch: 7, Train_Loss: 1.3318836688995361, Test_Loss: 1.3657703399658203\n",
      "291:Epoch: 7, Train_Loss: 1.3298794031143188, Test_Loss: 1.3201454877853394 *\n",
      "292:Epoch: 7, Train_Loss: 1.3320766687393188, Test_Loss: 1.412845492362976\n",
      "293:Epoch: 7, Train_Loss: 1.3108227252960205, Test_Loss: 1.6830439567565918\n",
      "294:Epoch: 7, Train_Loss: 1.3098516464233398, Test_Loss: 1.4054641723632812 *\n",
      "295:Epoch: 7, Train_Loss: 1.310449242591858, Test_Loss: 1.4994443655014038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296:Epoch: 7, Train_Loss: 1.3097010850906372, Test_Loss: 1.3211313486099243 *\n",
      "297:Epoch: 7, Train_Loss: 1.3087079524993896, Test_Loss: 1.3211828470230103\n",
      "298:Epoch: 7, Train_Loss: 1.3079122304916382, Test_Loss: 1.3212924003601074\n",
      "299:Epoch: 7, Train_Loss: 1.397651195526123, Test_Loss: 1.3211966753005981 *\n",
      "300:Epoch: 7, Train_Loss: 1.3904433250427246, Test_Loss: 1.3342400789260864\n",
      "Model saved at location ../Saver/model.ckpt at epoch 7\n",
      "301:Epoch: 7, Train_Loss: 1.40528404712677, Test_Loss: 5.469517707824707\n",
      "302:Epoch: 7, Train_Loss: 1.3713860511779785, Test_Loss: 2.5199079513549805 *\n",
      "303:Epoch: 7, Train_Loss: 1.3062279224395752, Test_Loss: 1.3186659812927246 *\n",
      "304:Epoch: 7, Train_Loss: 1.445611834526062, Test_Loss: 1.30947744846344 *\n",
      "305:Epoch: 7, Train_Loss: 1.5257251262664795, Test_Loss: 1.310846209526062\n",
      "306:Epoch: 7, Train_Loss: 1.5226260423660278, Test_Loss: 1.3196419477462769\n",
      "307:Epoch: 7, Train_Loss: 1.4635636806488037, Test_Loss: 1.308204174041748 *\n",
      "308:Epoch: 7, Train_Loss: 1.305528163909912, Test_Loss: 1.3080506324768066 *\n",
      "309:Epoch: 7, Train_Loss: 1.3029643297195435, Test_Loss: 1.304190754890442 *\n",
      "310:Epoch: 7, Train_Loss: 1.3037716150283813, Test_Loss: 1.3051512241363525\n",
      "311:Epoch: 7, Train_Loss: 1.310532569885254, Test_Loss: 1.306566596031189\n",
      "312:Epoch: 7, Train_Loss: 1.3133643865585327, Test_Loss: 1.308059811592102\n",
      "313:Epoch: 7, Train_Loss: 1.308808445930481, Test_Loss: 1.3108646869659424\n",
      "314:Epoch: 7, Train_Loss: 1.3035094738006592, Test_Loss: 1.3290358781814575\n",
      "315:Epoch: 7, Train_Loss: 1.3010321855545044, Test_Loss: 1.313456654548645 *\n",
      "316:Epoch: 7, Train_Loss: 1.3051648139953613, Test_Loss: 1.3047319650650024 *\n",
      "317:Epoch: 7, Train_Loss: 1.3198926448822021, Test_Loss: 1.3000127077102661 *\n",
      "318:Epoch: 7, Train_Loss: 1.4744197130203247, Test_Loss: 1.3024225234985352\n",
      "319:Epoch: 7, Train_Loss: 1.4721728563308716, Test_Loss: 1.3011380434036255 *\n",
      "320:Epoch: 7, Train_Loss: 1.4708398580551147, Test_Loss: 1.29961359500885 *\n",
      "321:Epoch: 7, Train_Loss: 1.3571438789367676, Test_Loss: 1.3028831481933594\n",
      "322:Epoch: 7, Train_Loss: 1.4568108320236206, Test_Loss: 1.2987394332885742 *\n",
      "323:Epoch: 7, Train_Loss: 1.4060055017471313, Test_Loss: 1.3022816181182861\n",
      "324:Epoch: 7, Train_Loss: 1.404973030090332, Test_Loss: 1.3075685501098633\n",
      "325:Epoch: 7, Train_Loss: 1.440176010131836, Test_Loss: 1.3001432418823242 *\n",
      "326:Epoch: 7, Train_Loss: 1.626298189163208, Test_Loss: 1.2998769283294678 *\n",
      "327:Epoch: 7, Train_Loss: 1.305494785308838, Test_Loss: 1.2975783348083496 *\n",
      "328:Epoch: 7, Train_Loss: 1.3123739957809448, Test_Loss: 1.2980908155441284\n",
      "329:Epoch: 7, Train_Loss: 3.976036787033081, Test_Loss: 1.2971054315567017 *\n",
      "330:Epoch: 7, Train_Loss: 1.8999850749969482, Test_Loss: 1.2983016967773438\n",
      "331:Epoch: 7, Train_Loss: 1.327449917793274, Test_Loss: 1.3475922346115112\n",
      "332:Epoch: 7, Train_Loss: 1.3453304767608643, Test_Loss: 1.3915151357650757\n",
      "333:Epoch: 7, Train_Loss: 1.3489866256713867, Test_Loss: 6.68720006942749\n",
      "334:Epoch: 7, Train_Loss: 1.3203339576721191, Test_Loss: 1.327389121055603 *\n",
      "335:Epoch: 7, Train_Loss: 1.295081377029419, Test_Loss: 1.2931931018829346 *\n",
      "336:Epoch: 7, Train_Loss: 1.3441020250320435, Test_Loss: 1.3193049430847168\n",
      "337:Epoch: 7, Train_Loss: 1.4386756420135498, Test_Loss: 1.351509928703308\n",
      "338:Epoch: 7, Train_Loss: 1.4105002880096436, Test_Loss: 1.3550585508346558\n",
      "339:Epoch: 7, Train_Loss: 1.3879060745239258, Test_Loss: 1.2948331832885742 *\n",
      "340:Epoch: 7, Train_Loss: 1.3977131843566895, Test_Loss: 1.4041086435317993\n",
      "341:Epoch: 7, Train_Loss: 1.3390063047409058, Test_Loss: 1.3388903141021729 *\n",
      "342:Epoch: 7, Train_Loss: 1.331110954284668, Test_Loss: 1.2904853820800781 *\n",
      "343:Epoch: 7, Train_Loss: 1.2999221086502075, Test_Loss: 1.329526662826538\n",
      "344:Epoch: 7, Train_Loss: 1.319268822669983, Test_Loss: 1.3071703910827637 *\n",
      "345:Epoch: 7, Train_Loss: 1.3096333742141724, Test_Loss: 1.29628586769104 *\n",
      "346:Epoch: 7, Train_Loss: 1.2875531911849976, Test_Loss: 1.325900912284851\n",
      "347:Epoch: 7, Train_Loss: 1.3006254434585571, Test_Loss: 1.4246660470962524\n",
      "348:Epoch: 7, Train_Loss: 1.3482965230941772, Test_Loss: 1.329848051071167 *\n",
      "349:Epoch: 7, Train_Loss: 1.3516572713851929, Test_Loss: 1.3935927152633667\n",
      "350:Epoch: 7, Train_Loss: 1.2898839712142944, Test_Loss: 1.331193208694458 *\n",
      "351:Epoch: 7, Train_Loss: 1.285263180732727, Test_Loss: 1.327728509902954 *\n",
      "352:Epoch: 7, Train_Loss: 1.2845205068588257, Test_Loss: 1.30831778049469 *\n",
      "353:Epoch: 7, Train_Loss: 1.2838495969772339, Test_Loss: 1.302549958229065 *\n",
      "354:Epoch: 7, Train_Loss: 1.2837929725646973, Test_Loss: 1.3048009872436523\n",
      "355:Epoch: 7, Train_Loss: 1.2838716506958008, Test_Loss: 1.3049010038375854\n",
      "356:Epoch: 7, Train_Loss: 1.2827136516571045, Test_Loss: 1.3047378063201904 *\n",
      "357:Epoch: 7, Train_Loss: 1.2829996347427368, Test_Loss: 1.3040578365325928 *\n",
      "358:Epoch: 7, Train_Loss: 1.2822777032852173, Test_Loss: 1.3066269159317017\n",
      "359:Epoch: 7, Train_Loss: 1.2814722061157227, Test_Loss: 1.3093982934951782\n",
      "360:Epoch: 7, Train_Loss: 1.2820494174957275, Test_Loss: 1.3148562908172607\n",
      "361:Epoch: 7, Train_Loss: 1.2925854921340942, Test_Loss: 1.2873430252075195 *\n",
      "362:Epoch: 7, Train_Loss: 1.2896970510482788, Test_Loss: 1.2993513345718384\n",
      "363:Epoch: 7, Train_Loss: 1.2885076999664307, Test_Loss: 1.361018419265747\n",
      "364:Epoch: 7, Train_Loss: 1.2968597412109375, Test_Loss: 1.3014061450958252 *\n",
      "365:Epoch: 7, Train_Loss: 1.2857874631881714, Test_Loss: 1.6172151565551758\n",
      "366:Epoch: 7, Train_Loss: 1.2787117958068848, Test_Loss: 1.8881059885025024\n",
      "367:Epoch: 7, Train_Loss: 1.2783395051956177, Test_Loss: 1.5130431652069092 *\n",
      "368:Epoch: 7, Train_Loss: 1.2872048616409302, Test_Loss: 1.3430029153823853 *\n",
      "369:Epoch: 7, Train_Loss: 1.298414945602417, Test_Loss: 1.3145036697387695 *\n",
      "370:Epoch: 7, Train_Loss: 1.2775691747665405, Test_Loss: 1.290001630783081 *\n",
      "371:Epoch: 7, Train_Loss: 1.277057409286499, Test_Loss: 1.3960124254226685\n",
      "372:Epoch: 7, Train_Loss: 1.2766371965408325, Test_Loss: 2.1698904037475586\n",
      "373:Epoch: 7, Train_Loss: 1.3264137506484985, Test_Loss: 2.5860211849212646\n",
      "374:Epoch: 7, Train_Loss: 1.337714433670044, Test_Loss: 1.3876183032989502 *\n",
      "375:Epoch: 7, Train_Loss: 1.3175654411315918, Test_Loss: 1.379671573638916 *\n",
      "376:Epoch: 7, Train_Loss: 1.288307785987854, Test_Loss: 1.2751766443252563 *\n",
      "377:Epoch: 7, Train_Loss: 1.2750129699707031, Test_Loss: 1.2823714017868042\n",
      "378:Epoch: 7, Train_Loss: 1.323087453842163, Test_Loss: 1.2811017036437988 *\n",
      "379:Epoch: 7, Train_Loss: 1.2813079357147217, Test_Loss: 1.291229009628296\n",
      "380:Epoch: 7, Train_Loss: 1.2847095727920532, Test_Loss: 1.3238204717636108\n",
      "381:Epoch: 7, Train_Loss: 1.3123230934143066, Test_Loss: 1.3131060600280762 *\n",
      "382:Epoch: 7, Train_Loss: 1.286759376525879, Test_Loss: 1.2806599140167236 *\n",
      "383:Epoch: 7, Train_Loss: 1.425394058227539, Test_Loss: 1.3759660720825195\n",
      "384:Epoch: 7, Train_Loss: 1.3785533905029297, Test_Loss: 1.6713292598724365\n",
      "385:Epoch: 7, Train_Loss: 1.3269940614700317, Test_Loss: 1.3584786653518677 *\n",
      "386:Epoch: 7, Train_Loss: 1.288285255432129, Test_Loss: 1.406718134880066\n",
      "387:Epoch: 7, Train_Loss: 1.2854214906692505, Test_Loss: 1.276298999786377 *\n",
      "388:Epoch: 7, Train_Loss: 1.2893328666687012, Test_Loss: 1.2757726907730103 *\n",
      "389:Epoch: 7, Train_Loss: 1.2740575075149536, Test_Loss: 1.2753283977508545 *\n",
      "390:Epoch: 7, Train_Loss: 1.2795099020004272, Test_Loss: 1.274556040763855 *\n",
      "391:Epoch: 7, Train_Loss: 1.2816674709320068, Test_Loss: 1.2923550605773926\n",
      "392:Epoch: 7, Train_Loss: 1.2902469635009766, Test_Loss: 6.503632545471191\n",
      "393:Epoch: 7, Train_Loss: 1.3486238718032837, Test_Loss: 1.6067490577697754 *\n",
      "394:Epoch: 7, Train_Loss: 1.2879977226257324, Test_Loss: 1.2754265069961548 *\n",
      "395:Epoch: 7, Train_Loss: 1.3500596284866333, Test_Loss: 1.2680349349975586 *\n",
      "396:Epoch: 7, Train_Loss: 1.2846351861953735, Test_Loss: 1.2696259021759033\n",
      "397:Epoch: 7, Train_Loss: 1.2819713354110718, Test_Loss: 1.273082971572876\n",
      "398:Epoch: 7, Train_Loss: 1.3790948390960693, Test_Loss: 1.2679723501205444 *\n",
      "399:Epoch: 7, Train_Loss: 1.46818208694458, Test_Loss: 1.270193338394165\n",
      "400:Epoch: 7, Train_Loss: 1.2694149017333984, Test_Loss: 1.2678771018981934 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 7\n",
      "401:Epoch: 7, Train_Loss: 1.293594479560852, Test_Loss: 1.267366886138916 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402:Epoch: 7, Train_Loss: 1.266090750694275, Test_Loss: 1.2692207098007202\n",
      "403:Epoch: 7, Train_Loss: 1.265589714050293, Test_Loss: 1.2711467742919922\n",
      "404:Epoch: 7, Train_Loss: 1.264896035194397, Test_Loss: 1.2739592790603638\n",
      "405:Epoch: 7, Train_Loss: 1.2632362842559814, Test_Loss: 1.2921432256698608\n",
      "406:Epoch: 7, Train_Loss: 1.2873882055282593, Test_Loss: 1.2781342267990112 *\n",
      "407:Epoch: 7, Train_Loss: 1.2910890579223633, Test_Loss: 1.2648283243179321 *\n",
      "408:Epoch: 7, Train_Loss: 1.2871493101119995, Test_Loss: 1.2617820501327515 *\n",
      "409:Epoch: 7, Train_Loss: 1.282332181930542, Test_Loss: 1.264184594154358\n",
      "410:Epoch: 7, Train_Loss: 1.2913317680358887, Test_Loss: 1.2620331048965454 *\n",
      "411:Epoch: 7, Train_Loss: 1.2700614929199219, Test_Loss: 1.2602125406265259 *\n",
      "412:Epoch: 7, Train_Loss: 1.2647258043289185, Test_Loss: 1.2639899253845215\n",
      "413:Epoch: 7, Train_Loss: 1.2608931064605713, Test_Loss: 1.259698748588562 *\n",
      "414:Epoch: 7, Train_Loss: 1.2785844802856445, Test_Loss: 1.2608407735824585\n",
      "415:Epoch: 7, Train_Loss: 1.2814314365386963, Test_Loss: 1.2622088193893433\n",
      "416:Epoch: 7, Train_Loss: 1.2759629487991333, Test_Loss: 1.2604297399520874 *\n",
      "417:Epoch: 7, Train_Loss: 1.2638144493103027, Test_Loss: 1.2590346336364746 *\n",
      "418:Epoch: 7, Train_Loss: 1.3286186456680298, Test_Loss: 1.2598029375076294\n",
      "419:Epoch: 7, Train_Loss: 1.3215123414993286, Test_Loss: 1.258392572402954 *\n",
      "420:Epoch: 7, Train_Loss: 1.2947556972503662, Test_Loss: 1.258885145187378\n",
      "421:Epoch: 7, Train_Loss: 1.2627761363983154, Test_Loss: 1.2632941007614136\n",
      "422:Epoch: 7, Train_Loss: 1.278756022453308, Test_Loss: 1.316613793373108\n",
      "423:Epoch: 7, Train_Loss: 1.2610291242599487, Test_Loss: 2.2087855339050293\n",
      "424:Epoch: 7, Train_Loss: 1.2770525217056274, Test_Loss: 5.902644157409668\n",
      "425:Epoch: 7, Train_Loss: 1.2700443267822266, Test_Loss: 1.265480399131775 *\n",
      "426:Epoch: 7, Train_Loss: 1.285817265510559, Test_Loss: 1.2554049491882324 *\n",
      "427:Epoch: 7, Train_Loss: 3.0532991886138916, Test_Loss: 1.289077639579773\n",
      "428:Epoch: 7, Train_Loss: 4.90905237197876, Test_Loss: 1.3038123846054077\n",
      "429:Epoch: 7, Train_Loss: 1.2875078916549683, Test_Loss: 1.3056321144104004\n",
      "430:Epoch: 7, Train_Loss: 1.2704651355743408, Test_Loss: 1.267798900604248 *\n",
      "431:Epoch: 7, Train_Loss: 1.274369239807129, Test_Loss: 1.3967595100402832\n",
      "432:Epoch: 7, Train_Loss: 1.4476261138916016, Test_Loss: 1.2790637016296387 *\n",
      "433:Epoch: 7, Train_Loss: 1.3008480072021484, Test_Loss: 1.2565594911575317 *\n",
      "434:Epoch: 7, Train_Loss: 1.2641348838806152, Test_Loss: 1.2984364032745361\n",
      "435:Epoch: 7, Train_Loss: 1.251156210899353, Test_Loss: 1.2597216367721558 *\n",
      "436:Epoch: 7, Train_Loss: 1.3269925117492676, Test_Loss: 1.264812707901001\n",
      "437:Epoch: 7, Train_Loss: 1.264697790145874, Test_Loss: 1.3042370080947876\n",
      "438:Epoch: 7, Train_Loss: 1.265458583831787, Test_Loss: 1.3558688163757324\n",
      "439:Epoch: 7, Train_Loss: 1.8037714958190918, Test_Loss: 1.3194515705108643 *\n",
      "440:Epoch: 7, Train_Loss: 2.678955078125, Test_Loss: 1.3605777025222778\n",
      "441:Epoch: 7, Train_Loss: 2.188979387283325, Test_Loss: 1.2774240970611572 *\n",
      "442:Epoch: 7, Train_Loss: 1.3814252614974976, Test_Loss: 1.2896122932434082\n",
      "443:Epoch: 7, Train_Loss: 1.6046075820922852, Test_Loss: 1.2677844762802124 *\n",
      "444:Epoch: 7, Train_Loss: 3.717348098754883, Test_Loss: 1.2638967037200928 *\n",
      "445:Epoch: 7, Train_Loss: 2.0080068111419678, Test_Loss: 1.265488624572754\n",
      "446:Epoch: 7, Train_Loss: 1.2991385459899902, Test_Loss: 1.266084909439087\n",
      "447:Epoch: 7, Train_Loss: 1.2758311033248901, Test_Loss: 1.2651041746139526 *\n",
      "448:Epoch: 7, Train_Loss: 2.2011806964874268, Test_Loss: 1.2644329071044922 *\n",
      "449:Epoch: 7, Train_Loss: 2.7357916831970215, Test_Loss: 1.2694809436798096\n",
      "450:Epoch: 7, Train_Loss: 1.763329029083252, Test_Loss: 1.2645270824432373 *\n",
      "451:Epoch: 7, Train_Loss: 1.2512497901916504, Test_Loss: 1.2552658319473267 *\n",
      "452:Epoch: 7, Train_Loss: 1.2515896558761597, Test_Loss: 1.2640507221221924\n",
      "453:Epoch: 7, Train_Loss: 1.7461357116699219, Test_Loss: 1.2475457191467285 *\n",
      "454:Epoch: 7, Train_Loss: 1.5727391242980957, Test_Loss: 1.2573738098144531\n",
      "1:Epoch: 8, Train_Loss: 1.2763586044311523, Test_Loss: 1.3153282403945923 *\n",
      "2:Epoch: 8, Train_Loss: 1.3153568506240845, Test_Loss: 1.503715991973877\n",
      "3:Epoch: 8, Train_Loss: 1.4129630327224731, Test_Loss: 1.5684750080108643\n",
      "4:Epoch: 8, Train_Loss: 1.28312087059021, Test_Loss: 1.4311708211898804 *\n",
      "5:Epoch: 8, Train_Loss: 1.327559471130371, Test_Loss: 1.2665126323699951 *\n",
      "6:Epoch: 8, Train_Loss: 1.6361114978790283, Test_Loss: 1.2687695026397705\n",
      "7:Epoch: 8, Train_Loss: 1.336182951927185, Test_Loss: 1.2438551187515259 *\n",
      "8:Epoch: 8, Train_Loss: 1.3253501653671265, Test_Loss: 1.3120088577270508\n",
      "9:Epoch: 8, Train_Loss: 1.3891340494155884, Test_Loss: 1.994887113571167\n",
      "10:Epoch: 8, Train_Loss: 1.353751301765442, Test_Loss: 2.0156521797180176\n",
      "11:Epoch: 8, Train_Loss: 1.3914581537246704, Test_Loss: 1.2987829446792603 *\n",
      "12:Epoch: 8, Train_Loss: 1.4209383726119995, Test_Loss: 1.2937161922454834 *\n",
      "13:Epoch: 8, Train_Loss: 1.2893164157867432, Test_Loss: 1.2689849138259888 *\n",
      "14:Epoch: 8, Train_Loss: 1.4077489376068115, Test_Loss: 1.279544711112976\n",
      "15:Epoch: 8, Train_Loss: 1.3423373699188232, Test_Loss: 1.315725564956665\n",
      "16:Epoch: 8, Train_Loss: 1.2492789030075073, Test_Loss: 1.3137503862380981 *\n",
      "17:Epoch: 8, Train_Loss: 1.2388312816619873, Test_Loss: 1.3246681690216064\n",
      "18:Epoch: 8, Train_Loss: 1.2443290948867798, Test_Loss: 1.260865330696106 *\n",
      "19:Epoch: 8, Train_Loss: 1.235817551612854, Test_Loss: 1.2612882852554321\n",
      "20:Epoch: 8, Train_Loss: 1.2439494132995605, Test_Loss: 1.3243885040283203\n",
      "21:Epoch: 8, Train_Loss: 1.2866181135177612, Test_Loss: 1.5778050422668457\n",
      "22:Epoch: 8, Train_Loss: 1.3039984703063965, Test_Loss: 1.5754462480545044 *\n",
      "23:Epoch: 8, Train_Loss: 1.2978194952011108, Test_Loss: 1.3010399341583252 *\n",
      "24:Epoch: 8, Train_Loss: 1.4158447980880737, Test_Loss: 1.2314716577529907 *\n",
      "25:Epoch: 8, Train_Loss: 1.4098191261291504, Test_Loss: 1.2311798334121704 *\n",
      "26:Epoch: 8, Train_Loss: 1.3000704050064087, Test_Loss: 1.2305067777633667 *\n",
      "27:Epoch: 8, Train_Loss: 1.2870193719863892, Test_Loss: 1.243818759918213\n",
      "28:Epoch: 8, Train_Loss: 1.2985856533050537, Test_Loss: 1.4483197927474976\n",
      "29:Epoch: 8, Train_Loss: 1.7530364990234375, Test_Loss: 6.406167030334473\n",
      "30:Epoch: 8, Train_Loss: 1.582979440689087, Test_Loss: 1.3479032516479492 *\n",
      "31:Epoch: 8, Train_Loss: 1.2380927801132202, Test_Loss: 1.2569379806518555 *\n",
      "32:Epoch: 8, Train_Loss: 1.2486387491226196, Test_Loss: 1.242268681526184 *\n",
      "33:Epoch: 8, Train_Loss: 1.7672945261001587, Test_Loss: 1.2386202812194824 *\n",
      "34:Epoch: 8, Train_Loss: 1.5896706581115723, Test_Loss: 1.2542943954467773\n",
      "35:Epoch: 8, Train_Loss: 1.3630057573318481, Test_Loss: 1.3071283102035522\n",
      "36:Epoch: 8, Train_Loss: 1.25100839138031, Test_Loss: 1.3121763467788696\n",
      "37:Epoch: 8, Train_Loss: 1.2652781009674072, Test_Loss: 1.233049750328064 *\n",
      "38:Epoch: 8, Train_Loss: 1.8236684799194336, Test_Loss: 1.2685599327087402\n",
      "39:Epoch: 8, Train_Loss: 2.346407890319824, Test_Loss: 1.2836015224456787\n",
      "40:Epoch: 8, Train_Loss: 1.3059797286987305, Test_Loss: 1.376273274421692\n",
      "41:Epoch: 8, Train_Loss: 1.2501654624938965, Test_Loss: 1.2673207521438599 *\n",
      "42:Epoch: 8, Train_Loss: 1.226706862449646, Test_Loss: 1.2921019792556763\n",
      "43:Epoch: 8, Train_Loss: 1.2369259595870972, Test_Loss: 1.3088494539260864\n",
      "44:Epoch: 8, Train_Loss: 1.5615262985229492, Test_Loss: 1.2290024757385254 *\n",
      "45:Epoch: 8, Train_Loss: 1.2463313341140747, Test_Loss: 1.233357548713684\n",
      "46:Epoch: 8, Train_Loss: 1.3271626234054565, Test_Loss: 1.2396973371505737\n",
      "47:Epoch: 8, Train_Loss: 1.4702532291412354, Test_Loss: 1.261940360069275\n",
      "48:Epoch: 8, Train_Loss: 1.2593462467193604, Test_Loss: 1.2289519309997559 *\n",
      "49:Epoch: 8, Train_Loss: 1.2363451719284058, Test_Loss: 1.2655251026153564\n",
      "50:Epoch: 8, Train_Loss: 1.3303399085998535, Test_Loss: 1.2657606601715088\n",
      "51:Epoch: 8, Train_Loss: 1.383408546447754, Test_Loss: 1.3338083028793335\n",
      "52:Epoch: 8, Train_Loss: 1.2495403289794922, Test_Loss: 1.2981853485107422 *\n",
      "53:Epoch: 8, Train_Loss: 1.2684837579727173, Test_Loss: 1.27378249168396 *\n",
      "54:Epoch: 8, Train_Loss: 1.2734202146530151, Test_Loss: 1.2385693788528442 *\n",
      "55:Epoch: 8, Train_Loss: 1.3535853624343872, Test_Loss: 1.248273253440857\n",
      "56:Epoch: 8, Train_Loss: 1.2894532680511475, Test_Loss: 1.232393741607666 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57:Epoch: 8, Train_Loss: 1.244678258895874, Test_Loss: 1.2311279773712158 *\n",
      "58:Epoch: 8, Train_Loss: 1.2587169408798218, Test_Loss: 1.2571303844451904\n",
      "59:Epoch: 8, Train_Loss: 1.2542011737823486, Test_Loss: 1.3326681852340698\n",
      "60:Epoch: 8, Train_Loss: 1.612111210823059, Test_Loss: 3.2785778045654297\n",
      "61:Epoch: 8, Train_Loss: 1.4686598777770996, Test_Loss: 4.368931293487549\n",
      "62:Epoch: 8, Train_Loss: 1.6117489337921143, Test_Loss: 1.2322638034820557 *\n",
      "63:Epoch: 8, Train_Loss: 1.5720077753067017, Test_Loss: 1.2428476810455322\n",
      "64:Epoch: 8, Train_Loss: 1.461621642112732, Test_Loss: 1.2591501474380493\n",
      "65:Epoch: 8, Train_Loss: 1.5170975923538208, Test_Loss: 1.2377896308898926 *\n",
      "66:Epoch: 8, Train_Loss: 1.2770789861679077, Test_Loss: 1.2428514957427979\n",
      "67:Epoch: 8, Train_Loss: 1.2319421768188477, Test_Loss: 1.27085280418396\n",
      "68:Epoch: 8, Train_Loss: 1.2474631071090698, Test_Loss: 1.2711564302444458\n",
      "69:Epoch: 8, Train_Loss: 1.2342523336410522, Test_Loss: 1.2359752655029297 *\n",
      "70:Epoch: 8, Train_Loss: 1.457801103591919, Test_Loss: 1.2511619329452515\n",
      "71:Epoch: 8, Train_Loss: 1.704258680343628, Test_Loss: 1.2568312883377075\n",
      "72:Epoch: 8, Train_Loss: 1.6241320371627808, Test_Loss: 1.321083903312683\n",
      "73:Epoch: 8, Train_Loss: 2.6321563720703125, Test_Loss: 1.2520543336868286 *\n",
      "74:Epoch: 8, Train_Loss: 1.5952614545822144, Test_Loss: 1.318859577178955\n",
      "75:Epoch: 8, Train_Loss: 1.61515212059021, Test_Loss: 1.3125849962234497 *\n",
      "76:Epoch: 8, Train_Loss: 1.2741096019744873, Test_Loss: 1.281569242477417 *\n",
      "77:Epoch: 8, Train_Loss: 1.2382996082305908, Test_Loss: 1.218016266822815 *\n",
      "78:Epoch: 8, Train_Loss: 1.5464367866516113, Test_Loss: 1.3120036125183105\n",
      "79:Epoch: 8, Train_Loss: 2.0801780223846436, Test_Loss: 1.2762311697006226 *\n",
      "80:Epoch: 8, Train_Loss: 1.5990674495697021, Test_Loss: 1.2528046369552612 *\n",
      "81:Epoch: 8, Train_Loss: 1.2708302736282349, Test_Loss: 1.3508268594741821\n",
      "82:Epoch: 8, Train_Loss: 1.304823637008667, Test_Loss: 1.3467758893966675 *\n",
      "83:Epoch: 8, Train_Loss: 1.2707453966140747, Test_Loss: 1.2851166725158691 *\n",
      "84:Epoch: 8, Train_Loss: 1.5654042959213257, Test_Loss: 1.3137195110321045\n",
      "85:Epoch: 8, Train_Loss: 1.448399305343628, Test_Loss: 1.3528634309768677\n",
      "86:Epoch: 8, Train_Loss: 1.64247727394104, Test_Loss: 1.3245850801467896 *\n",
      "87:Epoch: 8, Train_Loss: 1.3747857809066772, Test_Loss: 1.325474500656128\n",
      "88:Epoch: 8, Train_Loss: 1.2887294292449951, Test_Loss: 1.3392884731292725\n",
      "89:Epoch: 8, Train_Loss: 1.2405539751052856, Test_Loss: 1.4656754732131958\n",
      "90:Epoch: 8, Train_Loss: 1.2378703355789185, Test_Loss: 1.3714754581451416 *\n",
      "91:Epoch: 8, Train_Loss: 1.2641257047653198, Test_Loss: 1.281023383140564 *\n",
      "92:Epoch: 8, Train_Loss: 1.264944076538086, Test_Loss: 1.4426324367523193\n",
      "93:Epoch: 8, Train_Loss: 1.2920325994491577, Test_Loss: 1.2733997106552124 *\n",
      "94:Epoch: 8, Train_Loss: 8.061779022216797, Test_Loss: 1.2878159284591675\n",
      "95:Epoch: 8, Train_Loss: 10.486319541931152, Test_Loss: 1.237867832183838 *\n",
      "96:Epoch: 8, Train_Loss: 1.9030978679656982, Test_Loss: 1.270979404449463\n",
      "97:Epoch: 8, Train_Loss: 3.491647243499756, Test_Loss: 1.3033185005187988\n",
      "98:Epoch: 8, Train_Loss: 1.5734212398529053, Test_Loss: 1.285752773284912 *\n",
      "99:Epoch: 8, Train_Loss: 1.333604097366333, Test_Loss: 1.319974660873413\n",
      "100:Epoch: 8, Train_Loss: 1.5095491409301758, Test_Loss: 1.6144721508026123\n",
      "Model saved at location ../Saver/model.ckpt at epoch 8\n",
      "101:Epoch: 8, Train_Loss: 8.150041580200195, Test_Loss: 1.4776442050933838 *\n",
      "102:Epoch: 8, Train_Loss: 2.6015067100524902, Test_Loss: 1.3560681343078613 *\n",
      "103:Epoch: 8, Train_Loss: 1.306369662284851, Test_Loss: 1.3756134510040283\n",
      "104:Epoch: 8, Train_Loss: 3.4927818775177, Test_Loss: 1.6171860694885254\n",
      "105:Epoch: 8, Train_Loss: 3.606419086456299, Test_Loss: 1.9034976959228516\n",
      "106:Epoch: 8, Train_Loss: 1.784862756729126, Test_Loss: 2.324801206588745\n",
      "107:Epoch: 8, Train_Loss: 1.2970161437988281, Test_Loss: 1.4178425073623657 *\n",
      "108:Epoch: 8, Train_Loss: 1.2674729824066162, Test_Loss: 2.39819598197937\n",
      "109:Epoch: 8, Train_Loss: 1.2291932106018066, Test_Loss: 1.2902675867080688 *\n",
      "110:Epoch: 8, Train_Loss: 1.2361125946044922, Test_Loss: 1.5313234329223633\n",
      "111:Epoch: 8, Train_Loss: 1.2005634307861328, Test_Loss: 1.9798705577850342\n",
      "112:Epoch: 8, Train_Loss: 1.1991117000579834, Test_Loss: 1.4824731349945068 *\n",
      "113:Epoch: 8, Train_Loss: 1.1979176998138428, Test_Loss: 1.4217177629470825 *\n",
      "114:Epoch: 8, Train_Loss: 1.2032430171966553, Test_Loss: 1.3886029720306396 *\n",
      "115:Epoch: 8, Train_Loss: 1.2274274826049805, Test_Loss: 1.5074559450149536\n",
      "116:Epoch: 8, Train_Loss: 1.2286310195922852, Test_Loss: 1.4650540351867676 *\n",
      "117:Epoch: 8, Train_Loss: 1.2217296361923218, Test_Loss: 1.4132137298583984 *\n",
      "118:Epoch: 8, Train_Loss: 1.2172280550003052, Test_Loss: 1.2721858024597168 *\n",
      "119:Epoch: 8, Train_Loss: 1.220518708229065, Test_Loss: 2.045117139816284\n",
      "120:Epoch: 8, Train_Loss: 1.203238844871521, Test_Loss: 6.37961483001709\n",
      "121:Epoch: 8, Train_Loss: 1.21327805519104, Test_Loss: 1.3122267723083496 *\n",
      "122:Epoch: 8, Train_Loss: 1.2117249965667725, Test_Loss: 1.3326220512390137\n",
      "123:Epoch: 8, Train_Loss: 1.1961613893508911, Test_Loss: 1.258310079574585 *\n",
      "124:Epoch: 8, Train_Loss: 1.1977912187576294, Test_Loss: 1.2059510946273804 *\n",
      "125:Epoch: 8, Train_Loss: 1.194035291671753, Test_Loss: 1.2414323091506958\n",
      "126:Epoch: 8, Train_Loss: 1.1928846836090088, Test_Loss: 1.2603834867477417\n",
      "127:Epoch: 8, Train_Loss: 1.1940633058547974, Test_Loss: 1.2332792282104492 *\n",
      "128:Epoch: 8, Train_Loss: 1.194445252418518, Test_Loss: 1.1938363313674927 *\n",
      "129:Epoch: 8, Train_Loss: 1.1914807558059692, Test_Loss: 1.2058484554290771\n",
      "130:Epoch: 8, Train_Loss: 1.1955662965774536, Test_Loss: 1.2031234502792358 *\n",
      "131:Epoch: 8, Train_Loss: 1.2142316102981567, Test_Loss: 1.2243695259094238\n",
      "132:Epoch: 8, Train_Loss: 1.2367196083068848, Test_Loss: 1.1974482536315918 *\n",
      "133:Epoch: 8, Train_Loss: 1.2479584217071533, Test_Loss: 1.1916735172271729 *\n",
      "134:Epoch: 8, Train_Loss: 1.193626880645752, Test_Loss: 1.2069873809814453\n",
      "135:Epoch: 8, Train_Loss: 1.1911818981170654, Test_Loss: 1.1882060766220093 *\n",
      "136:Epoch: 8, Train_Loss: 9.786099433898926, Test_Loss: 1.1904593706130981\n",
      "137:Epoch: 8, Train_Loss: 2.0156197547912598, Test_Loss: 1.195911169052124\n",
      "138:Epoch: 8, Train_Loss: 1.1961034536361694, Test_Loss: 1.2073967456817627\n",
      "139:Epoch: 8, Train_Loss: 1.2003482580184937, Test_Loss: 1.200678825378418 *\n",
      "140:Epoch: 8, Train_Loss: 1.2135106325149536, Test_Loss: 1.2085108757019043\n",
      "141:Epoch: 8, Train_Loss: 1.1952693462371826, Test_Loss: 1.21427583694458\n",
      "142:Epoch: 8, Train_Loss: 1.19573974609375, Test_Loss: 1.2355437278747559\n",
      "143:Epoch: 8, Train_Loss: 1.2420662641525269, Test_Loss: 1.228293538093567 *\n",
      "144:Epoch: 8, Train_Loss: 1.3700549602508545, Test_Loss: 1.2190618515014648 *\n",
      "145:Epoch: 8, Train_Loss: 1.4246550798416138, Test_Loss: 1.2088534832000732 *\n",
      "146:Epoch: 8, Train_Loss: 1.3705823421478271, Test_Loss: 1.2105987071990967\n",
      "147:Epoch: 8, Train_Loss: 1.2085107564926147, Test_Loss: 1.2019083499908447 *\n",
      "148:Epoch: 8, Train_Loss: 1.2820358276367188, Test_Loss: 1.2021623849868774\n",
      "149:Epoch: 8, Train_Loss: 1.2989617586135864, Test_Loss: 1.2300083637237549\n",
      "150:Epoch: 8, Train_Loss: 1.3254666328430176, Test_Loss: 1.2343974113464355\n",
      "151:Epoch: 8, Train_Loss: 1.3118085861206055, Test_Loss: 4.490564346313477\n",
      "152:Epoch: 8, Train_Loss: 1.290108561515808, Test_Loss: 3.63796067237854 *\n",
      "153:Epoch: 8, Train_Loss: 1.2345240116119385, Test_Loss: 1.1866319179534912 *\n",
      "154:Epoch: 8, Train_Loss: 1.1983623504638672, Test_Loss: 1.1844658851623535 *\n",
      "155:Epoch: 8, Train_Loss: 1.2706096172332764, Test_Loss: 1.2268142700195312\n",
      "156:Epoch: 8, Train_Loss: 1.2152098417282104, Test_Loss: 1.2283048629760742\n",
      "157:Epoch: 8, Train_Loss: 1.1855602264404297, Test_Loss: 1.2089530229568481 *\n",
      "158:Epoch: 8, Train_Loss: 1.1827661991119385, Test_Loss: 1.2516915798187256\n",
      "159:Epoch: 8, Train_Loss: 1.1827908754348755, Test_Loss: 1.2924343347549438\n",
      "160:Epoch: 8, Train_Loss: 1.242308259010315, Test_Loss: 1.1806122064590454 *\n",
      "161:Epoch: 8, Train_Loss: 6.582281112670898, Test_Loss: 1.2128881216049194\n",
      "162:Epoch: 8, Train_Loss: 1.213963508605957, Test_Loss: 1.19792902469635 *\n",
      "163:Epoch: 8, Train_Loss: 1.1997747421264648, Test_Loss: 1.1928257942199707 *\n",
      "164:Epoch: 8, Train_Loss: 1.292783498764038, Test_Loss: 1.1846319437026978 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165:Epoch: 8, Train_Loss: 1.2010434865951538, Test_Loss: 1.2157176733016968\n",
      "166:Epoch: 8, Train_Loss: 1.1896098852157593, Test_Loss: 1.2234660387039185\n",
      "167:Epoch: 8, Train_Loss: 1.186427116394043, Test_Loss: 1.278328537940979\n",
      "168:Epoch: 8, Train_Loss: 1.2021106481552124, Test_Loss: 1.2791962623596191\n",
      "169:Epoch: 8, Train_Loss: 1.1863102912902832, Test_Loss: 1.1953121423721313 *\n",
      "170:Epoch: 8, Train_Loss: 1.1861681938171387, Test_Loss: 1.1884280443191528 *\n",
      "171:Epoch: 8, Train_Loss: 1.2319743633270264, Test_Loss: 1.1790528297424316 *\n",
      "172:Epoch: 8, Train_Loss: 1.1776013374328613, Test_Loss: 1.1775425672531128 *\n",
      "173:Epoch: 8, Train_Loss: 1.1748020648956299, Test_Loss: 1.1795574426651\n",
      "174:Epoch: 8, Train_Loss: 1.1900242567062378, Test_Loss: 1.1831343173980713\n",
      "175:Epoch: 8, Train_Loss: 1.1746628284454346, Test_Loss: 1.1830424070358276 *\n",
      "176:Epoch: 8, Train_Loss: 1.174223780632019, Test_Loss: 1.180307149887085 *\n",
      "177:Epoch: 8, Train_Loss: 1.1931809186935425, Test_Loss: 1.1908369064331055\n",
      "178:Epoch: 8, Train_Loss: 1.2184792757034302, Test_Loss: 1.1844462156295776 *\n",
      "179:Epoch: 8, Train_Loss: 1.1970207691192627, Test_Loss: 1.177655577659607 *\n",
      "180:Epoch: 8, Train_Loss: 1.1710039377212524, Test_Loss: 1.1805474758148193\n",
      "181:Epoch: 8, Train_Loss: 1.170927882194519, Test_Loss: 1.1891621351242065\n",
      "182:Epoch: 8, Train_Loss: 1.2557761669158936, Test_Loss: 1.2019325494766235\n",
      "183:Epoch: 8, Train_Loss: 1.2510344982147217, Test_Loss: 1.232347011566162\n",
      "184:Epoch: 8, Train_Loss: 1.2349936962127686, Test_Loss: 1.6295503377914429\n",
      "185:Epoch: 8, Train_Loss: 1.1926867961883545, Test_Loss: 1.5468616485595703 *\n",
      "186:Epoch: 8, Train_Loss: 1.247220516204834, Test_Loss: 1.2776635885238647 *\n",
      "187:Epoch: 8, Train_Loss: 1.2433124780654907, Test_Loss: 1.1798173189163208 *\n",
      "188:Epoch: 8, Train_Loss: 1.2246538400650024, Test_Loss: 1.1873234510421753\n",
      "189:Epoch: 8, Train_Loss: 1.2031655311584473, Test_Loss: 1.2343398332595825\n",
      "190:Epoch: 8, Train_Loss: 1.3343262672424316, Test_Loss: 1.601607322692871\n",
      "191:Epoch: 8, Train_Loss: 1.1974843740463257, Test_Loss: 2.437666416168213\n",
      "192:Epoch: 8, Train_Loss: 1.1815365552902222, Test_Loss: 1.7470903396606445 *\n",
      "193:Epoch: 8, Train_Loss: 1.1703550815582275, Test_Loss: 1.24166738986969 *\n",
      "194:Epoch: 8, Train_Loss: 1.169150948524475, Test_Loss: 1.1829484701156616 *\n",
      "195:Epoch: 8, Train_Loss: 1.1685667037963867, Test_Loss: 1.1732430458068848 *\n",
      "196:Epoch: 8, Train_Loss: 1.1685000658035278, Test_Loss: 1.1675736904144287 *\n",
      "197:Epoch: 8, Train_Loss: 1.1743980646133423, Test_Loss: 1.1726874113082886\n",
      "198:Epoch: 8, Train_Loss: 5.766923427581787, Test_Loss: 1.1997568607330322\n",
      "199:Epoch: 8, Train_Loss: 1.4488171339035034, Test_Loss: 1.2280737161636353\n",
      "200:Epoch: 8, Train_Loss: 1.169830560684204, Test_Loss: 1.165771722793579 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 8\n",
      "201:Epoch: 8, Train_Loss: 1.1859463453292847, Test_Loss: 1.227474570274353\n",
      "202:Epoch: 8, Train_Loss: 1.166253685951233, Test_Loss: 1.3055617809295654\n",
      "203:Epoch: 8, Train_Loss: 1.161724328994751, Test_Loss: 1.5070863962173462\n",
      "204:Epoch: 8, Train_Loss: 1.162557601928711, Test_Loss: 1.3749022483825684 *\n",
      "205:Epoch: 8, Train_Loss: 1.1617157459259033, Test_Loss: 1.173148512840271 *\n",
      "206:Epoch: 8, Train_Loss: 1.1608026027679443, Test_Loss: 1.1693155765533447 *\n",
      "207:Epoch: 8, Train_Loss: 1.1603500843048096, Test_Loss: 1.1692132949829102 *\n",
      "208:Epoch: 8, Train_Loss: 1.2218360900878906, Test_Loss: 1.1691924333572388 *\n",
      "209:Epoch: 8, Train_Loss: 1.2536308765411377, Test_Loss: 1.1774123907089233\n",
      "210:Epoch: 8, Train_Loss: 1.2611392736434937, Test_Loss: 3.2124085426330566\n",
      "211:Epoch: 8, Train_Loss: 1.2520887851715088, Test_Loss: 4.556850433349609\n",
      "212:Epoch: 8, Train_Loss: 1.1592772006988525, Test_Loss: 1.1716818809509277 *\n",
      "213:Epoch: 8, Train_Loss: 1.2371565103530884, Test_Loss: 1.1623586416244507 *\n",
      "214:Epoch: 8, Train_Loss: 1.3752416372299194, Test_Loss: 1.1622012853622437 *\n",
      "215:Epoch: 8, Train_Loss: 1.3687962293624878, Test_Loss: 1.169985294342041\n",
      "216:Epoch: 8, Train_Loss: 1.3676257133483887, Test_Loss: 1.160698413848877 *\n",
      "217:Epoch: 8, Train_Loss: 1.1595540046691895, Test_Loss: 1.1623507738113403\n",
      "218:Epoch: 8, Train_Loss: 1.156038522720337, Test_Loss: 1.158517837524414 *\n",
      "219:Epoch: 8, Train_Loss: 1.156377911567688, Test_Loss: 1.1584808826446533 *\n",
      "220:Epoch: 8, Train_Loss: 1.1641547679901123, Test_Loss: 1.1596932411193848\n",
      "221:Epoch: 8, Train_Loss: 1.1664841175079346, Test_Loss: 1.1564607620239258 *\n",
      "222:Epoch: 8, Train_Loss: 1.164825201034546, Test_Loss: 1.1624873876571655\n",
      "223:Epoch: 8, Train_Loss: 1.1599501371383667, Test_Loss: 1.1865758895874023\n",
      "224:Epoch: 8, Train_Loss: 1.154236912727356, Test_Loss: 1.1797692775726318 *\n",
      "225:Epoch: 8, Train_Loss: 1.1577478647232056, Test_Loss: 1.1592832803726196 *\n",
      "226:Epoch: 8, Train_Loss: 1.1650865077972412, Test_Loss: 1.1534427404403687 *\n",
      "227:Epoch: 8, Train_Loss: 1.3084940910339355, Test_Loss: 1.1539512872695923\n",
      "228:Epoch: 8, Train_Loss: 1.3323642015457153, Test_Loss: 1.1535757780075073 *\n",
      "229:Epoch: 8, Train_Loss: 1.3540087938308716, Test_Loss: 1.1531212329864502 *\n",
      "230:Epoch: 8, Train_Loss: 1.196834683418274, Test_Loss: 1.154821753501892\n",
      "231:Epoch: 8, Train_Loss: 1.2990667819976807, Test_Loss: 1.1522010564804077 *\n",
      "232:Epoch: 8, Train_Loss: 1.2906644344329834, Test_Loss: 1.1527717113494873\n",
      "233:Epoch: 8, Train_Loss: 1.1974906921386719, Test_Loss: 1.1551762819290161\n",
      "234:Epoch: 8, Train_Loss: 1.3013263940811157, Test_Loss: 1.1520178318023682 *\n",
      "235:Epoch: 8, Train_Loss: 1.3239675760269165, Test_Loss: 1.152693271636963\n",
      "236:Epoch: 8, Train_Loss: 1.3283483982086182, Test_Loss: 1.1509809494018555 *\n",
      "237:Epoch: 8, Train_Loss: 1.1646171808242798, Test_Loss: 1.1513628959655762\n",
      "238:Epoch: 8, Train_Loss: 2.9284446239471436, Test_Loss: 1.150062918663025 *\n",
      "239:Epoch: 8, Train_Loss: 2.5826127529144287, Test_Loss: 1.1495705842971802 *\n",
      "240:Epoch: 8, Train_Loss: 1.1850603818893433, Test_Loss: 1.1860326528549194\n",
      "241:Epoch: 8, Train_Loss: 1.2032935619354248, Test_Loss: 1.1812242269515991 *\n",
      "242:Epoch: 8, Train_Loss: 1.2070332765579224, Test_Loss: 5.440121173858643\n",
      "243:Epoch: 8, Train_Loss: 1.1903862953186035, Test_Loss: 2.327996015548706 *\n",
      "244:Epoch: 8, Train_Loss: 1.1476556062698364, Test_Loss: 1.1476619243621826 *\n",
      "245:Epoch: 8, Train_Loss: 1.1669071912765503, Test_Loss: 1.1599489450454712\n",
      "246:Epoch: 8, Train_Loss: 1.2842425107955933, Test_Loss: 1.2097501754760742\n",
      "247:Epoch: 8, Train_Loss: 1.243247389793396, Test_Loss: 1.2243322134017944\n",
      "248:Epoch: 8, Train_Loss: 1.244863748550415, Test_Loss: 1.1617612838745117 *\n",
      "249:Epoch: 8, Train_Loss: 1.2452821731567383, Test_Loss: 1.2222527265548706\n",
      "250:Epoch: 8, Train_Loss: 1.210352897644043, Test_Loss: 1.223740816116333\n",
      "251:Epoch: 8, Train_Loss: 1.177794337272644, Test_Loss: 1.1466752290725708 *\n",
      "252:Epoch: 8, Train_Loss: 1.1636269092559814, Test_Loss: 1.1830453872680664\n",
      "253:Epoch: 8, Train_Loss: 1.1678093671798706, Test_Loss: 1.159833550453186 *\n",
      "254:Epoch: 8, Train_Loss: 1.1699923276901245, Test_Loss: 1.1558473110198975 *\n",
      "255:Epoch: 8, Train_Loss: 1.1493407487869263, Test_Loss: 1.1466461420059204 *\n",
      "256:Epoch: 8, Train_Loss: 1.1441595554351807, Test_Loss: 1.2882533073425293\n",
      "257:Epoch: 8, Train_Loss: 1.1936501264572144, Test_Loss: 1.1910971403121948 *\n",
      "258:Epoch: 8, Train_Loss: 1.2024222612380981, Test_Loss: 1.2549046277999878\n",
      "259:Epoch: 8, Train_Loss: 1.1594936847686768, Test_Loss: 1.2165210247039795 *\n",
      "260:Epoch: 8, Train_Loss: 1.140349268913269, Test_Loss: 1.1774730682373047 *\n",
      "261:Epoch: 8, Train_Loss: 1.1400936841964722, Test_Loss: 1.1644477844238281 *\n",
      "262:Epoch: 8, Train_Loss: 1.1392993927001953, Test_Loss: 1.165393352508545\n",
      "263:Epoch: 8, Train_Loss: 1.1393409967422485, Test_Loss: 1.1629637479782104 *\n",
      "264:Epoch: 8, Train_Loss: 1.1390199661254883, Test_Loss: 1.164094090461731\n",
      "265:Epoch: 8, Train_Loss: 1.138904333114624, Test_Loss: 1.1638089418411255 *\n",
      "266:Epoch: 8, Train_Loss: 1.1390247344970703, Test_Loss: 1.1631779670715332 *\n",
      "267:Epoch: 8, Train_Loss: 1.1384409666061401, Test_Loss: 1.1608363389968872 *\n",
      "268:Epoch: 8, Train_Loss: 1.1374489068984985, Test_Loss: 1.1790671348571777\n",
      "269:Epoch: 8, Train_Loss: 1.137512445449829, Test_Loss: 1.1705009937286377 *\n",
      "270:Epoch: 8, Train_Loss: 1.1472227573394775, Test_Loss: 1.157205581665039 *\n",
      "271:Epoch: 8, Train_Loss: 1.1486263275146484, Test_Loss: 1.1499241590499878 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272:Epoch: 8, Train_Loss: 1.1477162837982178, Test_Loss: 1.2062519788742065\n",
      "273:Epoch: 8, Train_Loss: 1.1571649312973022, Test_Loss: 1.1881605386734009 *\n",
      "274:Epoch: 8, Train_Loss: 1.1416000127792358, Test_Loss: 1.3537620306015015\n",
      "275:Epoch: 8, Train_Loss: 1.1360641717910767, Test_Loss: 1.745458960533142\n",
      "276:Epoch: 8, Train_Loss: 1.1344833374023438, Test_Loss: 1.5099129676818848 *\n",
      "277:Epoch: 8, Train_Loss: 1.138460636138916, Test_Loss: 1.2510769367218018 *\n",
      "278:Epoch: 8, Train_Loss: 1.1586017608642578, Test_Loss: 1.1803394556045532 *\n",
      "279:Epoch: 8, Train_Loss: 1.1398773193359375, Test_Loss: 1.144553303718567 *\n",
      "280:Epoch: 8, Train_Loss: 1.1343278884887695, Test_Loss: 1.208988070487976\n",
      "281:Epoch: 8, Train_Loss: 1.132439136505127, Test_Loss: 1.7273473739624023\n",
      "282:Epoch: 8, Train_Loss: 1.148566484451294, Test_Loss: 2.4327239990234375\n",
      "283:Epoch: 8, Train_Loss: 1.2025113105773926, Test_Loss: 1.48675537109375 *\n",
      "284:Epoch: 8, Train_Loss: 1.181251883506775, Test_Loss: 1.2483469247817993 *\n",
      "285:Epoch: 8, Train_Loss: 1.1686224937438965, Test_Loss: 1.1376973390579224 *\n",
      "286:Epoch: 8, Train_Loss: 1.1304348707199097, Test_Loss: 1.135951042175293 *\n",
      "287:Epoch: 8, Train_Loss: 1.1841700077056885, Test_Loss: 1.1311008930206299 *\n",
      "288:Epoch: 8, Train_Loss: 1.1534907817840576, Test_Loss: 1.1415995359420776\n",
      "289:Epoch: 8, Train_Loss: 1.1323426961898804, Test_Loss: 1.1595244407653809\n",
      "290:Epoch: 8, Train_Loss: 1.1514006853103638, Test_Loss: 1.1738207340240479\n",
      "291:Epoch: 8, Train_Loss: 1.1541459560394287, Test_Loss: 1.1329563856124878 *\n",
      "292:Epoch: 8, Train_Loss: 1.235424280166626, Test_Loss: 1.2367379665374756\n",
      "293:Epoch: 8, Train_Loss: 1.2150778770446777, Test_Loss: 1.4664682149887085\n",
      "294:Epoch: 8, Train_Loss: 1.1864975690841675, Test_Loss: 1.2800860404968262 *\n",
      "295:Epoch: 8, Train_Loss: 1.1469242572784424, Test_Loss: 1.3345223665237427\n",
      "296:Epoch: 8, Train_Loss: 1.1324235200881958, Test_Loss: 1.1405372619628906 *\n",
      "297:Epoch: 8, Train_Loss: 1.148058533668518, Test_Loss: 1.1398611068725586 *\n",
      "298:Epoch: 8, Train_Loss: 1.1275917291641235, Test_Loss: 1.1393033266067505 *\n",
      "299:Epoch: 8, Train_Loss: 1.1316876411437988, Test_Loss: 1.1388463973999023 *\n",
      "300:Epoch: 8, Train_Loss: 1.1493526697158813, Test_Loss: 1.1489784717559814\n",
      "Model saved at location ../Saver/model.ckpt at epoch 8\n",
      "301:Epoch: 8, Train_Loss: 1.1516273021697998, Test_Loss: 4.559495449066162\n",
      "302:Epoch: 8, Train_Loss: 1.2316967248916626, Test_Loss: 3.104740619659424 *\n",
      "303:Epoch: 8, Train_Loss: 1.1286075115203857, Test_Loss: 1.1376498937606812 *\n",
      "304:Epoch: 8, Train_Loss: 1.1998274326324463, Test_Loss: 1.1288954019546509 *\n",
      "305:Epoch: 8, Train_Loss: 1.1345419883728027, Test_Loss: 1.129164218902588\n",
      "306:Epoch: 8, Train_Loss: 1.1620169878005981, Test_Loss: 1.1387079954147339\n",
      "307:Epoch: 8, Train_Loss: 1.1754509210586548, Test_Loss: 1.1269350051879883 *\n",
      "308:Epoch: 8, Train_Loss: 1.3986586332321167, Test_Loss: 1.1268492937088013 *\n",
      "309:Epoch: 8, Train_Loss: 1.1383390426635742, Test_Loss: 1.1228641271591187 *\n",
      "310:Epoch: 8, Train_Loss: 1.1585602760314941, Test_Loss: 1.1238014698028564\n",
      "311:Epoch: 8, Train_Loss: 1.1214523315429688, Test_Loss: 1.1243443489074707\n",
      "312:Epoch: 8, Train_Loss: 1.1212455034255981, Test_Loss: 1.1222138404846191 *\n",
      "313:Epoch: 8, Train_Loss: 1.1218774318695068, Test_Loss: 1.1312931776046753\n",
      "314:Epoch: 8, Train_Loss: 1.1201651096343994, Test_Loss: 1.1575489044189453\n",
      "315:Epoch: 8, Train_Loss: 1.1339818239212036, Test_Loss: 1.1442726850509644 *\n",
      "316:Epoch: 8, Train_Loss: 1.1362082958221436, Test_Loss: 1.1218067407608032 *\n",
      "317:Epoch: 8, Train_Loss: 1.1393933296203613, Test_Loss: 1.1196119785308838 *\n",
      "318:Epoch: 8, Train_Loss: 1.134665846824646, Test_Loss: 1.1191357374191284 *\n",
      "319:Epoch: 8, Train_Loss: 1.1367727518081665, Test_Loss: 1.11925208568573\n",
      "320:Epoch: 8, Train_Loss: 1.132598638534546, Test_Loss: 1.1178910732269287 *\n",
      "321:Epoch: 8, Train_Loss: 1.120140790939331, Test_Loss: 1.119236707687378\n",
      "322:Epoch: 8, Train_Loss: 1.116895318031311, Test_Loss: 1.1169956922531128 *\n",
      "323:Epoch: 8, Train_Loss: 1.138211965560913, Test_Loss: 1.116959571838379 *\n",
      "324:Epoch: 8, Train_Loss: 1.1463143825531006, Test_Loss: 1.1182068586349487\n",
      "325:Epoch: 8, Train_Loss: 1.1531306505203247, Test_Loss: 1.1167515516281128 *\n",
      "326:Epoch: 8, Train_Loss: 1.1156655550003052, Test_Loss: 1.1165735721588135 *\n",
      "327:Epoch: 8, Train_Loss: 1.1648778915405273, Test_Loss: 1.1155285835266113 *\n",
      "328:Epoch: 8, Train_Loss: 1.1726919412612915, Test_Loss: 1.1152679920196533 *\n",
      "329:Epoch: 8, Train_Loss: 1.1575266122817993, Test_Loss: 1.114988923072815 *\n",
      "330:Epoch: 8, Train_Loss: 1.1159751415252686, Test_Loss: 1.1154009103775024\n",
      "331:Epoch: 8, Train_Loss: 1.1509699821472168, Test_Loss: 1.15338933467865\n",
      "332:Epoch: 8, Train_Loss: 1.115108609199524, Test_Loss: 1.146501898765564 *\n",
      "333:Epoch: 8, Train_Loss: 1.1327564716339111, Test_Loss: 6.370793342590332\n",
      "334:Epoch: 8, Train_Loss: 1.1169215440750122, Test_Loss: 1.2915804386138916 *\n",
      "335:Epoch: 8, Train_Loss: 1.1419847011566162, Test_Loss: 1.1133214235305786 *\n",
      "336:Epoch: 8, Train_Loss: 1.4957963228225708, Test_Loss: 1.1389907598495483\n",
      "337:Epoch: 8, Train_Loss: 4.98942232131958, Test_Loss: 1.1815357208251953\n",
      "338:Epoch: 8, Train_Loss: 2.368736743927002, Test_Loss: 1.1876907348632812\n",
      "339:Epoch: 8, Train_Loss: 1.1278146505355835, Test_Loss: 1.1185712814331055 *\n",
      "340:Epoch: 8, Train_Loss: 1.1137278079986572, Test_Loss: 1.2062675952911377\n",
      "341:Epoch: 8, Train_Loss: 1.2598750591278076, Test_Loss: 1.1713742017745972 *\n",
      "342:Epoch: 8, Train_Loss: 1.2003308534622192, Test_Loss: 1.1112226247787476 *\n",
      "343:Epoch: 8, Train_Loss: 1.1253608465194702, Test_Loss: 1.152206540107727\n",
      "344:Epoch: 8, Train_Loss: 1.109519124031067, Test_Loss: 1.1253701448440552 *\n",
      "345:Epoch: 8, Train_Loss: 1.170021891593933, Test_Loss: 1.1191469430923462 *\n",
      "346:Epoch: 8, Train_Loss: 1.1304093599319458, Test_Loss: 1.1289604902267456\n",
      "347:Epoch: 8, Train_Loss: 1.1292420625686646, Test_Loss: 1.2608293294906616\n",
      "348:Epoch: 8, Train_Loss: 1.3269106149673462, Test_Loss: 1.143042802810669 *\n",
      "349:Epoch: 8, Train_Loss: 2.4676742553710938, Test_Loss: 1.2208271026611328\n",
      "350:Epoch: 8, Train_Loss: 2.3602566719055176, Test_Loss: 1.1615452766418457 *\n",
      "351:Epoch: 8, Train_Loss: 1.2151216268539429, Test_Loss: 1.1497491598129272 *\n",
      "352:Epoch: 8, Train_Loss: 1.186362385749817, Test_Loss: 1.1318223476409912 *\n",
      "353:Epoch: 8, Train_Loss: 3.3517251014709473, Test_Loss: 1.129851222038269 *\n",
      "354:Epoch: 8, Train_Loss: 2.382673501968384, Test_Loss: 1.1292413473129272 *\n",
      "355:Epoch: 8, Train_Loss: 1.1609175205230713, Test_Loss: 1.1307116746902466\n",
      "356:Epoch: 8, Train_Loss: 1.144266128540039, Test_Loss: 1.1300196647644043 *\n",
      "357:Epoch: 8, Train_Loss: 1.614569902420044, Test_Loss: 1.1301425695419312\n",
      "358:Epoch: 8, Train_Loss: 2.7419114112854004, Test_Loss: 1.126225233078003 *\n",
      "359:Epoch: 8, Train_Loss: 2.0423598289489746, Test_Loss: 1.1387031078338623\n",
      "360:Epoch: 8, Train_Loss: 1.1139028072357178, Test_Loss: 1.1346997022628784 *\n",
      "361:Epoch: 8, Train_Loss: 1.1212035417556763, Test_Loss: 1.1114990711212158 *\n",
      "362:Epoch: 8, Train_Loss: 1.364797830581665, Test_Loss: 1.1146939992904663\n",
      "363:Epoch: 8, Train_Loss: 1.625903606414795, Test_Loss: 1.1602365970611572\n",
      "364:Epoch: 8, Train_Loss: 1.1299493312835693, Test_Loss: 1.1221078634262085 *\n",
      "365:Epoch: 8, Train_Loss: 1.1675149202346802, Test_Loss: 1.3766838312149048\n",
      "366:Epoch: 8, Train_Loss: 1.2141128778457642, Test_Loss: 1.6291160583496094\n",
      "367:Epoch: 8, Train_Loss: 1.2007458209991455, Test_Loss: 1.3780521154403687 *\n",
      "368:Epoch: 8, Train_Loss: 1.2000778913497925, Test_Loss: 1.182839035987854 *\n",
      "369:Epoch: 8, Train_Loss: 1.3973548412322998, Test_Loss: 1.1245052814483643 *\n",
      "370:Epoch: 8, Train_Loss: 1.2356598377227783, Test_Loss: 1.105054497718811 *\n",
      "371:Epoch: 8, Train_Loss: 1.1585955619812012, Test_Loss: 1.1708545684814453\n",
      "372:Epoch: 8, Train_Loss: 1.2980914115905762, Test_Loss: 1.6880457401275635\n",
      "373:Epoch: 8, Train_Loss: 1.2686268091201782, Test_Loss: 2.074270009994507\n",
      "374:Epoch: 8, Train_Loss: 1.376822590827942, Test_Loss: 1.24738609790802 *\n",
      "375:Epoch: 8, Train_Loss: 1.2971625328063965, Test_Loss: 1.1622806787490845 *\n",
      "376:Epoch: 8, Train_Loss: 1.1407912969589233, Test_Loss: 1.1071034669876099 *\n",
      "377:Epoch: 8, Train_Loss: 1.2581567764282227, Test_Loss: 1.129658579826355\n",
      "378:Epoch: 8, Train_Loss: 1.2300503253936768, Test_Loss: 1.1291325092315674 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379:Epoch: 8, Train_Loss: 1.1159673929214478, Test_Loss: 1.1783630847930908\n",
      "380:Epoch: 8, Train_Loss: 1.1015734672546387, Test_Loss: 1.128401756286621 *\n",
      "381:Epoch: 8, Train_Loss: 1.10594642162323, Test_Loss: 1.1481853723526\n",
      "382:Epoch: 8, Train_Loss: 1.0997244119644165, Test_Loss: 1.1109580993652344 *\n",
      "383:Epoch: 8, Train_Loss: 1.103524088859558, Test_Loss: 1.2005810737609863\n",
      "384:Epoch: 8, Train_Loss: 1.1288785934448242, Test_Loss: 1.4441239833831787\n",
      "385:Epoch: 8, Train_Loss: 1.1633493900299072, Test_Loss: 1.2244513034820557 *\n",
      "386:Epoch: 8, Train_Loss: 1.1477779150009155, Test_Loss: 1.329256296157837\n",
      "387:Epoch: 8, Train_Loss: 1.2077511548995972, Test_Loss: 1.0956263542175293 *\n",
      "388:Epoch: 8, Train_Loss: 1.1966285705566406, Test_Loss: 1.0936452150344849 *\n",
      "389:Epoch: 8, Train_Loss: 1.3401434421539307, Test_Loss: 1.0935155153274536 *\n",
      "390:Epoch: 8, Train_Loss: 1.1178393363952637, Test_Loss: 1.0948340892791748\n",
      "391:Epoch: 8, Train_Loss: 1.128739595413208, Test_Loss: 1.1706234216690063\n",
      "392:Epoch: 8, Train_Loss: 1.5614179372787476, Test_Loss: 6.106925010681152\n",
      "393:Epoch: 8, Train_Loss: 1.5416579246520996, Test_Loss: 1.9472715854644775 *\n",
      "394:Epoch: 8, Train_Loss: 1.151038646697998, Test_Loss: 1.1327284574508667 *\n",
      "395:Epoch: 8, Train_Loss: 1.1413682699203491, Test_Loss: 1.1056193113327026 *\n",
      "396:Epoch: 8, Train_Loss: 1.5068891048431396, Test_Loss: 1.1141512393951416\n",
      "397:Epoch: 8, Train_Loss: 1.4519894123077393, Test_Loss: 1.1009349822998047 *\n",
      "398:Epoch: 8, Train_Loss: 1.2535748481750488, Test_Loss: 1.1592580080032349\n",
      "399:Epoch: 8, Train_Loss: 1.1152334213256836, Test_Loss: 1.1973458528518677\n",
      "400:Epoch: 8, Train_Loss: 1.1328144073486328, Test_Loss: 1.1099705696105957 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 8\n",
      "401:Epoch: 8, Train_Loss: 1.3687776327133179, Test_Loss: 1.146641492843628\n",
      "402:Epoch: 8, Train_Loss: 2.4694106578826904, Test_Loss: 1.1375799179077148 *\n",
      "403:Epoch: 8, Train_Loss: 1.3358428478240967, Test_Loss: 1.1863943338394165\n",
      "404:Epoch: 8, Train_Loss: 1.1170586347579956, Test_Loss: 1.1588926315307617 *\n",
      "405:Epoch: 8, Train_Loss: 1.0948549509048462, Test_Loss: 1.1204123497009277 *\n",
      "406:Epoch: 8, Train_Loss: 1.0933886766433716, Test_Loss: 1.1528246402740479\n",
      "407:Epoch: 8, Train_Loss: 1.4465374946594238, Test_Loss: 1.1183452606201172 *\n",
      "408:Epoch: 8, Train_Loss: 1.1880801916122437, Test_Loss: 1.0982022285461426 *\n",
      "409:Epoch: 8, Train_Loss: 1.101605772972107, Test_Loss: 1.095792531967163 *\n",
      "410:Epoch: 8, Train_Loss: 1.3816585540771484, Test_Loss: 1.1051331758499146\n",
      "411:Epoch: 8, Train_Loss: 1.105370044708252, Test_Loss: 1.1042859554290771 *\n",
      "412:Epoch: 8, Train_Loss: 1.1159043312072754, Test_Loss: 1.0976203680038452 *\n",
      "413:Epoch: 8, Train_Loss: 1.1699769496917725, Test_Loss: 1.1254972219467163\n",
      "414:Epoch: 8, Train_Loss: 1.3219016790390015, Test_Loss: 1.1426379680633545\n",
      "415:Epoch: 8, Train_Loss: 1.1262757778167725, Test_Loss: 1.1155871152877808 *\n",
      "416:Epoch: 8, Train_Loss: 1.1588119268417358, Test_Loss: 1.134313941001892\n",
      "417:Epoch: 8, Train_Loss: 1.112790822982788, Test_Loss: 1.1014506816864014 *\n",
      "418:Epoch: 8, Train_Loss: 1.248076319694519, Test_Loss: 1.0988129377365112 *\n",
      "419:Epoch: 8, Train_Loss: 1.142677903175354, Test_Loss: 1.1033142805099487\n",
      "420:Epoch: 8, Train_Loss: 1.1002322435379028, Test_Loss: 1.1005600690841675 *\n",
      "421:Epoch: 8, Train_Loss: 1.1041501760482788, Test_Loss: 1.1105427742004395\n",
      "422:Epoch: 8, Train_Loss: 1.1366640329360962, Test_Loss: 1.1800459623336792\n",
      "423:Epoch: 8, Train_Loss: 1.3061405420303345, Test_Loss: 1.4119281768798828\n",
      "424:Epoch: 8, Train_Loss: 1.3943301439285278, Test_Loss: 6.326756954193115\n",
      "425:Epoch: 8, Train_Loss: 1.3319069147109985, Test_Loss: 1.0860050916671753 *\n",
      "426:Epoch: 8, Train_Loss: 1.443345546722412, Test_Loss: 1.1086775064468384\n",
      "427:Epoch: 8, Train_Loss: 1.347710371017456, Test_Loss: 1.126432180404663\n",
      "428:Epoch: 8, Train_Loss: 1.3454716205596924, Test_Loss: 1.0900352001190186 *\n",
      "429:Epoch: 8, Train_Loss: 1.1663930416107178, Test_Loss: 1.1044833660125732\n",
      "430:Epoch: 8, Train_Loss: 1.1121584177017212, Test_Loss: 1.1035305261611938 *\n",
      "431:Epoch: 8, Train_Loss: 1.1201879978179932, Test_Loss: 1.1569217443466187\n",
      "432:Epoch: 8, Train_Loss: 1.0887534618377686, Test_Loss: 1.107162594795227 *\n",
      "433:Epoch: 8, Train_Loss: 1.2271751165390015, Test_Loss: 1.1076586246490479\n",
      "434:Epoch: 8, Train_Loss: 1.4860069751739502, Test_Loss: 1.1012108325958252 *\n",
      "435:Epoch: 8, Train_Loss: 1.381298303604126, Test_Loss: 1.1877591609954834\n",
      "436:Epoch: 8, Train_Loss: 2.4911346435546875, Test_Loss: 1.1147865056991577 *\n",
      "437:Epoch: 8, Train_Loss: 1.7320836782455444, Test_Loss: 1.1663897037506104\n",
      "438:Epoch: 8, Train_Loss: 1.5279513597488403, Test_Loss: 1.121282935142517 *\n",
      "439:Epoch: 8, Train_Loss: 1.2058335542678833, Test_Loss: 1.1747852563858032\n",
      "440:Epoch: 8, Train_Loss: 1.109634518623352, Test_Loss: 1.0830901861190796 *\n",
      "441:Epoch: 8, Train_Loss: 1.2792291641235352, Test_Loss: 1.137053370475769\n",
      "442:Epoch: 8, Train_Loss: 1.6956040859222412, Test_Loss: 1.1799683570861816\n",
      "443:Epoch: 8, Train_Loss: 1.8515321016311646, Test_Loss: 1.0852307081222534 *\n",
      "444:Epoch: 8, Train_Loss: 1.1210341453552246, Test_Loss: 1.1928743124008179\n",
      "445:Epoch: 8, Train_Loss: 1.142152190208435, Test_Loss: 1.1657936573028564 *\n",
      "446:Epoch: 8, Train_Loss: 1.1752617359161377, Test_Loss: 1.152031660079956 *\n",
      "447:Epoch: 8, Train_Loss: 1.3495304584503174, Test_Loss: 1.1403956413269043 *\n",
      "448:Epoch: 8, Train_Loss: 1.293907642364502, Test_Loss: 1.1439675092697144\n",
      "449:Epoch: 8, Train_Loss: 1.4356977939605713, Test_Loss: 1.179549217224121\n",
      "450:Epoch: 8, Train_Loss: 1.2293869256973267, Test_Loss: 1.1404484510421753 *\n",
      "451:Epoch: 8, Train_Loss: 1.1691596508026123, Test_Loss: 1.1812901496887207\n",
      "452:Epoch: 8, Train_Loss: 1.1217998266220093, Test_Loss: 1.282110333442688\n",
      "453:Epoch: 8, Train_Loss: 1.1282267570495605, Test_Loss: 1.235938310623169 *\n",
      "454:Epoch: 8, Train_Loss: 1.1210315227508545, Test_Loss: 1.1175509691238403 *\n",
      "1:Epoch: 9, Train_Loss: 1.133386254310608, Test_Loss: 1.3067768812179565 *\n",
      "2:Epoch: 9, Train_Loss: 1.1421971321105957, Test_Loss: 1.16013765335083 *\n",
      "3:Epoch: 9, Train_Loss: 1.1848922967910767, Test_Loss: 1.1219836473464966 *\n",
      "4:Epoch: 9, Train_Loss: 16.70269775390625, Test_Loss: 1.101970911026001 *\n",
      "5:Epoch: 9, Train_Loss: 1.1917353868484497, Test_Loss: 1.1259346008300781\n",
      "6:Epoch: 9, Train_Loss: 2.9298925399780273, Test_Loss: 1.189236044883728\n",
      "7:Epoch: 9, Train_Loss: 1.8177670240402222, Test_Loss: 1.1712089776992798 *\n",
      "8:Epoch: 9, Train_Loss: 1.1245719194412231, Test_Loss: 1.234210729598999\n",
      "9:Epoch: 9, Train_Loss: 1.1786350011825562, Test_Loss: 1.3060922622680664\n",
      "10:Epoch: 9, Train_Loss: 5.845539093017578, Test_Loss: 1.343847632408142\n",
      "11:Epoch: 9, Train_Loss: 3.9842145442962646, Test_Loss: 1.2733534574508667 *\n",
      "12:Epoch: 9, Train_Loss: 1.1791226863861084, Test_Loss: 1.2744860649108887\n",
      "13:Epoch: 9, Train_Loss: 1.5710155963897705, Test_Loss: 1.2943803071975708\n",
      "14:Epoch: 9, Train_Loss: 4.935177326202393, Test_Loss: 1.679878830909729\n",
      "15:Epoch: 9, Train_Loss: 1.7032355070114136, Test_Loss: 1.7259430885314941\n",
      "16:Epoch: 9, Train_Loss: 1.1429481506347656, Test_Loss: 1.539905309677124 *\n",
      "17:Epoch: 9, Train_Loss: 1.079992413520813, Test_Loss: 1.5946471691131592\n",
      "18:Epoch: 9, Train_Loss: 1.0870931148529053, Test_Loss: 1.4832652807235718 *\n",
      "19:Epoch: 9, Train_Loss: 1.08738374710083, Test_Loss: 1.4223411083221436 *\n",
      "20:Epoch: 9, Train_Loss: 1.064630150794983, Test_Loss: 1.249802589416504 *\n",
      "21:Epoch: 9, Train_Loss: 1.0658330917358398, Test_Loss: 1.5683681964874268\n",
      "22:Epoch: 9, Train_Loss: 1.0624339580535889, Test_Loss: 1.1293100118637085 *\n",
      "23:Epoch: 9, Train_Loss: 1.071904182434082, Test_Loss: 1.2694790363311768\n",
      "24:Epoch: 9, Train_Loss: 1.1096737384796143, Test_Loss: 1.3342784643173218\n",
      "25:Epoch: 9, Train_Loss: 1.0801204442977905, Test_Loss: 1.290807843208313 *\n",
      "26:Epoch: 9, Train_Loss: 1.0917636156082153, Test_Loss: 1.242740273475647 *\n",
      "27:Epoch: 9, Train_Loss: 1.0943686962127686, Test_Loss: 1.1638368368148804 *\n",
      "28:Epoch: 9, Train_Loss: 1.1000821590423584, Test_Loss: 1.1499203443527222 *\n",
      "29:Epoch: 9, Train_Loss: 1.0657676458358765, Test_Loss: 7.68243408203125\n",
      "30:Epoch: 9, Train_Loss: 1.0703870058059692, Test_Loss: 1.423200011253357 *\n",
      "31:Epoch: 9, Train_Loss: 1.068927526473999, Test_Loss: 1.1803367137908936 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32:Epoch: 9, Train_Loss: 1.0646910667419434, Test_Loss: 1.130906581878662 *\n",
      "33:Epoch: 9, Train_Loss: 1.0641286373138428, Test_Loss: 1.1296138763427734 *\n",
      "34:Epoch: 9, Train_Loss: 1.0628492832183838, Test_Loss: 1.080264925956726 *\n",
      "35:Epoch: 9, Train_Loss: 1.0616105794906616, Test_Loss: 1.1516207456588745\n",
      "36:Epoch: 9, Train_Loss: 1.060420036315918, Test_Loss: 1.1712058782577515\n",
      "37:Epoch: 9, Train_Loss: 1.0646439790725708, Test_Loss: 1.070996880531311 *\n",
      "38:Epoch: 9, Train_Loss: 1.0607119798660278, Test_Loss: 1.0804601907730103\n",
      "39:Epoch: 9, Train_Loss: 1.0619194507598877, Test_Loss: 1.0874110460281372\n",
      "40:Epoch: 9, Train_Loss: 1.0720484256744385, Test_Loss: 1.1304470300674438\n",
      "41:Epoch: 9, Train_Loss: 1.0975617170333862, Test_Loss: 1.0759871006011963 *\n",
      "42:Epoch: 9, Train_Loss: 1.1421912908554077, Test_Loss: 1.090592861175537\n",
      "43:Epoch: 9, Train_Loss: 1.070226788520813, Test_Loss: 1.117587924003601\n",
      "44:Epoch: 9, Train_Loss: 1.06077241897583, Test_Loss: 1.06070077419281 *\n",
      "45:Epoch: 9, Train_Loss: 6.122711181640625, Test_Loss: 1.0574986934661865 *\n",
      "46:Epoch: 9, Train_Loss: 5.341230392456055, Test_Loss: 1.0637381076812744\n",
      "47:Epoch: 9, Train_Loss: 1.071439504623413, Test_Loss: 1.076236367225647\n",
      "48:Epoch: 9, Train_Loss: 1.112947940826416, Test_Loss: 1.073318600654602 *\n",
      "49:Epoch: 9, Train_Loss: 1.132328748703003, Test_Loss: 1.1070951223373413\n",
      "50:Epoch: 9, Train_Loss: 1.066227674484253, Test_Loss: 1.0954406261444092 *\n",
      "51:Epoch: 9, Train_Loss: 1.0587836503982544, Test_Loss: 1.1168149709701538\n",
      "52:Epoch: 9, Train_Loss: 1.0844061374664307, Test_Loss: 1.0860223770141602 *\n",
      "53:Epoch: 9, Train_Loss: 1.122010588645935, Test_Loss: 1.0683716535568237 *\n",
      "54:Epoch: 9, Train_Loss: 1.284330129623413, Test_Loss: 1.063272476196289 *\n",
      "55:Epoch: 9, Train_Loss: 1.2231261730194092, Test_Loss: 1.0665159225463867\n",
      "56:Epoch: 9, Train_Loss: 1.1068224906921387, Test_Loss: 1.0616145133972168 *\n",
      "57:Epoch: 9, Train_Loss: 1.1110706329345703, Test_Loss: 1.0628160238265991\n",
      "58:Epoch: 9, Train_Loss: 1.1750459671020508, Test_Loss: 1.0713398456573486\n",
      "59:Epoch: 9, Train_Loss: 1.1698471307754517, Test_Loss: 1.115138053894043\n",
      "60:Epoch: 9, Train_Loss: 1.194576382637024, Test_Loss: 2.6414613723754883\n",
      "61:Epoch: 9, Train_Loss: 1.158203363418579, Test_Loss: 5.154595375061035\n",
      "62:Epoch: 9, Train_Loss: 1.1208571195602417, Test_Loss: 1.0619813203811646 *\n",
      "63:Epoch: 9, Train_Loss: 1.05467689037323, Test_Loss: 1.0525598526000977 *\n",
      "64:Epoch: 9, Train_Loss: 1.1289132833480835, Test_Loss: 1.083386778831482\n",
      "65:Epoch: 9, Train_Loss: 1.0949139595031738, Test_Loss: 1.0854023694992065\n",
      "66:Epoch: 9, Train_Loss: 1.058432698249817, Test_Loss: 1.0947010517120361\n",
      "67:Epoch: 9, Train_Loss: 1.0505226850509644, Test_Loss: 1.081148624420166 *\n",
      "68:Epoch: 9, Train_Loss: 1.0518333911895752, Test_Loss: 1.197628140449524\n",
      "69:Epoch: 9, Train_Loss: 1.0643208026885986, Test_Loss: 1.0639241933822632 *\n",
      "70:Epoch: 9, Train_Loss: 5.481653213500977, Test_Loss: 1.0620529651641846 *\n",
      "71:Epoch: 9, Train_Loss: 2.4758248329162598, Test_Loss: 1.091838002204895\n",
      "72:Epoch: 9, Train_Loss: 1.054519534111023, Test_Loss: 1.0564852952957153 *\n",
      "73:Epoch: 9, Train_Loss: 1.063146710395813, Test_Loss: 1.0649112462997437\n",
      "74:Epoch: 9, Train_Loss: 1.0649827718734741, Test_Loss: 1.0975208282470703\n",
      "75:Epoch: 9, Train_Loss: 1.0583584308624268, Test_Loss: 1.1199512481689453\n",
      "76:Epoch: 9, Train_Loss: 1.051281452178955, Test_Loss: 1.1405104398727417\n",
      "77:Epoch: 9, Train_Loss: 1.0527677536010742, Test_Loss: 1.1993049383163452\n",
      "78:Epoch: 9, Train_Loss: 1.06996750831604, Test_Loss: 1.0730620622634888 *\n",
      "79:Epoch: 9, Train_Loss: 1.0783236026763916, Test_Loss: 1.0632601976394653 *\n",
      "80:Epoch: 9, Train_Loss: 1.0563335418701172, Test_Loss: 1.0511137247085571 *\n",
      "81:Epoch: 9, Train_Loss: 1.0502761602401733, Test_Loss: 1.0487756729125977 *\n",
      "82:Epoch: 9, Train_Loss: 1.049059271812439, Test_Loss: 1.0500739812850952\n",
      "83:Epoch: 9, Train_Loss: 1.0675586462020874, Test_Loss: 1.0509490966796875\n",
      "84:Epoch: 9, Train_Loss: 1.0459340810775757, Test_Loss: 1.0503700971603394 *\n",
      "85:Epoch: 9, Train_Loss: 1.045298457145691, Test_Loss: 1.0499178171157837 *\n",
      "86:Epoch: 9, Train_Loss: 1.0529435873031616, Test_Loss: 1.055311679840088\n",
      "87:Epoch: 9, Train_Loss: 1.1016708612442017, Test_Loss: 1.051133155822754 *\n",
      "88:Epoch: 9, Train_Loss: 1.0862135887145996, Test_Loss: 1.0539168119430542\n",
      "89:Epoch: 9, Train_Loss: 1.0427629947662354, Test_Loss: 1.0477268695831299 *\n",
      "90:Epoch: 9, Train_Loss: 1.0435595512390137, Test_Loss: 1.05362069606781\n",
      "91:Epoch: 9, Train_Loss: 1.0903325080871582, Test_Loss: 1.0968501567840576\n",
      "92:Epoch: 9, Train_Loss: 1.151540994644165, Test_Loss: 1.049906849861145 *\n",
      "93:Epoch: 9, Train_Loss: 1.1139777898788452, Test_Loss: 1.448442816734314\n",
      "94:Epoch: 9, Train_Loss: 1.121171474456787, Test_Loss: 1.518140435218811\n",
      "95:Epoch: 9, Train_Loss: 1.074439287185669, Test_Loss: 1.1824967861175537 *\n",
      "96:Epoch: 9, Train_Loss: 1.1183345317840576, Test_Loss: 1.0572009086608887 *\n",
      "97:Epoch: 9, Train_Loss: 1.0949676036834717, Test_Loss: 1.0583652257919312\n",
      "98:Epoch: 9, Train_Loss: 1.0856276750564575, Test_Loss: 1.0696680545806885\n",
      "99:Epoch: 9, Train_Loss: 1.1248586177825928, Test_Loss: 1.249472975730896\n",
      "100:Epoch: 9, Train_Loss: 1.0789779424667358, Test_Loss: 2.277341842651367\n",
      "Model saved at location ../Saver/model.ckpt at epoch 9\n",
      "101:Epoch: 9, Train_Loss: 1.0550119876861572, Test_Loss: 2.0241549015045166 *\n",
      "102:Epoch: 9, Train_Loss: 1.0417207479476929, Test_Loss: 1.0847117900848389 *\n",
      "103:Epoch: 9, Train_Loss: 1.0413117408752441, Test_Loss: 1.0931686162948608\n",
      "104:Epoch: 9, Train_Loss: 1.0399280786514282, Test_Loss: 1.040819764137268 *\n",
      "105:Epoch: 9, Train_Loss: 1.0393143892288208, Test_Loss: 1.0448538064956665\n",
      "106:Epoch: 9, Train_Loss: 1.0397926568984985, Test_Loss: 1.0454660654067993\n",
      "107:Epoch: 9, Train_Loss: 4.854588508605957, Test_Loss: 1.0645627975463867\n",
      "108:Epoch: 9, Train_Loss: 2.2037200927734375, Test_Loss: 1.1074182987213135\n",
      "109:Epoch: 9, Train_Loss: 1.036774754524231, Test_Loss: 1.0433757305145264 *\n",
      "110:Epoch: 9, Train_Loss: 1.0575814247131348, Test_Loss: 1.0530434846878052\n",
      "111:Epoch: 9, Train_Loss: 1.041980266571045, Test_Loss: 1.1518720388412476\n",
      "112:Epoch: 9, Train_Loss: 1.033238172531128, Test_Loss: 1.422297477722168\n",
      "113:Epoch: 9, Train_Loss: 1.0342681407928467, Test_Loss: 1.2530510425567627 *\n",
      "114:Epoch: 9, Train_Loss: 1.033255934715271, Test_Loss: 1.0470526218414307 *\n",
      "115:Epoch: 9, Train_Loss: 1.0327426195144653, Test_Loss: 1.041114330291748 *\n",
      "116:Epoch: 9, Train_Loss: 1.032085657119751, Test_Loss: 1.041084885597229 *\n",
      "117:Epoch: 9, Train_Loss: 1.0689575672149658, Test_Loss: 1.0412448644638062\n",
      "118:Epoch: 9, Train_Loss: 1.129412055015564, Test_Loss: 1.0409584045410156 *\n",
      "119:Epoch: 9, Train_Loss: 1.122713565826416, Test_Loss: 1.4833526611328125\n",
      "120:Epoch: 9, Train_Loss: 1.1308610439300537, Test_Loss: 6.054874897003174\n",
      "121:Epoch: 9, Train_Loss: 1.045335054397583, Test_Loss: 1.0808019638061523 *\n",
      "122:Epoch: 9, Train_Loss: 1.0575870275497437, Test_Loss: 1.0383851528167725 *\n",
      "123:Epoch: 9, Train_Loss: 1.2409948110580444, Test_Loss: 1.0334042310714722 *\n",
      "124:Epoch: 9, Train_Loss: 1.24239182472229, Test_Loss: 1.0380679368972778\n",
      "125:Epoch: 9, Train_Loss: 1.2445183992385864, Test_Loss: 1.0355671644210815 *\n",
      "126:Epoch: 9, Train_Loss: 1.0700929164886475, Test_Loss: 1.0322736501693726 *\n",
      "127:Epoch: 9, Train_Loss: 1.0283278226852417, Test_Loss: 1.033813714981079\n",
      "128:Epoch: 9, Train_Loss: 1.0285671949386597, Test_Loss: 1.0311554670333862 *\n",
      "129:Epoch: 9, Train_Loss: 1.0336151123046875, Test_Loss: 1.0311329364776611 *\n",
      "130:Epoch: 9, Train_Loss: 1.0375101566314697, Test_Loss: 1.031130313873291 *\n",
      "131:Epoch: 9, Train_Loss: 1.037152886390686, Test_Loss: 1.0346626043319702\n",
      "132:Epoch: 9, Train_Loss: 1.0336766242980957, Test_Loss: 1.048793911933899\n",
      "133:Epoch: 9, Train_Loss: 1.0270745754241943, Test_Loss: 1.0478980541229248 *\n",
      "134:Epoch: 9, Train_Loss: 1.0268793106079102, Test_Loss: 1.0394188165664673 *\n",
      "135:Epoch: 9, Train_Loss: 1.0372282266616821, Test_Loss: 1.0275704860687256 *\n",
      "136:Epoch: 9, Train_Loss: 1.135593295097351, Test_Loss: 1.0267524719238281 *\n",
      "137:Epoch: 9, Train_Loss: 1.2182698249816895, Test_Loss: 1.0273280143737793\n",
      "138:Epoch: 9, Train_Loss: 1.2024391889572144, Test_Loss: 1.027433156967163\n",
      "139:Epoch: 9, Train_Loss: 1.1071586608886719, Test_Loss: 1.0262951850891113 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140:Epoch: 9, Train_Loss: 1.1597294807434082, Test_Loss: 1.02762770652771\n",
      "141:Epoch: 9, Train_Loss: 1.1886813640594482, Test_Loss: 1.0259560346603394 *\n",
      "142:Epoch: 9, Train_Loss: 1.0329506397247314, Test_Loss: 1.0284537076950073\n",
      "143:Epoch: 9, Train_Loss: 1.1914278268814087, Test_Loss: 1.0271254777908325 *\n",
      "144:Epoch: 9, Train_Loss: 1.14459228515625, Test_Loss: 1.0263562202453613 *\n",
      "145:Epoch: 9, Train_Loss: 1.2817084789276123, Test_Loss: 1.0254347324371338 *\n",
      "146:Epoch: 9, Train_Loss: 1.0348553657531738, Test_Loss: 1.0253267288208008 *\n",
      "147:Epoch: 9, Train_Loss: 1.8270268440246582, Test_Loss: 1.0235501527786255 *\n",
      "148:Epoch: 9, Train_Loss: 3.4394583702087402, Test_Loss: 1.0243521928787231\n",
      "149:Epoch: 9, Train_Loss: 1.0610289573669434, Test_Loss: 1.0335204601287842\n",
      "150:Epoch: 9, Train_Loss: 1.077691674232483, Test_Loss: 1.0799791812896729\n",
      "151:Epoch: 9, Train_Loss: 1.0773786306381226, Test_Loss: 3.662269115447998\n",
      "152:Epoch: 9, Train_Loss: 1.0771689414978027, Test_Loss: 3.8824081420898438\n",
      "153:Epoch: 9, Train_Loss: 1.0211808681488037, Test_Loss: 1.0231486558914185 *\n",
      "154:Epoch: 9, Train_Loss: 1.02572500705719, Test_Loss: 1.0208046436309814 *\n",
      "155:Epoch: 9, Train_Loss: 1.1395657062530518, Test_Loss: 1.074198603630066\n",
      "156:Epoch: 9, Train_Loss: 1.1281934976577759, Test_Loss: 1.0759459733963013\n",
      "157:Epoch: 9, Train_Loss: 1.1292804479599, Test_Loss: 1.0673413276672363 *\n",
      "158:Epoch: 9, Train_Loss: 1.101016879081726, Test_Loss: 1.0707165002822876\n",
      "159:Epoch: 9, Train_Loss: 1.0965715646743774, Test_Loss: 1.1351559162139893\n",
      "160:Epoch: 9, Train_Loss: 1.0450687408447266, Test_Loss: 1.0213669538497925 *\n",
      "161:Epoch: 9, Train_Loss: 1.046912431716919, Test_Loss: 1.0428091287612915\n",
      "162:Epoch: 9, Train_Loss: 1.0342352390289307, Test_Loss: 1.0395734310150146 *\n",
      "163:Epoch: 9, Train_Loss: 1.049277424812317, Test_Loss: 1.0341696739196777 *\n",
      "164:Epoch: 9, Train_Loss: 1.0281254053115845, Test_Loss: 1.0250672101974487 *\n",
      "165:Epoch: 9, Train_Loss: 1.0162914991378784, Test_Loss: 1.108507752418518\n",
      "166:Epoch: 9, Train_Loss: 1.0606796741485596, Test_Loss: 1.1025296449661255 *\n",
      "167:Epoch: 9, Train_Loss: 1.0758534669876099, Test_Loss: 1.103865146636963\n",
      "168:Epoch: 9, Train_Loss: 1.0480459928512573, Test_Loss: 1.1294220685958862\n",
      "169:Epoch: 9, Train_Loss: 1.0149815082550049, Test_Loss: 1.0334820747375488 *\n",
      "170:Epoch: 9, Train_Loss: 1.0148454904556274, Test_Loss: 1.0469300746917725\n",
      "171:Epoch: 9, Train_Loss: 1.0140423774719238, Test_Loss: 1.0358221530914307 *\n",
      "172:Epoch: 9, Train_Loss: 1.0137393474578857, Test_Loss: 1.0327670574188232 *\n",
      "173:Epoch: 9, Train_Loss: 1.0139912366867065, Test_Loss: 1.0329670906066895\n",
      "174:Epoch: 9, Train_Loss: 1.0135928392410278, Test_Loss: 1.034171462059021\n",
      "175:Epoch: 9, Train_Loss: 1.013723611831665, Test_Loss: 1.03378164768219 *\n",
      "176:Epoch: 9, Train_Loss: 1.0132668018341064, Test_Loss: 1.0325533151626587 *\n",
      "177:Epoch: 9, Train_Loss: 1.0120482444763184, Test_Loss: 1.0451123714447021\n",
      "178:Epoch: 9, Train_Loss: 1.0122624635696411, Test_Loss: 1.0393052101135254 *\n",
      "179:Epoch: 9, Train_Loss: 1.0182397365570068, Test_Loss: 1.0358120203018188 *\n",
      "180:Epoch: 9, Train_Loss: 1.023747444152832, Test_Loss: 1.016821026802063 *\n",
      "181:Epoch: 9, Train_Loss: 1.0209643840789795, Test_Loss: 1.0491759777069092\n",
      "182:Epoch: 9, Train_Loss: 1.0330348014831543, Test_Loss: 1.0913883447647095\n",
      "183:Epoch: 9, Train_Loss: 1.0140485763549805, Test_Loss: 1.047037959098816 *\n",
      "184:Epoch: 9, Train_Loss: 1.0130013227462769, Test_Loss: 1.5875294208526611\n",
      "185:Epoch: 9, Train_Loss: 1.0098522901535034, Test_Loss: 1.5239624977111816 *\n",
      "186:Epoch: 9, Train_Loss: 1.0098130702972412, Test_Loss: 1.1577874422073364 *\n",
      "187:Epoch: 9, Train_Loss: 1.029242992401123, Test_Loss: 1.0461500883102417 *\n",
      "188:Epoch: 9, Train_Loss: 1.0200861692428589, Test_Loss: 1.0313304662704468 *\n",
      "189:Epoch: 9, Train_Loss: 1.0095024108886719, Test_Loss: 1.0464023351669312\n",
      "190:Epoch: 9, Train_Loss: 1.0080612897872925, Test_Loss: 1.3134640455245972\n",
      "191:Epoch: 9, Train_Loss: 1.0158991813659668, Test_Loss: 2.269102096557617\n",
      "192:Epoch: 9, Train_Loss: 1.0872588157653809, Test_Loss: 1.6742222309112549 *\n",
      "193:Epoch: 9, Train_Loss: 1.0400501489639282, Test_Loss: 1.0916744470596313 *\n",
      "194:Epoch: 9, Train_Loss: 1.0580195188522339, Test_Loss: 1.0449286699295044 *\n",
      "195:Epoch: 9, Train_Loss: 1.0066554546356201, Test_Loss: 1.0115768909454346 *\n",
      "196:Epoch: 9, Train_Loss: 1.039004921913147, Test_Loss: 1.007871150970459 *\n",
      "197:Epoch: 9, Train_Loss: 1.0493745803833008, Test_Loss: 1.014714241027832\n",
      "198:Epoch: 9, Train_Loss: 1.0072587728500366, Test_Loss: 1.0325202941894531\n",
      "199:Epoch: 9, Train_Loss: 1.0247312784194946, Test_Loss: 1.0656791925430298\n",
      "200:Epoch: 9, Train_Loss: 1.0358915328979492, Test_Loss: 1.0059326887130737 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 9\n",
      "201:Epoch: 9, Train_Loss: 1.0900604724884033, Test_Loss: 1.0575430393218994\n",
      "202:Epoch: 9, Train_Loss: 1.0987365245819092, Test_Loss: 1.1285490989685059\n",
      "203:Epoch: 9, Train_Loss: 1.0805598497390747, Test_Loss: 1.3856414556503296\n",
      "204:Epoch: 9, Train_Loss: 1.034668207168579, Test_Loss: 1.2272146940231323 *\n",
      "205:Epoch: 9, Train_Loss: 1.0079200267791748, Test_Loss: 1.0201894044876099 *\n",
      "206:Epoch: 9, Train_Loss: 1.0272014141082764, Test_Loss: 1.014107346534729 *\n",
      "207:Epoch: 9, Train_Loss: 1.0034946203231812, Test_Loss: 1.0137245655059814 *\n",
      "208:Epoch: 9, Train_Loss: 1.0084071159362793, Test_Loss: 1.0135897397994995 *\n",
      "209:Epoch: 9, Train_Loss: 1.0206414461135864, Test_Loss: 1.016658902168274\n",
      "210:Epoch: 9, Train_Loss: 1.0227787494659424, Test_Loss: 2.3863296508789062\n",
      "211:Epoch: 9, Train_Loss: 1.10313081741333, Test_Loss: 5.085411071777344\n",
      "212:Epoch: 9, Train_Loss: 1.0077348947525024, Test_Loss: 1.0158474445343018 *\n",
      "213:Epoch: 9, Train_Loss: 1.0792946815490723, Test_Loss: 1.0053240060806274 *\n",
      "214:Epoch: 9, Train_Loss: 1.004650592803955, Test_Loss: 1.0049388408660889 *\n",
      "215:Epoch: 9, Train_Loss: 1.0377869606018066, Test_Loss: 1.0106183290481567\n",
      "216:Epoch: 9, Train_Loss: 1.0114827156066895, Test_Loss: 1.0045899152755737 *\n",
      "217:Epoch: 9, Train_Loss: 1.2756000757217407, Test_Loss: 1.004077434539795 *\n",
      "218:Epoch: 9, Train_Loss: 1.058111548423767, Test_Loss: 1.0022978782653809 *\n",
      "219:Epoch: 9, Train_Loss: 1.0280934572219849, Test_Loss: 1.0012571811676025 *\n",
      "220:Epoch: 9, Train_Loss: 1.0071403980255127, Test_Loss: 1.002012014389038\n",
      "221:Epoch: 9, Train_Loss: 0.9984973073005676, Test_Loss: 0.9994101524353027 *\n",
      "222:Epoch: 9, Train_Loss: 0.9993330836296082, Test_Loss: 1.0040671825408936\n",
      "223:Epoch: 9, Train_Loss: 0.9978136420249939, Test_Loss: 1.0289851427078247\n",
      "224:Epoch: 9, Train_Loss: 1.006371021270752, Test_Loss: 1.0255231857299805 *\n",
      "225:Epoch: 9, Train_Loss: 1.0144684314727783, Test_Loss: 1.0059430599212646 *\n",
      "226:Epoch: 9, Train_Loss: 1.0231168270111084, Test_Loss: 0.997049868106842 *\n",
      "227:Epoch: 9, Train_Loss: 1.0128198862075806, Test_Loss: 0.9973166584968567\n",
      "228:Epoch: 9, Train_Loss: 1.0138155221939087, Test_Loss: 0.997083306312561 *\n",
      "229:Epoch: 9, Train_Loss: 1.0169262886047363, Test_Loss: 0.9966357946395874 *\n",
      "230:Epoch: 9, Train_Loss: 0.9986268281936646, Test_Loss: 0.9969044327735901\n",
      "231:Epoch: 9, Train_Loss: 0.994962751865387, Test_Loss: 0.9958401322364807 *\n",
      "232:Epoch: 9, Train_Loss: 1.0037789344787598, Test_Loss: 0.9952265024185181 *\n",
      "233:Epoch: 9, Train_Loss: 1.0234955549240112, Test_Loss: 0.9969518780708313\n",
      "234:Epoch: 9, Train_Loss: 1.0346806049346924, Test_Loss: 0.995050311088562 *\n",
      "235:Epoch: 9, Train_Loss: 0.9958701729774475, Test_Loss: 0.9952206015586853\n",
      "236:Epoch: 9, Train_Loss: 1.0300756692886353, Test_Loss: 0.9945154190063477 *\n",
      "237:Epoch: 9, Train_Loss: 1.0520620346069336, Test_Loss: 0.9943368434906006 *\n",
      "238:Epoch: 9, Train_Loss: 1.0515934228897095, Test_Loss: 0.9936510920524597 *\n",
      "239:Epoch: 9, Train_Loss: 0.9927161931991577, Test_Loss: 0.9935492277145386 *\n",
      "240:Epoch: 9, Train_Loss: 1.024829387664795, Test_Loss: 1.0239125490188599\n",
      "241:Epoch: 9, Train_Loss: 0.9974486231803894, Test_Loss: 1.0303728580474854\n",
      "242:Epoch: 9, Train_Loss: 1.0116344690322876, Test_Loss: 4.617509841918945\n",
      "243:Epoch: 9, Train_Loss: 0.9927263855934143, Test_Loss: 2.8435122966766357 *\n",
      "244:Epoch: 9, Train_Loss: 1.020491123199463, Test_Loss: 0.9924024939537048 *\n",
      "245:Epoch: 9, Train_Loss: 1.0612813234329224, Test_Loss: 0.997116208076477\n",
      "246:Epoch: 9, Train_Loss: 3.4542784690856934, Test_Loss: 1.052730917930603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247:Epoch: 9, Train_Loss: 3.9532084465026855, Test_Loss: 1.0615966320037842\n",
      "248:Epoch: 9, Train_Loss: 1.0077100992202759, Test_Loss: 1.0150880813598633 *\n",
      "249:Epoch: 9, Train_Loss: 0.9897913932800293, Test_Loss: 1.0606573820114136\n",
      "250:Epoch: 9, Train_Loss: 1.1003066301345825, Test_Loss: 1.0834815502166748\n",
      "251:Epoch: 9, Train_Loss: 1.126515507698059, Test_Loss: 0.9913754463195801 *\n",
      "252:Epoch: 9, Train_Loss: 1.010717511177063, Test_Loss: 1.0226365327835083\n",
      "253:Epoch: 9, Train_Loss: 0.98940509557724, Test_Loss: 1.006664514541626 *\n",
      "254:Epoch: 9, Train_Loss: 1.0313341617584229, Test_Loss: 1.0020966529846191 *\n",
      "255:Epoch: 9, Train_Loss: 1.0286401510238647, Test_Loss: 0.992068350315094 *\n",
      "256:Epoch: 9, Train_Loss: 1.0042121410369873, Test_Loss: 1.1061742305755615\n",
      "257:Epoch: 9, Train_Loss: 1.036325216293335, Test_Loss: 1.0459837913513184 *\n",
      "258:Epoch: 9, Train_Loss: 2.2283530235290527, Test_Loss: 1.0907312631607056\n",
      "259:Epoch: 9, Train_Loss: 2.4490623474121094, Test_Loss: 1.0705779790878296 *\n",
      "260:Epoch: 9, Train_Loss: 1.1651004552841187, Test_Loss: 1.0201176404953003 *\n",
      "261:Epoch: 9, Train_Loss: 1.0557552576065063, Test_Loss: 1.0122677087783813 *\n",
      "262:Epoch: 9, Train_Loss: 2.6575539112091064, Test_Loss: 1.0091055631637573 *\n",
      "263:Epoch: 9, Train_Loss: 2.84133243560791, Test_Loss: 1.0054057836532593 *\n",
      "264:Epoch: 9, Train_Loss: 1.0412099361419678, Test_Loss: 1.0071027278900146\n",
      "265:Epoch: 9, Train_Loss: 1.0285536050796509, Test_Loss: 1.008070945739746\n",
      "266:Epoch: 9, Train_Loss: 1.1942278146743774, Test_Loss: 1.0072218179702759 *\n",
      "267:Epoch: 9, Train_Loss: 2.6025304794311523, Test_Loss: 1.001548409461975 *\n",
      "268:Epoch: 9, Train_Loss: 2.256196975708008, Test_Loss: 1.0157334804534912\n",
      "269:Epoch: 9, Train_Loss: 0.9932166337966919, Test_Loss: 1.0089796781539917 *\n",
      "270:Epoch: 9, Train_Loss: 1.0058878660202026, Test_Loss: 1.0017304420471191 *\n",
      "271:Epoch: 9, Train_Loss: 1.060652732849121, Test_Loss: 0.9913070201873779 *\n",
      "272:Epoch: 9, Train_Loss: 1.6734479665756226, Test_Loss: 1.0282849073410034\n",
      "273:Epoch: 9, Train_Loss: 1.0016982555389404, Test_Loss: 1.0335438251495361\n",
      "274:Epoch: 9, Train_Loss: 1.0394117832183838, Test_Loss: 1.1029750108718872\n",
      "275:Epoch: 9, Train_Loss: 1.0217957496643066, Test_Loss: 1.5398969650268555\n",
      "276:Epoch: 9, Train_Loss: 1.1218352317810059, Test_Loss: 1.3987069129943848 *\n",
      "277:Epoch: 9, Train_Loss: 1.105034589767456, Test_Loss: 1.1053693294525146 *\n",
      "278:Epoch: 9, Train_Loss: 1.2353434562683105, Test_Loss: 1.0012208223342896 *\n",
      "279:Epoch: 9, Train_Loss: 1.1866366863250732, Test_Loss: 0.9995025992393494 *\n",
      "280:Epoch: 9, Train_Loss: 1.0311301946640015, Test_Loss: 1.0204975605010986\n",
      "281:Epoch: 9, Train_Loss: 1.155691385269165, Test_Loss: 1.3513022661209106\n",
      "282:Epoch: 9, Train_Loss: 1.1786249876022339, Test_Loss: 1.9691146612167358\n",
      "283:Epoch: 9, Train_Loss: 1.3362194299697876, Test_Loss: 1.3678853511810303 *\n",
      "284:Epoch: 9, Train_Loss: 1.1747424602508545, Test_Loss: 1.0638160705566406 *\n",
      "285:Epoch: 9, Train_Loss: 1.006260871887207, Test_Loss: 0.9937648177146912 *\n",
      "286:Epoch: 9, Train_Loss: 1.088004231452942, Test_Loss: 0.9887760877609253 *\n",
      "287:Epoch: 9, Train_Loss: 1.1359208822250366, Test_Loss: 0.9877600073814392 *\n",
      "288:Epoch: 9, Train_Loss: 0.9979466795921326, Test_Loss: 1.0118767023086548\n",
      "289:Epoch: 9, Train_Loss: 0.9883288145065308, Test_Loss: 0.9917148947715759 *\n",
      "290:Epoch: 9, Train_Loss: 0.9811087846755981, Test_Loss: 1.021733045578003\n",
      "291:Epoch: 9, Train_Loss: 0.9815125465393066, Test_Loss: 0.9825997352600098 *\n",
      "292:Epoch: 9, Train_Loss: 0.9844692349433899, Test_Loss: 1.070517659187317\n",
      "293:Epoch: 9, Train_Loss: 0.9846116900444031, Test_Loss: 1.1990220546722412\n",
      "294:Epoch: 9, Train_Loss: 1.0391250848770142, Test_Loss: 1.2686187028884888\n",
      "295:Epoch: 9, Train_Loss: 1.0118820667266846, Test_Loss: 1.2773178815841675\n",
      "296:Epoch: 9, Train_Loss: 1.080947756767273, Test_Loss: 0.9826114773750305 *\n",
      "297:Epoch: 9, Train_Loss: 1.0994195938110352, Test_Loss: 0.9778086543083191 *\n",
      "298:Epoch: 9, Train_Loss: 1.269136905670166, Test_Loss: 0.9790695905685425\n",
      "299:Epoch: 9, Train_Loss: 0.9894222617149353, Test_Loss: 0.979608416557312\n",
      "300:Epoch: 9, Train_Loss: 1.0109095573425293, Test_Loss: 1.0341464281082153\n",
      "Model saved at location ../Saver/model.ckpt at epoch 9\n",
      "301:Epoch: 9, Train_Loss: 1.3017324209213257, Test_Loss: 3.719430685043335\n",
      "302:Epoch: 9, Train_Loss: 1.4819319248199463, Test_Loss: 3.936605453491211\n",
      "303:Epoch: 9, Train_Loss: 1.1649818420410156, Test_Loss: 1.009335994720459 *\n",
      "304:Epoch: 9, Train_Loss: 1.0373194217681885, Test_Loss: 0.9793001413345337 *\n",
      "305:Epoch: 9, Train_Loss: 1.2860124111175537, Test_Loss: 1.0011125802993774\n",
      "306:Epoch: 9, Train_Loss: 1.4350533485412598, Test_Loss: 0.9811028838157654 *\n",
      "307:Epoch: 9, Train_Loss: 1.24332594871521, Test_Loss: 1.0138535499572754\n",
      "308:Epoch: 9, Train_Loss: 1.0064945220947266, Test_Loss: 1.0471378564834595\n",
      "309:Epoch: 9, Train_Loss: 1.0048656463623047, Test_Loss: 0.997200071811676 *\n",
      "310:Epoch: 9, Train_Loss: 1.0337464809417725, Test_Loss: 1.0037113428115845\n",
      "311:Epoch: 9, Train_Loss: 2.214381217956543, Test_Loss: 1.0165154933929443\n",
      "312:Epoch: 9, Train_Loss: 1.5628292560577393, Test_Loss: 1.0237215757369995\n",
      "313:Epoch: 9, Train_Loss: 0.981063187122345, Test_Loss: 1.0682467222213745\n",
      "314:Epoch: 9, Train_Loss: 0.9903507828712463, Test_Loss: 0.9838432669639587 *\n",
      "315:Epoch: 9, Train_Loss: 0.9734869003295898, Test_Loss: 1.01887845993042\n",
      "316:Epoch: 9, Train_Loss: 1.1632217168807983, Test_Loss: 1.0257052183151245\n",
      "317:Epoch: 9, Train_Loss: 1.2353001832962036, Test_Loss: 0.9731723666191101 *\n",
      "318:Epoch: 9, Train_Loss: 0.9802756905555725, Test_Loss: 0.9765665531158447\n",
      "319:Epoch: 9, Train_Loss: 1.2976304292678833, Test_Loss: 0.9821796417236328\n",
      "320:Epoch: 9, Train_Loss: 0.9830261468887329, Test_Loss: 0.9956753253936768\n",
      "321:Epoch: 9, Train_Loss: 0.9855743050575256, Test_Loss: 0.9757241010665894 *\n",
      "322:Epoch: 9, Train_Loss: 1.034193992614746, Test_Loss: 1.0049853324890137\n",
      "323:Epoch: 9, Train_Loss: 1.1825478076934814, Test_Loss: 1.0120532512664795\n",
      "324:Epoch: 9, Train_Loss: 1.0519535541534424, Test_Loss: 1.036583662033081\n",
      "325:Epoch: 9, Train_Loss: 1.0297398567199707, Test_Loss: 1.0189332962036133 *\n",
      "326:Epoch: 9, Train_Loss: 0.998345136642456, Test_Loss: 0.9982182383537292 *\n",
      "327:Epoch: 9, Train_Loss: 1.0961400270462036, Test_Loss: 0.9769559502601624 *\n",
      "328:Epoch: 9, Train_Loss: 1.0177099704742432, Test_Loss: 0.9996667504310608\n",
      "329:Epoch: 9, Train_Loss: 1.0056300163269043, Test_Loss: 0.9968687295913696 *\n",
      "330:Epoch: 9, Train_Loss: 0.9847498536109924, Test_Loss: 0.9916601181030273 *\n",
      "331:Epoch: 9, Train_Loss: 0.9918580651283264, Test_Loss: 1.0732389688491821\n",
      "332:Epoch: 9, Train_Loss: 1.02741539478302, Test_Loss: 1.0388143062591553 *\n",
      "333:Epoch: 9, Train_Loss: 1.287817358970642, Test_Loss: 5.964420795440674\n",
      "334:Epoch: 9, Train_Loss: 1.128746509552002, Test_Loss: 1.5048847198486328 *\n",
      "335:Epoch: 9, Train_Loss: 1.2904462814331055, Test_Loss: 1.0002529621124268 *\n",
      "336:Epoch: 9, Train_Loss: 1.238058090209961, Test_Loss: 1.0089187622070312\n",
      "337:Epoch: 9, Train_Loss: 1.1902694702148438, Test_Loss: 0.9819198250770569 *\n",
      "338:Epoch: 9, Train_Loss: 1.1017913818359375, Test_Loss: 0.9849510192871094\n",
      "339:Epoch: 9, Train_Loss: 1.0368133783340454, Test_Loss: 0.9810847640037537 *\n",
      "340:Epoch: 9, Train_Loss: 0.9955285787582397, Test_Loss: 1.0402803421020508\n",
      "341:Epoch: 9, Train_Loss: 0.9831774830818176, Test_Loss: 1.0003156661987305 *\n",
      "342:Epoch: 9, Train_Loss: 1.0253676176071167, Test_Loss: 0.9840104579925537 *\n",
      "343:Epoch: 9, Train_Loss: 1.343248963356018, Test_Loss: 0.990611732006073\n",
      "344:Epoch: 9, Train_Loss: 1.360992670059204, Test_Loss: 1.0896754264831543\n",
      "345:Epoch: 9, Train_Loss: 2.12984561920166, Test_Loss: 1.048109769821167 *\n",
      "346:Epoch: 9, Train_Loss: 1.7836397886276245, Test_Loss: 1.0215145349502563 *\n",
      "347:Epoch: 9, Train_Loss: 1.597168207168579, Test_Loss: 0.9863036274909973 *\n",
      "348:Epoch: 9, Train_Loss: 1.2347499132156372, Test_Loss: 1.0712953805923462\n",
      "349:Epoch: 9, Train_Loss: 0.9971174001693726, Test_Loss: 0.9752891659736633 *\n",
      "350:Epoch: 9, Train_Loss: 1.0663468837738037, Test_Loss: 1.0109814405441284\n",
      "351:Epoch: 9, Train_Loss: 1.3835439682006836, Test_Loss: 1.1444692611694336\n",
      "352:Epoch: 9, Train_Loss: 1.7953808307647705, Test_Loss: 1.0035518407821655 *\n",
      "353:Epoch: 9, Train_Loss: 1.011635661125183, Test_Loss: 1.0466327667236328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354:Epoch: 9, Train_Loss: 1.0344289541244507, Test_Loss: 1.049644947052002\n",
      "355:Epoch: 9, Train_Loss: 1.0618175268173218, Test_Loss: 1.060560941696167\n",
      "356:Epoch: 9, Train_Loss: 1.0446600914001465, Test_Loss: 1.030863881111145 *\n",
      "357:Epoch: 9, Train_Loss: 1.1824640035629272, Test_Loss: 1.03934907913208\n",
      "358:Epoch: 9, Train_Loss: 1.308758020401001, Test_Loss: 1.105434775352478\n",
      "359:Epoch: 9, Train_Loss: 1.1501224040985107, Test_Loss: 1.0443196296691895 *\n",
      "360:Epoch: 9, Train_Loss: 1.2301912307739258, Test_Loss: 1.1621556282043457\n",
      "361:Epoch: 9, Train_Loss: 1.0003762245178223, Test_Loss: 1.1533761024475098 *\n",
      "362:Epoch: 9, Train_Loss: 1.0058177709579468, Test_Loss: 1.248331904411316\n",
      "363:Epoch: 9, Train_Loss: 0.9975894689559937, Test_Loss: 1.0650748014450073 *\n",
      "364:Epoch: 9, Train_Loss: 1.0796105861663818, Test_Loss: 1.1407864093780518\n",
      "365:Epoch: 9, Train_Loss: 0.9990483522415161, Test_Loss: 1.0929878950119019 *\n",
      "366:Epoch: 9, Train_Loss: 1.0188323259353638, Test_Loss: 0.9895806312561035 *\n",
      "367:Epoch: 9, Train_Loss: 15.456415176391602, Test_Loss: 1.0011104345321655\n",
      "368:Epoch: 9, Train_Loss: 1.153342604637146, Test_Loss: 1.3445802927017212\n",
      "369:Epoch: 9, Train_Loss: 3.108802318572998, Test_Loss: 1.3691930770874023\n",
      "370:Epoch: 9, Train_Loss: 2.247488260269165, Test_Loss: 1.1759727001190186 *\n",
      "371:Epoch: 9, Train_Loss: 1.0215061902999878, Test_Loss: 1.1371277570724487 *\n",
      "372:Epoch: 9, Train_Loss: 1.1485049724578857, Test_Loss: 1.1976284980773926\n",
      "373:Epoch: 9, Train_Loss: 4.178637504577637, Test_Loss: 1.206296682357788\n",
      "374:Epoch: 9, Train_Loss: 7.129563808441162, Test_Loss: 1.139539122581482 *\n",
      "375:Epoch: 9, Train_Loss: 1.0975537300109863, Test_Loss: 1.0439153909683228 *\n",
      "376:Epoch: 9, Train_Loss: 1.025963306427002, Test_Loss: 0.9967643618583679 *\n",
      "377:Epoch: 9, Train_Loss: 6.738282680511475, Test_Loss: 1.1323899030685425\n",
      "378:Epoch: 9, Train_Loss: 1.0241672992706299, Test_Loss: 1.134802222251892\n",
      "379:Epoch: 9, Train_Loss: 0.9893847703933716, Test_Loss: 1.2556366920471191\n",
      "380:Epoch: 9, Train_Loss: 0.9619486331939697, Test_Loss: 1.0408790111541748 *\n",
      "381:Epoch: 9, Train_Loss: 0.9905334115028381, Test_Loss: 1.1881517171859741\n",
      "382:Epoch: 9, Train_Loss: 1.0132845640182495, Test_Loss: 1.0597668886184692 *\n",
      "383:Epoch: 9, Train_Loss: 1.013623833656311, Test_Loss: 1.0964107513427734\n",
      "384:Epoch: 9, Train_Loss: 0.9984038472175598, Test_Loss: 1.4077086448669434\n",
      "385:Epoch: 9, Train_Loss: 0.9696098566055298, Test_Loss: 1.132204294204712 *\n",
      "386:Epoch: 9, Train_Loss: 0.9604636430740356, Test_Loss: 1.4144017696380615\n",
      "387:Epoch: 9, Train_Loss: 0.9625658392906189, Test_Loss: 1.0126579999923706 *\n",
      "388:Epoch: 9, Train_Loss: 0.9982336759567261, Test_Loss: 1.0336097478866577\n",
      "389:Epoch: 9, Train_Loss: 1.0356357097625732, Test_Loss: 1.0436592102050781\n",
      "390:Epoch: 9, Train_Loss: 1.043522596359253, Test_Loss: 1.0460537672042847\n",
      "391:Epoch: 9, Train_Loss: 1.0493383407592773, Test_Loss: 1.0212808847427368 *\n",
      "392:Epoch: 9, Train_Loss: 0.9590732455253601, Test_Loss: 5.7796125411987305\n",
      "393:Epoch: 9, Train_Loss: 0.9972022771835327, Test_Loss: 3.086397171020508 *\n",
      "394:Epoch: 9, Train_Loss: 0.9668524265289307, Test_Loss: 1.2391290664672852 *\n",
      "395:Epoch: 9, Train_Loss: 1.014188289642334, Test_Loss: 1.2425827980041504\n",
      "396:Epoch: 9, Train_Loss: 0.9464918971061707, Test_Loss: 1.2665834426879883\n",
      "397:Epoch: 9, Train_Loss: 0.9443284869194031, Test_Loss: 1.0258698463439941 *\n",
      "398:Epoch: 9, Train_Loss: 0.9435091018676758, Test_Loss: 1.4076189994812012\n",
      "399:Epoch: 9, Train_Loss: 0.9437529444694519, Test_Loss: 1.4104702472686768\n",
      "400:Epoch: 9, Train_Loss: 0.9442887306213379, Test_Loss: 1.1752687692642212 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 9\n",
      "401:Epoch: 9, Train_Loss: 0.9426066279411316, Test_Loss: 1.1773053407669067\n",
      "402:Epoch: 9, Train_Loss: 0.9424413442611694, Test_Loss: 1.1763455867767334 *\n",
      "403:Epoch: 9, Train_Loss: 0.9504149556159973, Test_Loss: 1.2460685968399048\n",
      "404:Epoch: 9, Train_Loss: 0.9605647921562195, Test_Loss: 1.3031340837478638\n",
      "405:Epoch: 9, Train_Loss: 0.9851153492927551, Test_Loss: 1.11026930809021 *\n",
      "406:Epoch: 9, Train_Loss: 0.979384183883667, Test_Loss: 1.223325490951538\n",
      "407:Epoch: 9, Train_Loss: 1.0293188095092773, Test_Loss: 1.0999053716659546 *\n",
      "408:Epoch: 9, Train_Loss: 2.6222143173217773, Test_Loss: 0.9438065886497498 *\n",
      "409:Epoch: 9, Train_Loss: 7.187489032745361, Test_Loss: 1.0104515552520752\n",
      "410:Epoch: 9, Train_Loss: 1.0101600885391235, Test_Loss: 1.033798336982727\n",
      "411:Epoch: 9, Train_Loss: 1.1644208431243896, Test_Loss: 1.0747746229171753\n",
      "412:Epoch: 9, Train_Loss: 1.2343807220458984, Test_Loss: 1.066767930984497 *\n",
      "413:Epoch: 9, Train_Loss: 1.113173007965088, Test_Loss: 1.0993907451629639\n",
      "414:Epoch: 9, Train_Loss: 0.9874486327171326, Test_Loss: 1.1398319005966187\n",
      "415:Epoch: 9, Train_Loss: 1.0703513622283936, Test_Loss: 1.1035354137420654 *\n",
      "416:Epoch: 9, Train_Loss: 1.1481328010559082, Test_Loss: 1.0438741445541382 *\n",
      "417:Epoch: 9, Train_Loss: 1.1884403228759766, Test_Loss: 0.992321252822876 *\n",
      "418:Epoch: 9, Train_Loss: 1.0704164505004883, Test_Loss: 0.9628880620002747 *\n",
      "419:Epoch: 9, Train_Loss: 0.9977893233299255, Test_Loss: 0.9620164036750793 *\n",
      "420:Epoch: 9, Train_Loss: 0.955950915813446, Test_Loss: 0.9502649307250977 *\n",
      "421:Epoch: 9, Train_Loss: 1.0207206010818481, Test_Loss: 0.9491060376167297 *\n",
      "422:Epoch: 9, Train_Loss: 1.0142526626586914, Test_Loss: 1.0097497701644897\n",
      "423:Epoch: 9, Train_Loss: 1.0435709953308105, Test_Loss: 1.009985327720642\n",
      "424:Epoch: 9, Train_Loss: 1.0167698860168457, Test_Loss: 6.425366401672363\n",
      "425:Epoch: 9, Train_Loss: 1.0117918252944946, Test_Loss: 0.9820476174354553 *\n",
      "426:Epoch: 9, Train_Loss: 0.9380100965499878, Test_Loss: 0.9361231327056885 *\n",
      "427:Epoch: 9, Train_Loss: 1.007430911064148, Test_Loss: 0.9631194472312927\n",
      "428:Epoch: 9, Train_Loss: 1.0177011489868164, Test_Loss: 0.9843906164169312\n",
      "429:Epoch: 9, Train_Loss: 0.9568889737129211, Test_Loss: 0.9929452538490295\n",
      "430:Epoch: 9, Train_Loss: 0.9346491098403931, Test_Loss: 0.9388201236724854 *\n",
      "431:Epoch: 9, Train_Loss: 0.9336278438568115, Test_Loss: 1.0452982187271118\n",
      "432:Epoch: 9, Train_Loss: 0.9365620017051697, Test_Loss: 0.9881311058998108 *\n",
      "433:Epoch: 9, Train_Loss: 2.9726004600524902, Test_Loss: 0.9341675639152527 *\n",
      "434:Epoch: 9, Train_Loss: 4.915755748748779, Test_Loss: 0.9763906598091125\n",
      "435:Epoch: 9, Train_Loss: 0.9343588948249817, Test_Loss: 0.9479163885116577 *\n",
      "436:Epoch: 9, Train_Loss: 0.9407638907432556, Test_Loss: 0.9426683783531189 *\n",
      "437:Epoch: 9, Train_Loss: 0.9430421590805054, Test_Loss: 0.9626837968826294\n",
      "438:Epoch: 9, Train_Loss: 0.9392532110214233, Test_Loss: 1.0350255966186523\n",
      "439:Epoch: 9, Train_Loss: 0.9352819323539734, Test_Loss: 0.9768803715705872 *\n",
      "440:Epoch: 9, Train_Loss: 0.9334392547607422, Test_Loss: 1.0566824674606323\n",
      "441:Epoch: 9, Train_Loss: 0.9485862255096436, Test_Loss: 0.9859977960586548 *\n",
      "442:Epoch: 9, Train_Loss: 0.9832774996757507, Test_Loss: 0.9624346494674683 *\n",
      "443:Epoch: 9, Train_Loss: 0.9476796388626099, Test_Loss: 0.9463979005813599 *\n",
      "444:Epoch: 9, Train_Loss: 0.9332535862922668, Test_Loss: 0.9413580894470215 *\n",
      "445:Epoch: 9, Train_Loss: 0.931988000869751, Test_Loss: 0.9421452283859253\n",
      "446:Epoch: 9, Train_Loss: 0.9319359064102173, Test_Loss: 0.9431231021881104\n",
      "447:Epoch: 9, Train_Loss: 0.9459182024002075, Test_Loss: 0.9430997967720032 *\n",
      "448:Epoch: 9, Train_Loss: 0.9303337335586548, Test_Loss: 0.9421992897987366 *\n",
      "449:Epoch: 9, Train_Loss: 0.930623471736908, Test_Loss: 0.9431219100952148\n",
      "450:Epoch: 9, Train_Loss: 0.9713266491889954, Test_Loss: 0.9486181139945984\n",
      "451:Epoch: 9, Train_Loss: 0.9758528470993042, Test_Loss: 0.9513285756111145\n",
      "452:Epoch: 9, Train_Loss: 0.9310897588729858, Test_Loss: 0.9334049820899963 *\n",
      "453:Epoch: 9, Train_Loss: 0.9288021922111511, Test_Loss: 0.939747154712677\n",
      "454:Epoch: 9, Train_Loss: 0.9513569474220276, Test_Loss: 0.9900813102722168\n",
      "1:Epoch: 10, Train_Loss: 1.0474618673324585, Test_Loss: 0.9458481669425964 *\n",
      "2:Epoch: 10, Train_Loss: 1.0086897611618042, Test_Loss: 1.2337177991867065\n",
      "3:Epoch: 10, Train_Loss: 1.038571834564209, Test_Loss: 1.457127332687378\n",
      "4:Epoch: 10, Train_Loss: 0.9621543288230896, Test_Loss: 1.1454601287841797 *\n",
      "5:Epoch: 10, Train_Loss: 0.9875839352607727, Test_Loss: 0.9860056042671204 *\n",
      "6:Epoch: 10, Train_Loss: 0.9659467935562134, Test_Loss: 0.9448843598365784 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:Epoch: 10, Train_Loss: 0.9790241718292236, Test_Loss: 0.9386659264564514 *\n",
      "8:Epoch: 10, Train_Loss: 0.9460374712944031, Test_Loss: 1.050147294998169\n",
      "9:Epoch: 10, Train_Loss: 1.030003547668457, Test_Loss: 1.756319284439087\n",
      "10:Epoch: 10, Train_Loss: 0.9400729537010193, Test_Loss: 2.1145944595336914\n",
      "11:Epoch: 10, Train_Loss: 0.924777090549469, Test_Loss: 1.0317094326019287 *\n",
      "12:Epoch: 10, Train_Loss: 0.9250422716140747, Test_Loss: 1.0182944536209106 *\n",
      "13:Epoch: 10, Train_Loss: 0.9242621660232544, Test_Loss: 0.9239731431007385 *\n",
      "14:Epoch: 10, Train_Loss: 0.9234241843223572, Test_Loss: 0.930833101272583\n",
      "15:Epoch: 10, Train_Loss: 0.9239180684089661, Test_Loss: 0.9298307299613953 *\n",
      "16:Epoch: 10, Train_Loss: 3.003300428390503, Test_Loss: 0.9394381046295166\n",
      "17:Epoch: 10, Train_Loss: 3.6626908779144287, Test_Loss: 0.9611696004867554\n",
      "18:Epoch: 10, Train_Loss: 0.9230790138244629, Test_Loss: 0.9520279169082642 *\n",
      "19:Epoch: 10, Train_Loss: 0.9328338503837585, Test_Loss: 0.926517903804779 *\n",
      "20:Epoch: 10, Train_Loss: 0.9283314347267151, Test_Loss: 1.0356080532073975\n",
      "21:Epoch: 10, Train_Loss: 0.9208045601844788, Test_Loss: 1.3289966583251953\n",
      "22:Epoch: 10, Train_Loss: 0.920814573764801, Test_Loss: 1.0241197347640991 *\n",
      "23:Epoch: 10, Train_Loss: 0.9202523231506348, Test_Loss: 1.0807311534881592\n",
      "24:Epoch: 10, Train_Loss: 0.9195645451545715, Test_Loss: 0.9306696057319641 *\n",
      "25:Epoch: 10, Train_Loss: 0.919235110282898, Test_Loss: 0.9305145740509033 *\n",
      "26:Epoch: 10, Train_Loss: 0.9295742511749268, Test_Loss: 0.9308658838272095\n",
      "27:Epoch: 10, Train_Loss: 1.0055619478225708, Test_Loss: 0.9319130778312683\n",
      "28:Epoch: 10, Train_Loss: 0.9876611232757568, Test_Loss: 0.9508234262466431\n",
      "29:Epoch: 10, Train_Loss: 1.0128884315490723, Test_Loss: 5.915619373321533\n",
      "30:Epoch: 10, Train_Loss: 0.954228401184082, Test_Loss: 1.2822145223617554 *\n",
      "31:Epoch: 10, Train_Loss: 0.919708788394928, Test_Loss: 0.9288855791091919 *\n",
      "32:Epoch: 10, Train_Loss: 1.107173204421997, Test_Loss: 0.920967161655426 *\n",
      "33:Epoch: 10, Train_Loss: 1.141704797744751, Test_Loss: 0.9235135912895203\n",
      "34:Epoch: 10, Train_Loss: 1.1316959857940674, Test_Loss: 0.9288231730461121\n",
      "35:Epoch: 10, Train_Loss: 1.0109063386917114, Test_Loss: 0.9202880263328552 *\n",
      "36:Epoch: 10, Train_Loss: 0.9165903925895691, Test_Loss: 0.921050488948822\n",
      "37:Epoch: 10, Train_Loss: 0.9162509441375732, Test_Loss: 0.9173324704170227 *\n",
      "38:Epoch: 10, Train_Loss: 0.918938934803009, Test_Loss: 0.9174197316169739\n",
      "39:Epoch: 10, Train_Loss: 0.9247903823852539, Test_Loss: 0.9187770485877991\n",
      "40:Epoch: 10, Train_Loss: 0.9314214587211609, Test_Loss: 0.9228876233100891\n",
      "41:Epoch: 10, Train_Loss: 0.92331862449646, Test_Loss: 0.9258638620376587\n",
      "42:Epoch: 10, Train_Loss: 0.9148021936416626, Test_Loss: 0.9449875950813293\n",
      "43:Epoch: 10, Train_Loss: 0.9140447974205017, Test_Loss: 0.9327248334884644 *\n",
      "44:Epoch: 10, Train_Loss: 0.9254577159881592, Test_Loss: 0.9148102402687073 *\n",
      "45:Epoch: 10, Train_Loss: 0.9855027198791504, Test_Loss: 0.9150199890136719\n",
      "46:Epoch: 10, Train_Loss: 1.123619556427002, Test_Loss: 0.914759635925293 *\n",
      "47:Epoch: 10, Train_Loss: 1.1128724813461304, Test_Loss: 0.914638102054596 *\n",
      "48:Epoch: 10, Train_Loss: 1.0475013256072998, Test_Loss: 0.9129074215888977 *\n",
      "49:Epoch: 10, Train_Loss: 1.0118942260742188, Test_Loss: 0.9156489968299866\n",
      "50:Epoch: 10, Train_Loss: 1.0622215270996094, Test_Loss: 0.9135390520095825 *\n",
      "51:Epoch: 10, Train_Loss: 0.9615084528923035, Test_Loss: 0.9165536165237427\n",
      "52:Epoch: 10, Train_Loss: 1.0581891536712646, Test_Loss: 0.9161482453346252 *\n",
      "53:Epoch: 10, Train_Loss: 1.036815881729126, Test_Loss: 0.9147090911865234 *\n",
      "54:Epoch: 10, Train_Loss: 1.1997501850128174, Test_Loss: 0.9129801988601685 *\n",
      "55:Epoch: 10, Train_Loss: 0.9213859438896179, Test_Loss: 0.9125491976737976 *\n",
      "56:Epoch: 10, Train_Loss: 1.0154869556427002, Test_Loss: 0.9113772511482239 *\n",
      "57:Epoch: 10, Train_Loss: 3.8515892028808594, Test_Loss: 0.9116813540458679\n",
      "58:Epoch: 10, Train_Loss: 1.0646828413009644, Test_Loss: 0.9152929186820984\n",
      "59:Epoch: 10, Train_Loss: 0.9640552401542664, Test_Loss: 0.9709036350250244\n",
      "60:Epoch: 10, Train_Loss: 0.9703390002250671, Test_Loss: 1.708362102508545\n",
      "61:Epoch: 10, Train_Loss: 0.9704408049583435, Test_Loss: 5.5726823806762695\n",
      "62:Epoch: 10, Train_Loss: 0.9162198901176453, Test_Loss: 0.9155443906784058 *\n",
      "63:Epoch: 10, Train_Loss: 0.911838710308075, Test_Loss: 0.9091432094573975 *\n",
      "64:Epoch: 10, Train_Loss: 0.9890545606613159, Test_Loss: 0.9498686790466309\n",
      "65:Epoch: 10, Train_Loss: 1.0283068418502808, Test_Loss: 0.9652798771858215\n",
      "66:Epoch: 10, Train_Loss: 1.0135388374328613, Test_Loss: 0.971975564956665\n",
      "67:Epoch: 10, Train_Loss: 0.9864886999130249, Test_Loss: 0.9181511402130127 *\n",
      "68:Epoch: 10, Train_Loss: 0.9931190013885498, Test_Loss: 1.0335345268249512\n",
      "69:Epoch: 10, Train_Loss: 0.9397525191307068, Test_Loss: 0.9337525963783264 *\n",
      "70:Epoch: 10, Train_Loss: 0.9448596239089966, Test_Loss: 0.9111076593399048 *\n",
      "71:Epoch: 10, Train_Loss: 0.9147553443908691, Test_Loss: 0.9411290884017944\n",
      "72:Epoch: 10, Train_Loss: 0.9372987151145935, Test_Loss: 0.9265448451042175 *\n",
      "73:Epoch: 10, Train_Loss: 0.9218286275863647, Test_Loss: 0.9118261337280273 *\n",
      "74:Epoch: 10, Train_Loss: 0.9054991006851196, Test_Loss: 0.9670198559761047\n",
      "75:Epoch: 10, Train_Loss: 0.9317231178283691, Test_Loss: 1.02085280418396\n",
      "76:Epoch: 10, Train_Loss: 0.9609869122505188, Test_Loss: 0.963428258895874 *\n",
      "77:Epoch: 10, Train_Loss: 0.9441916942596436, Test_Loss: 0.997795820236206\n",
      "78:Epoch: 10, Train_Loss: 0.9050441980361938, Test_Loss: 0.9333253502845764 *\n",
      "79:Epoch: 10, Train_Loss: 0.9040054082870483, Test_Loss: 0.9505125284194946\n",
      "80:Epoch: 10, Train_Loss: 0.9033430814743042, Test_Loss: 0.9270703196525574 *\n",
      "81:Epoch: 10, Train_Loss: 0.9028882384300232, Test_Loss: 0.9231553673744202 *\n",
      "82:Epoch: 10, Train_Loss: 0.9037436842918396, Test_Loss: 0.9270882606506348\n",
      "83:Epoch: 10, Train_Loss: 0.9039818644523621, Test_Loss: 0.9312432408332825\n",
      "84:Epoch: 10, Train_Loss: 0.9044128656387329, Test_Loss: 0.9313411116600037\n",
      "85:Epoch: 10, Train_Loss: 0.9027411937713623, Test_Loss: 0.9267430305480957 *\n",
      "86:Epoch: 10, Train_Loss: 0.9020296335220337, Test_Loss: 0.9332054853439331\n",
      "87:Epoch: 10, Train_Loss: 0.9028510451316833, Test_Loss: 0.9274805188179016 *\n",
      "88:Epoch: 10, Train_Loss: 0.9081422090530396, Test_Loss: 0.9365811347961426\n",
      "89:Epoch: 10, Train_Loss: 0.9159420728683472, Test_Loss: 0.9053876399993896 *\n",
      "90:Epoch: 10, Train_Loss: 0.9193784594535828, Test_Loss: 0.9278094172477722\n",
      "91:Epoch: 10, Train_Loss: 0.9228758215904236, Test_Loss: 0.9946680665016174\n",
      "92:Epoch: 10, Train_Loss: 0.9112991094589233, Test_Loss: 0.9143972396850586 *\n",
      "93:Epoch: 10, Train_Loss: 0.9122161865234375, Test_Loss: 1.3469250202178955\n",
      "94:Epoch: 10, Train_Loss: 0.9016541838645935, Test_Loss: 1.5038191080093384\n",
      "95:Epoch: 10, Train_Loss: 0.9014676213264465, Test_Loss: 1.1076558828353882 *\n",
      "96:Epoch: 10, Train_Loss: 0.9218181371688843, Test_Loss: 0.9468997120857239 *\n",
      "97:Epoch: 10, Train_Loss: 0.925162136554718, Test_Loss: 0.93793123960495 *\n",
      "98:Epoch: 10, Train_Loss: 0.9000180959701538, Test_Loss: 0.9096148610115051 *\n",
      "99:Epoch: 10, Train_Loss: 0.902153730392456, Test_Loss: 1.015039324760437\n",
      "100:Epoch: 10, Train_Loss: 0.9010165333747864, Test_Loss: 1.888540506362915\n",
      "Model saved at location ../Saver/model.ckpt at epoch 10\n",
      "101:Epoch: 10, Train_Loss: 0.9514397382736206, Test_Loss: 1.89164137840271\n",
      "102:Epoch: 10, Train_Loss: 0.9326435327529907, Test_Loss: 0.9519127607345581 *\n",
      "103:Epoch: 10, Train_Loss: 0.9499122500419617, Test_Loss: 1.002382755279541\n",
      "104:Epoch: 10, Train_Loss: 0.9061742424964905, Test_Loss: 0.898330569267273 *\n",
      "105:Epoch: 10, Train_Loss: 0.9047431349754333, Test_Loss: 0.9001288414001465\n",
      "106:Epoch: 10, Train_Loss: 0.9686276316642761, Test_Loss: 0.9046366214752197\n",
      "107:Epoch: 10, Train_Loss: 0.8979257345199585, Test_Loss: 0.9090152978897095\n",
      "108:Epoch: 10, Train_Loss: 0.9088725447654724, Test_Loss: 0.9416983723640442\n",
      "109:Epoch: 10, Train_Loss: 0.9209319353103638, Test_Loss: 0.90993332862854 *\n",
      "110:Epoch: 10, Train_Loss: 0.9367982745170593, Test_Loss: 0.9040635228157043 *\n",
      "111:Epoch: 10, Train_Loss: 1.0113108158111572, Test_Loss: 1.0182440280914307\n",
      "112:Epoch: 10, Train_Loss: 0.9666553139686584, Test_Loss: 1.3133835792541504\n",
      "113:Epoch: 10, Train_Loss: 0.9269437789916992, Test_Loss: 1.1138176918029785 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114:Epoch: 10, Train_Loss: 0.8997446894645691, Test_Loss: 0.9397085309028625 *\n",
      "115:Epoch: 10, Train_Loss: 0.9189603328704834, Test_Loss: 0.9064934253692627 *\n",
      "116:Epoch: 10, Train_Loss: 0.8946840763092041, Test_Loss: 0.9060999155044556 *\n",
      "117:Epoch: 10, Train_Loss: 0.8979346752166748, Test_Loss: 0.9060969352722168 *\n",
      "118:Epoch: 10, Train_Loss: 0.9062992930412292, Test_Loss: 0.9059506058692932 *\n",
      "119:Epoch: 10, Train_Loss: 0.9149002432823181, Test_Loss: 1.0732027292251587\n",
      "120:Epoch: 10, Train_Loss: 0.9596103429794312, Test_Loss: 6.021023273468018\n",
      "121:Epoch: 10, Train_Loss: 0.9487191438674927, Test_Loss: 0.9875052571296692 *\n",
      "122:Epoch: 10, Train_Loss: 0.9371362924575806, Test_Loss: 0.9014637470245361 *\n",
      "123:Epoch: 10, Train_Loss: 0.9210605025291443, Test_Loss: 0.8950261473655701 *\n",
      "124:Epoch: 10, Train_Loss: 0.9278242588043213, Test_Loss: 0.9006125330924988\n",
      "125:Epoch: 10, Train_Loss: 0.9056596159934998, Test_Loss: 0.8994409441947937 *\n",
      "126:Epoch: 10, Train_Loss: 1.048171043395996, Test_Loss: 0.8950210809707642 *\n",
      "127:Epoch: 10, Train_Loss: 1.0755908489227295, Test_Loss: 0.892978310585022 *\n",
      "128:Epoch: 10, Train_Loss: 0.895232617855072, Test_Loss: 0.8913859724998474 *\n",
      "129:Epoch: 10, Train_Loss: 0.9305084943771362, Test_Loss: 0.8907848596572876 *\n",
      "130:Epoch: 10, Train_Loss: 0.8892709612846375, Test_Loss: 0.8914151787757874\n",
      "131:Epoch: 10, Train_Loss: 0.8888348937034607, Test_Loss: 0.8936559557914734\n",
      "132:Epoch: 10, Train_Loss: 0.889279842376709, Test_Loss: 0.9123222231864929\n",
      "133:Epoch: 10, Train_Loss: 0.8907965421676636, Test_Loss: 0.9212837815284729\n",
      "134:Epoch: 10, Train_Loss: 0.903731107711792, Test_Loss: 0.9062774777412415 *\n",
      "135:Epoch: 10, Train_Loss: 0.9116950035095215, Test_Loss: 0.887892484664917 *\n",
      "136:Epoch: 10, Train_Loss: 0.9001696109771729, Test_Loss: 0.8881325721740723\n",
      "137:Epoch: 10, Train_Loss: 0.9011664986610413, Test_Loss: 0.8875163197517395 *\n",
      "138:Epoch: 10, Train_Loss: 0.9098063707351685, Test_Loss: 0.8877622485160828\n",
      "139:Epoch: 10, Train_Loss: 0.8871992826461792, Test_Loss: 0.8863840699195862 *\n",
      "140:Epoch: 10, Train_Loss: 0.8884339332580566, Test_Loss: 0.88763827085495\n",
      "141:Epoch: 10, Train_Loss: 0.8859366178512573, Test_Loss: 0.8859410285949707 *\n",
      "142:Epoch: 10, Train_Loss: 0.9208504557609558, Test_Loss: 0.8872205018997192\n",
      "143:Epoch: 10, Train_Loss: 0.9229705333709717, Test_Loss: 0.886577308177948 *\n",
      "144:Epoch: 10, Train_Loss: 0.898842453956604, Test_Loss: 0.8862410187721252 *\n",
      "145:Epoch: 10, Train_Loss: 0.9038017392158508, Test_Loss: 0.8854334950447083 *\n",
      "146:Epoch: 10, Train_Loss: 0.949253499507904, Test_Loss: 0.8849245309829712 *\n",
      "147:Epoch: 10, Train_Loss: 0.9406003355979919, Test_Loss: 0.8841077089309692 *\n",
      "148:Epoch: 10, Train_Loss: 0.8961624503135681, Test_Loss: 0.8843328952789307\n",
      "149:Epoch: 10, Train_Loss: 0.9061822295188904, Test_Loss: 0.8891782164573669\n",
      "150:Epoch: 10, Train_Loss: 0.9000649452209473, Test_Loss: 0.9442713856697083\n",
      "151:Epoch: 10, Train_Loss: 0.8952361941337585, Test_Loss: 2.880873680114746\n",
      "152:Epoch: 10, Train_Loss: 0.8906117081642151, Test_Loss: 4.307851314544678\n",
      "153:Epoch: 10, Train_Loss: 0.9145779609680176, Test_Loss: 0.8871740102767944 *\n",
      "154:Epoch: 10, Train_Loss: 0.9495439529418945, Test_Loss: 0.882365882396698 *\n",
      "155:Epoch: 10, Train_Loss: 3.137131452560425, Test_Loss: 0.9363797307014465\n",
      "156:Epoch: 10, Train_Loss: 4.056068420410156, Test_Loss: 0.9397756457328796\n",
      "157:Epoch: 10, Train_Loss: 0.8957160115242004, Test_Loss: 0.9398835301399231\n",
      "158:Epoch: 10, Train_Loss: 0.8845314383506775, Test_Loss: 0.9158788919448853 *\n",
      "159:Epoch: 10, Train_Loss: 0.9459581971168518, Test_Loss: 1.0025649070739746\n",
      "160:Epoch: 10, Train_Loss: 1.0494335889816284, Test_Loss: 0.8867849707603455 *\n",
      "161:Epoch: 10, Train_Loss: 0.9008923768997192, Test_Loss: 0.8975043892860413\n",
      "162:Epoch: 10, Train_Loss: 0.8837347626686096, Test_Loss: 0.9055273532867432\n",
      "163:Epoch: 10, Train_Loss: 0.8923195600509644, Test_Loss: 0.8992340564727783 *\n",
      "164:Epoch: 10, Train_Loss: 0.9367251396179199, Test_Loss: 0.8877970576286316 *\n",
      "165:Epoch: 10, Train_Loss: 0.8888983726501465, Test_Loss: 0.9581230282783508\n",
      "166:Epoch: 10, Train_Loss: 0.8922749161720276, Test_Loss: 0.9744051694869995\n",
      "167:Epoch: 10, Train_Loss: 1.829209327697754, Test_Loss: 0.9510303735733032 *\n",
      "168:Epoch: 10, Train_Loss: 2.2375295162200928, Test_Loss: 0.9716355800628662\n",
      "169:Epoch: 10, Train_Loss: 1.3693393468856812, Test_Loss: 0.9003428816795349 *\n",
      "170:Epoch: 10, Train_Loss: 0.9757120609283447, Test_Loss: 0.9214488863945007\n",
      "171:Epoch: 10, Train_Loss: 1.9627386331558228, Test_Loss: 0.9008863568305969 *\n",
      "172:Epoch: 10, Train_Loss: 3.1092727184295654, Test_Loss: 0.8973599672317505 *\n",
      "173:Epoch: 10, Train_Loss: 1.1170884370803833, Test_Loss: 0.8999032378196716\n",
      "174:Epoch: 10, Train_Loss: 0.9227763414382935, Test_Loss: 0.9080982208251953\n",
      "175:Epoch: 10, Train_Loss: 0.9124402403831482, Test_Loss: 0.902198314666748 *\n",
      "176:Epoch: 10, Train_Loss: 2.3280513286590576, Test_Loss: 0.8981954455375671 *\n",
      "177:Epoch: 10, Train_Loss: 2.333662271499634, Test_Loss: 0.9041984677314758\n",
      "178:Epoch: 10, Train_Loss: 0.9252713918685913, Test_Loss: 0.8968203067779541 *\n",
      "179:Epoch: 10, Train_Loss: 0.8951754570007324, Test_Loss: 0.8952499628067017 *\n",
      "180:Epoch: 10, Train_Loss: 0.8780144453048706, Test_Loss: 0.8789401054382324 *\n",
      "181:Epoch: 10, Train_Loss: 1.6083934307098389, Test_Loss: 0.8929978013038635\n",
      "182:Epoch: 10, Train_Loss: 1.0039863586425781, Test_Loss: 0.9357426166534424\n",
      "183:Epoch: 10, Train_Loss: 0.9426344633102417, Test_Loss: 0.8882713913917542 *\n",
      "184:Epoch: 10, Train_Loss: 0.9112045764923096, Test_Loss: 1.3604289293289185\n",
      "185:Epoch: 10, Train_Loss: 1.0638576745986938, Test_Loss: 1.3365627527236938 *\n",
      "186:Epoch: 10, Train_Loss: 1.0245366096496582, Test_Loss: 1.043475866317749 *\n",
      "187:Epoch: 10, Train_Loss: 1.0380926132202148, Test_Loss: 0.9053546190261841 *\n",
      "188:Epoch: 10, Train_Loss: 1.147637128829956, Test_Loss: 0.9008948802947998 *\n",
      "189:Epoch: 10, Train_Loss: 0.9124789237976074, Test_Loss: 0.8874387145042419 *\n",
      "190:Epoch: 10, Train_Loss: 0.9866265654563904, Test_Loss: 1.0486876964569092\n",
      "191:Epoch: 10, Train_Loss: 1.1245746612548828, Test_Loss: 1.8950409889221191\n",
      "192:Epoch: 10, Train_Loss: 1.2784146070480347, Test_Loss: 1.5256911516189575 *\n",
      "193:Epoch: 10, Train_Loss: 1.216996192932129, Test_Loss: 0.9451699256896973 *\n",
      "194:Epoch: 10, Train_Loss: 0.9104126691818237, Test_Loss: 0.9248305559158325 *\n",
      "195:Epoch: 10, Train_Loss: 0.9618819952011108, Test_Loss: 0.8755629062652588 *\n",
      "196:Epoch: 10, Train_Loss: 0.9856274127960205, Test_Loss: 0.8743378520011902 *\n",
      "197:Epoch: 10, Train_Loss: 0.8953377604484558, Test_Loss: 0.882803738117218\n",
      "198:Epoch: 10, Train_Loss: 0.8768982887268066, Test_Loss: 0.8802124857902527 *\n",
      "199:Epoch: 10, Train_Loss: 0.8706808090209961, Test_Loss: 0.9305115938186646\n",
      "200:Epoch: 10, Train_Loss: 0.869499683380127, Test_Loss: 0.8812541365623474 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 10\n",
      "201:Epoch: 10, Train_Loss: 0.8704717755317688, Test_Loss: 0.9051081538200378\n",
      "202:Epoch: 10, Train_Loss: 0.8734303712844849, Test_Loss: 0.9982423782348633\n",
      "203:Epoch: 10, Train_Loss: 0.9212145209312439, Test_Loss: 1.2409056425094604\n",
      "204:Epoch: 10, Train_Loss: 0.9063652157783508, Test_Loss: 1.1903384923934937 *\n",
      "205:Epoch: 10, Train_Loss: 0.9395061731338501, Test_Loss: 0.9032652378082275 *\n",
      "206:Epoch: 10, Train_Loss: 0.9750307202339172, Test_Loss: 0.8821314573287964 *\n",
      "207:Epoch: 10, Train_Loss: 1.271346092224121, Test_Loss: 0.8811883330345154 *\n",
      "208:Epoch: 10, Train_Loss: 0.871902346611023, Test_Loss: 0.8797760605812073 *\n",
      "209:Epoch: 10, Train_Loss: 0.9045721888542175, Test_Loss: 0.8949871063232422\n",
      "210:Epoch: 10, Train_Loss: 1.0197199583053589, Test_Loss: 1.5999071598052979\n",
      "211:Epoch: 10, Train_Loss: 1.3226877450942993, Test_Loss: 5.5735554695129395\n",
      "212:Epoch: 10, Train_Loss: 1.105628252029419, Test_Loss: 0.9014075994491577 *\n",
      "213:Epoch: 10, Train_Loss: 0.8754650354385376, Test_Loss: 0.8762446045875549 *\n",
      "214:Epoch: 10, Train_Loss: 1.0177881717681885, Test_Loss: 0.8720160126686096 *\n",
      "215:Epoch: 10, Train_Loss: 1.3472115993499756, Test_Loss: 0.8708712458610535 *\n",
      "216:Epoch: 10, Train_Loss: 1.2301650047302246, Test_Loss: 0.8775039315223694\n",
      "217:Epoch: 10, Train_Loss: 0.9097813963890076, Test_Loss: 0.8851848840713501\n",
      "218:Epoch: 10, Train_Loss: 0.8758832812309265, Test_Loss: 0.9002842903137207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219:Epoch: 10, Train_Loss: 0.876010537147522, Test_Loss: 0.8715671896934509 *\n",
      "220:Epoch: 10, Train_Loss: 1.8783316612243652, Test_Loss: 0.8914535045623779\n",
      "221:Epoch: 10, Train_Loss: 1.718061923980713, Test_Loss: 0.8886590600013733 *\n",
      "222:Epoch: 10, Train_Loss: 0.8750883936882019, Test_Loss: 0.9220729470252991\n",
      "223:Epoch: 10, Train_Loss: 0.8896387815475464, Test_Loss: 0.8665550351142883 *\n",
      "224:Epoch: 10, Train_Loss: 0.8637571334838867, Test_Loss: 0.8688023686408997\n",
      "225:Epoch: 10, Train_Loss: 0.9141577482223511, Test_Loss: 0.9008798599243164\n",
      "226:Epoch: 10, Train_Loss: 1.1996577978134155, Test_Loss: 0.8670410513877869 *\n",
      "227:Epoch: 10, Train_Loss: 0.8751108646392822, Test_Loss: 0.869624674320221\n",
      "228:Epoch: 10, Train_Loss: 1.1004678010940552, Test_Loss: 0.8706552982330322\n",
      "229:Epoch: 10, Train_Loss: 0.9252035617828369, Test_Loss: 0.8970425724983215\n",
      "230:Epoch: 10, Train_Loss: 0.8833423256874084, Test_Loss: 0.8672043681144714 *\n",
      "231:Epoch: 10, Train_Loss: 0.9134359955787659, Test_Loss: 0.8893765807151794\n",
      "232:Epoch: 10, Train_Loss: 1.0385375022888184, Test_Loss: 0.8858546614646912 *\n",
      "233:Epoch: 10, Train_Loss: 0.9667107462882996, Test_Loss: 0.91811603307724\n",
      "234:Epoch: 10, Train_Loss: 0.8916357159614563, Test_Loss: 0.9103715419769287 *\n",
      "235:Epoch: 10, Train_Loss: 0.9148508906364441, Test_Loss: 0.9006409645080566 *\n",
      "236:Epoch: 10, Train_Loss: 0.9646686315536499, Test_Loss: 0.8715797066688538 *\n",
      "237:Epoch: 10, Train_Loss: 0.9107368588447571, Test_Loss: 0.8847143650054932\n",
      "238:Epoch: 10, Train_Loss: 0.9126125574111938, Test_Loss: 0.8789002299308777 *\n",
      "239:Epoch: 10, Train_Loss: 0.8791587948799133, Test_Loss: 0.8687930107116699 *\n",
      "240:Epoch: 10, Train_Loss: 0.8787719011306763, Test_Loss: 0.9378548860549927\n",
      "241:Epoch: 10, Train_Loss: 0.8863170146942139, Test_Loss: 0.9371712803840637 *\n",
      "242:Epoch: 10, Train_Loss: 1.2825278043746948, Test_Loss: 3.944161891937256\n",
      "243:Epoch: 10, Train_Loss: 1.0969905853271484, Test_Loss: 3.143833637237549 *\n",
      "244:Epoch: 10, Train_Loss: 1.2869517803192139, Test_Loss: 0.8726711869239807 *\n",
      "245:Epoch: 10, Train_Loss: 1.1999521255493164, Test_Loss: 0.8712401986122131 *\n",
      "246:Epoch: 10, Train_Loss: 1.076450228691101, Test_Loss: 0.8774160742759705\n",
      "247:Epoch: 10, Train_Loss: 1.114594578742981, Test_Loss: 0.8601499795913696 *\n",
      "248:Epoch: 10, Train_Loss: 0.9246198534965515, Test_Loss: 0.8779273629188538\n",
      "249:Epoch: 10, Train_Loss: 0.8770277500152588, Test_Loss: 0.9316484928131104\n",
      "250:Epoch: 10, Train_Loss: 0.8729918599128723, Test_Loss: 0.9236733317375183 *\n",
      "251:Epoch: 10, Train_Loss: 0.9130747318267822, Test_Loss: 0.8639197945594788 *\n",
      "252:Epoch: 10, Train_Loss: 1.1415830850601196, Test_Loss: 0.8829197883605957\n",
      "253:Epoch: 10, Train_Loss: 1.301481008529663, Test_Loss: 0.9131792783737183\n",
      "254:Epoch: 10, Train_Loss: 1.6306324005126953, Test_Loss: 0.9384773969650269\n",
      "255:Epoch: 10, Train_Loss: 1.9710578918457031, Test_Loss: 0.891711950302124 *\n",
      "256:Epoch: 10, Train_Loss: 1.2756963968276978, Test_Loss: 0.9009128212928772\n",
      "257:Epoch: 10, Train_Loss: 1.1604090929031372, Test_Loss: 0.9145258665084839\n",
      "258:Epoch: 10, Train_Loss: 0.8869050145149231, Test_Loss: 0.9018905162811279 *\n",
      "259:Epoch: 10, Train_Loss: 0.9002368450164795, Test_Loss: 0.8627542853355408 *\n",
      "260:Epoch: 10, Train_Loss: 1.2751445770263672, Test_Loss: 0.9830296635627747\n",
      "261:Epoch: 10, Train_Loss: 1.9101369380950928, Test_Loss: 0.8972283601760864 *\n",
      "262:Epoch: 10, Train_Loss: 0.9421833753585815, Test_Loss: 0.8927334547042847 *\n",
      "263:Epoch: 10, Train_Loss: 0.8990470767021179, Test_Loss: 0.9204927086830139\n",
      "264:Epoch: 10, Train_Loss: 0.9549391269683838, Test_Loss: 0.9193647503852844 *\n",
      "265:Epoch: 10, Train_Loss: 0.918975293636322, Test_Loss: 0.8956414461135864 *\n",
      "266:Epoch: 10, Train_Loss: 1.1913230419158936, Test_Loss: 0.9101029634475708\n",
      "267:Epoch: 10, Train_Loss: 1.1273369789123535, Test_Loss: 0.9392490386962891\n",
      "268:Epoch: 10, Train_Loss: 1.1144452095031738, Test_Loss: 0.9272069334983826 *\n",
      "269:Epoch: 10, Train_Loss: 1.163411021232605, Test_Loss: 0.9512042999267578\n",
      "270:Epoch: 10, Train_Loss: 0.8972113728523254, Test_Loss: 0.9598615169525146\n",
      "271:Epoch: 10, Train_Loss: 0.8878591060638428, Test_Loss: 1.0427311658859253\n",
      "272:Epoch: 10, Train_Loss: 0.8886632323265076, Test_Loss: 0.9372754693031311 *\n",
      "273:Epoch: 10, Train_Loss: 0.9851990938186646, Test_Loss: 0.9194710850715637 *\n",
      "274:Epoch: 10, Train_Loss: 0.8775365352630615, Test_Loss: 1.0274946689605713\n",
      "275:Epoch: 10, Train_Loss: 0.9490500688552856, Test_Loss: 0.9191603660583496 *\n",
      "276:Epoch: 10, Train_Loss: 15.76162052154541, Test_Loss: 0.9120045304298401 *\n",
      "277:Epoch: 10, Train_Loss: 1.026782751083374, Test_Loss: 0.9668276906013489\n",
      "278:Epoch: 10, Train_Loss: 1.9625980854034424, Test_Loss: 1.29734468460083\n",
      "279:Epoch: 10, Train_Loss: 1.672095775604248, Test_Loss: 1.6090989112854004\n",
      "280:Epoch: 10, Train_Loss: 0.9795424938201904, Test_Loss: 1.6990373134613037\n",
      "281:Epoch: 10, Train_Loss: 1.0655611753463745, Test_Loss: 1.5340266227722168 *\n",
      "282:Epoch: 10, Train_Loss: 2.3009443283081055, Test_Loss: 1.0141810178756714 *\n",
      "283:Epoch: 10, Train_Loss: 5.240764617919922, Test_Loss: 1.441367745399475\n",
      "284:Epoch: 10, Train_Loss: 1.3081738948822021, Test_Loss: 1.713450312614441\n",
      "285:Epoch: 10, Train_Loss: 1.140950322151184, Test_Loss: 1.5328727960586548 *\n",
      "286:Epoch: 10, Train_Loss: 3.774167776107788, Test_Loss: 2.15433669090271\n",
      "287:Epoch: 10, Train_Loss: 1.8920501470565796, Test_Loss: 1.6281485557556152 *\n",
      "288:Epoch: 10, Train_Loss: 1.2134380340576172, Test_Loss: 1.4600744247436523 *\n",
      "289:Epoch: 10, Train_Loss: 0.8663070201873779, Test_Loss: 0.9442986249923706 *\n",
      "290:Epoch: 10, Train_Loss: 0.8720585703849792, Test_Loss: 1.168858289718628\n",
      "291:Epoch: 10, Train_Loss: 0.8746225237846375, Test_Loss: 0.9175150990486145 *\n",
      "292:Epoch: 10, Train_Loss: 0.8560444116592407, Test_Loss: 0.9714681506156921\n",
      "293:Epoch: 10, Train_Loss: 0.8498055934906006, Test_Loss: 1.3314801454544067\n",
      "294:Epoch: 10, Train_Loss: 0.8447423577308655, Test_Loss: 1.0831578969955444 *\n",
      "295:Epoch: 10, Train_Loss: 0.8436145186424255, Test_Loss: 1.2311005592346191\n",
      "296:Epoch: 10, Train_Loss: 0.8449429273605347, Test_Loss: 0.8875485062599182 *\n",
      "297:Epoch: 10, Train_Loss: 0.8661327958106995, Test_Loss: 0.909611701965332\n",
      "298:Epoch: 10, Train_Loss: 0.8781883716583252, Test_Loss: 0.9148064851760864\n",
      "299:Epoch: 10, Train_Loss: 0.9202909469604492, Test_Loss: 0.9172674417495728\n",
      "300:Epoch: 10, Train_Loss: 0.8750821948051453, Test_Loss: 0.8888229131698608 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 10\n",
      "301:Epoch: 10, Train_Loss: 0.8634533882141113, Test_Loss: 2.813328266143799\n",
      "302:Epoch: 10, Train_Loss: 0.864583432674408, Test_Loss: 5.2856340408325195\n",
      "303:Epoch: 10, Train_Loss: 0.8474138975143433, Test_Loss: 0.980621874332428 *\n",
      "304:Epoch: 10, Train_Loss: 0.892245352268219, Test_Loss: 0.9790844321250916 *\n",
      "305:Epoch: 10, Train_Loss: 0.8422357439994812, Test_Loss: 0.9910921454429626\n",
      "306:Epoch: 10, Train_Loss: 0.8414871692657471, Test_Loss: 0.88351970911026 *\n",
      "307:Epoch: 10, Train_Loss: 0.8408291339874268, Test_Loss: 1.0150995254516602\n",
      "308:Epoch: 10, Train_Loss: 0.841344952583313, Test_Loss: 1.0787276029586792\n",
      "309:Epoch: 10, Train_Loss: 0.8411402702331543, Test_Loss: 0.9890351295471191 *\n",
      "310:Epoch: 10, Train_Loss: 0.8407371640205383, Test_Loss: 0.884090781211853 *\n",
      "311:Epoch: 10, Train_Loss: 0.8399336934089661, Test_Loss: 0.9437273144721985\n",
      "312:Epoch: 10, Train_Loss: 0.844684898853302, Test_Loss: 0.9692557454109192\n",
      "313:Epoch: 10, Train_Loss: 0.8621512651443481, Test_Loss: 1.0818763971328735\n",
      "314:Epoch: 10, Train_Loss: 0.8675500154495239, Test_Loss: 0.9131045937538147 *\n",
      "315:Epoch: 10, Train_Loss: 0.9086670279502869, Test_Loss: 0.9717155694961548\n",
      "316:Epoch: 10, Train_Loss: 0.8690973520278931, Test_Loss: 0.9514455795288086 *\n",
      "317:Epoch: 10, Train_Loss: 0.988538384437561, Test_Loss: 0.8414165377616882 *\n",
      "318:Epoch: 10, Train_Loss: 9.144856452941895, Test_Loss: 0.8944847583770752\n",
      "319:Epoch: 10, Train_Loss: 0.883428156375885, Test_Loss: 0.8824285268783569 *\n",
      "320:Epoch: 10, Train_Loss: 0.8915011882781982, Test_Loss: 1.0396676063537598\n",
      "321:Epoch: 10, Train_Loss: 0.9941530227661133, Test_Loss: 0.9057818651199341 *\n",
      "322:Epoch: 10, Train_Loss: 1.0179771184921265, Test_Loss: 0.9607293009757996\n",
      "323:Epoch: 10, Train_Loss: 0.8760727643966675, Test_Loss: 0.991187334060669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324:Epoch: 10, Train_Loss: 0.8983194231987, Test_Loss: 1.082215428352356\n",
      "325:Epoch: 10, Train_Loss: 1.013097882270813, Test_Loss: 1.0359277725219727 *\n",
      "326:Epoch: 10, Train_Loss: 1.1212583780288696, Test_Loss: 0.9527343511581421 *\n",
      "327:Epoch: 10, Train_Loss: 0.9962271451950073, Test_Loss: 0.8780142664909363 *\n",
      "328:Epoch: 10, Train_Loss: 0.9280284643173218, Test_Loss: 0.9073721766471863\n",
      "329:Epoch: 10, Train_Loss: 0.8385213017463684, Test_Loss: 0.8860041499137878 *\n",
      "330:Epoch: 10, Train_Loss: 0.9457007646560669, Test_Loss: 0.868179202079773 *\n",
      "331:Epoch: 10, Train_Loss: 0.9118386507034302, Test_Loss: 0.9591408371925354\n",
      "332:Epoch: 10, Train_Loss: 1.0142078399658203, Test_Loss: 0.8665139079093933 *\n",
      "333:Epoch: 10, Train_Loss: 0.9211730360984802, Test_Loss: 5.139129638671875\n",
      "334:Epoch: 10, Train_Loss: 0.9170331358909607, Test_Loss: 2.181870222091675 *\n",
      "335:Epoch: 10, Train_Loss: 0.8473496437072754, Test_Loss: 0.8349655866622925 *\n",
      "336:Epoch: 10, Train_Loss: 0.8712950348854065, Test_Loss: 0.8449178338050842\n",
      "337:Epoch: 10, Train_Loss: 0.9152955412864685, Test_Loss: 0.8672884702682495\n",
      "338:Epoch: 10, Train_Loss: 0.862350344657898, Test_Loss: 0.8631364107131958 *\n",
      "339:Epoch: 10, Train_Loss: 0.8342987895011902, Test_Loss: 0.8446652889251709 *\n",
      "340:Epoch: 10, Train_Loss: 0.8376656174659729, Test_Loss: 0.9131084084510803\n",
      "341:Epoch: 10, Train_Loss: 0.8387069702148438, Test_Loss: 0.9190359115600586\n",
      "342:Epoch: 10, Train_Loss: 1.3980612754821777, Test_Loss: 0.833967387676239 *\n",
      "343:Epoch: 10, Train_Loss: 6.097469329833984, Test_Loss: 0.8697409629821777\n",
      "344:Epoch: 10, Train_Loss: 0.8337631225585938, Test_Loss: 0.8487014174461365 *\n",
      "345:Epoch: 10, Train_Loss: 0.8453915119171143, Test_Loss: 0.8436865210533142 *\n",
      "346:Epoch: 10, Train_Loss: 0.8541024923324585, Test_Loss: 0.8347270488739014 *\n",
      "347:Epoch: 10, Train_Loss: 0.8397635817527771, Test_Loss: 0.8992307186126709\n",
      "348:Epoch: 10, Train_Loss: 0.8347980976104736, Test_Loss: 0.8703590035438538 *\n",
      "349:Epoch: 10, Train_Loss: 0.8321371674537659, Test_Loss: 0.9496785998344421\n",
      "350:Epoch: 10, Train_Loss: 0.8390229344367981, Test_Loss: 0.910330057144165 *\n",
      "351:Epoch: 10, Train_Loss: 0.867987871170044, Test_Loss: 0.8602834343910217 *\n",
      "352:Epoch: 10, Train_Loss: 0.8451871871948242, Test_Loss: 0.8394857048988342 *\n",
      "353:Epoch: 10, Train_Loss: 0.8419973254203796, Test_Loss: 0.8351955413818359 *\n",
      "354:Epoch: 10, Train_Loss: 0.8308451771736145, Test_Loss: 0.8340058326721191 *\n",
      "355:Epoch: 10, Train_Loss: 0.829135537147522, Test_Loss: 0.8369065523147583\n",
      "356:Epoch: 10, Train_Loss: 0.8453865051269531, Test_Loss: 0.8399662971496582\n",
      "357:Epoch: 10, Train_Loss: 0.8294700980186462, Test_Loss: 0.837856113910675 *\n",
      "358:Epoch: 10, Train_Loss: 0.8301196694374084, Test_Loss: 0.8317325115203857 *\n",
      "359:Epoch: 10, Train_Loss: 0.8529609441757202, Test_Loss: 0.8404908180236816\n",
      "360:Epoch: 10, Train_Loss: 0.8680846095085144, Test_Loss: 0.8317592144012451 *\n",
      "361:Epoch: 10, Train_Loss: 0.8394680619239807, Test_Loss: 0.8323693871498108\n",
      "362:Epoch: 10, Train_Loss: 0.8265405893325806, Test_Loss: 0.8418859243392944\n",
      "363:Epoch: 10, Train_Loss: 0.831099271774292, Test_Loss: 0.845720648765564\n",
      "364:Epoch: 10, Train_Loss: 0.9127645492553711, Test_Loss: 0.8494207262992859\n",
      "365:Epoch: 10, Train_Loss: 0.8728885054588318, Test_Loss: 0.9663372039794922\n",
      "366:Epoch: 10, Train_Loss: 0.8971787691116333, Test_Loss: 1.2310218811035156\n",
      "367:Epoch: 10, Train_Loss: 0.8436928987503052, Test_Loss: 1.1248326301574707 *\n",
      "368:Epoch: 10, Train_Loss: 0.9175440073013306, Test_Loss: 0.9119458198547363 *\n",
      "369:Epoch: 10, Train_Loss: 0.8558679223060608, Test_Loss: 0.8324378728866577 *\n",
      "370:Epoch: 10, Train_Loss: 0.8893197178840637, Test_Loss: 0.8385424613952637\n",
      "371:Epoch: 10, Train_Loss: 0.8411499857902527, Test_Loss: 0.8983673453330994\n",
      "372:Epoch: 10, Train_Loss: 0.9737446308135986, Test_Loss: 1.307709813117981\n",
      "373:Epoch: 10, Train_Loss: 0.8416144251823425, Test_Loss: 1.8771255016326904\n",
      "374:Epoch: 10, Train_Loss: 0.8265171647071838, Test_Loss: 1.1690804958343506 *\n",
      "375:Epoch: 10, Train_Loss: 0.8265205025672913, Test_Loss: 0.9062161445617676 *\n",
      "376:Epoch: 10, Train_Loss: 0.8236023783683777, Test_Loss: 0.8314719200134277 *\n",
      "377:Epoch: 10, Train_Loss: 0.8231409788131714, Test_Loss: 0.8345025181770325\n",
      "378:Epoch: 10, Train_Loss: 0.8236445188522339, Test_Loss: 0.8285335302352905 *\n",
      "379:Epoch: 10, Train_Loss: 1.3758469820022583, Test_Loss: 0.8394070863723755\n",
      "380:Epoch: 10, Train_Loss: 5.073388576507568, Test_Loss: 0.848139762878418\n",
      "381:Epoch: 10, Train_Loss: 0.8308694362640381, Test_Loss: 0.8643810153007507\n",
      "382:Epoch: 10, Train_Loss: 0.8317830562591553, Test_Loss: 0.8251403570175171 *\n",
      "383:Epoch: 10, Train_Loss: 0.828697681427002, Test_Loss: 0.9250650405883789\n",
      "384:Epoch: 10, Train_Loss: 0.8226034641265869, Test_Loss: 1.129681944847107\n",
      "385:Epoch: 10, Train_Loss: 0.821054220199585, Test_Loss: 1.0054216384887695 *\n",
      "386:Epoch: 10, Train_Loss: 0.8211241364479065, Test_Loss: 1.0736198425292969\n",
      "387:Epoch: 10, Train_Loss: 0.8198540806770325, Test_Loss: 0.8357424139976501 *\n",
      "388:Epoch: 10, Train_Loss: 0.819750189781189, Test_Loss: 0.8331201672554016 *\n",
      "389:Epoch: 10, Train_Loss: 0.8201504945755005, Test_Loss: 0.8329260945320129 *\n",
      "390:Epoch: 10, Train_Loss: 0.885133683681488, Test_Loss: 0.8332912921905518\n",
      "391:Epoch: 10, Train_Loss: 0.8658207654953003, Test_Loss: 0.8630378246307373\n",
      "392:Epoch: 10, Train_Loss: 0.8998072743415833, Test_Loss: 4.053092956542969\n",
      "393:Epoch: 10, Train_Loss: 0.8589596748352051, Test_Loss: 2.95481014251709 *\n",
      "394:Epoch: 10, Train_Loss: 0.820229172706604, Test_Loss: 0.8298848271369934 *\n",
      "395:Epoch: 10, Train_Loss: 0.9755838513374329, Test_Loss: 0.8223606944084167 *\n",
      "396:Epoch: 10, Train_Loss: 1.0416861772537231, Test_Loss: 0.8219540119171143 *\n",
      "397:Epoch: 10, Train_Loss: 1.0435900688171387, Test_Loss: 0.8318231701850891\n",
      "398:Epoch: 10, Train_Loss: 0.962867021560669, Test_Loss: 0.8231461644172668 *\n",
      "399:Epoch: 10, Train_Loss: 0.8190480470657349, Test_Loss: 0.8336297869682312\n",
      "400:Epoch: 10, Train_Loss: 0.8167683482170105, Test_Loss: 0.8222401738166809 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 10\n",
      "401:Epoch: 10, Train_Loss: 0.818650484085083, Test_Loss: 0.8252416849136353\n",
      "402:Epoch: 10, Train_Loss: 0.8285800218582153, Test_Loss: 0.8276199698448181\n",
      "403:Epoch: 10, Train_Loss: 0.8333425521850586, Test_Loss: 0.826030433177948 *\n",
      "404:Epoch: 10, Train_Loss: 0.8284592628479004, Test_Loss: 0.8342058062553406\n",
      "405:Epoch: 10, Train_Loss: 0.8186711668968201, Test_Loss: 0.8253394365310669 *\n",
      "406:Epoch: 10, Train_Loss: 0.8152410984039307, Test_Loss: 0.8206836581230164 *\n",
      "407:Epoch: 10, Train_Loss: 0.8257816433906555, Test_Loss: 0.8262721300125122\n",
      "408:Epoch: 10, Train_Loss: 0.8505926132202148, Test_Loss: 0.8155783414840698 *\n",
      "409:Epoch: 10, Train_Loss: 0.9824151396751404, Test_Loss: 0.8171021342277527\n",
      "410:Epoch: 10, Train_Loss: 1.0032192468643188, Test_Loss: 0.8165304064750671 *\n",
      "411:Epoch: 10, Train_Loss: 0.9804720878601074, Test_Loss: 0.8196285367012024\n",
      "412:Epoch: 10, Train_Loss: 0.8732827305793762, Test_Loss: 0.8189356327056885 *\n",
      "413:Epoch: 10, Train_Loss: 0.9436624050140381, Test_Loss: 0.8197465538978577\n",
      "414:Epoch: 10, Train_Loss: 0.9082653522491455, Test_Loss: 0.8236658573150635\n",
      "415:Epoch: 10, Train_Loss: 0.9182949662208557, Test_Loss: 0.8316599726676941\n",
      "416:Epoch: 10, Train_Loss: 0.9588799476623535, Test_Loss: 0.821225106716156 *\n",
      "417:Epoch: 10, Train_Loss: 1.1337798833847046, Test_Loss: 0.8194823265075684 *\n",
      "418:Epoch: 10, Train_Loss: 0.824425220489502, Test_Loss: 0.8161982893943787 *\n",
      "419:Epoch: 10, Train_Loss: 0.820426881313324, Test_Loss: 0.8176009654998779\n",
      "420:Epoch: 10, Train_Loss: 3.5446836948394775, Test_Loss: 0.816783607006073 *\n",
      "421:Epoch: 10, Train_Loss: 1.290597677230835, Test_Loss: 0.815403938293457 *\n",
      "422:Epoch: 10, Train_Loss: 0.8551492691040039, Test_Loss: 0.8571164608001709\n",
      "423:Epoch: 10, Train_Loss: 0.8619548082351685, Test_Loss: 0.8472999930381775 *\n",
      "424:Epoch: 10, Train_Loss: 0.8522613048553467, Test_Loss: 6.018576622009277\n",
      "425:Epoch: 10, Train_Loss: 0.8347935080528259, Test_Loss: 1.0400021076202393 *\n",
      "426:Epoch: 10, Train_Loss: 0.8188221454620361, Test_Loss: 0.8122711777687073 *\n",
      "427:Epoch: 10, Train_Loss: 0.8715263605117798, Test_Loss: 0.8401246070861816\n",
      "428:Epoch: 10, Train_Loss: 0.961776614189148, Test_Loss: 0.871928334236145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429:Epoch: 10, Train_Loss: 0.9246368408203125, Test_Loss: 0.8763414025306702\n",
      "430:Epoch: 10, Train_Loss: 0.8897028565406799, Test_Loss: 0.8169777989387512 *\n",
      "431:Epoch: 10, Train_Loss: 0.894470751285553, Test_Loss: 0.8939088582992554\n",
      "432:Epoch: 10, Train_Loss: 0.8414307236671448, Test_Loss: 0.8660431504249573 *\n",
      "433:Epoch: 10, Train_Loss: 0.8408164978027344, Test_Loss: 0.8118154406547546 *\n",
      "434:Epoch: 10, Train_Loss: 0.8172163367271423, Test_Loss: 0.8377349376678467\n",
      "435:Epoch: 10, Train_Loss: 0.853431761264801, Test_Loss: 0.8351641893386841 *\n",
      "436:Epoch: 10, Train_Loss: 0.8378148078918457, Test_Loss: 0.8134604692459106 *\n",
      "437:Epoch: 10, Train_Loss: 0.8113617897033691, Test_Loss: 0.8300061821937561\n",
      "438:Epoch: 10, Train_Loss: 0.8187853693962097, Test_Loss: 0.9423714280128479\n",
      "439:Epoch: 10, Train_Loss: 0.8610870242118835, Test_Loss: 0.8423888683319092 *\n",
      "440:Epoch: 10, Train_Loss: 0.8608759045600891, Test_Loss: 0.8951577544212341\n",
      "441:Epoch: 10, Train_Loss: 0.8097526431083679, Test_Loss: 0.8503050804138184 *\n",
      "442:Epoch: 10, Train_Loss: 0.8063026070594788, Test_Loss: 0.8589429259300232\n",
      "443:Epoch: 10, Train_Loss: 0.8067162036895752, Test_Loss: 0.8314492106437683 *\n",
      "444:Epoch: 10, Train_Loss: 0.8056686520576477, Test_Loss: 0.8291425704956055 *\n",
      "445:Epoch: 10, Train_Loss: 0.806271493434906, Test_Loss: 0.8299693465232849\n",
      "446:Epoch: 10, Train_Loss: 0.8077946901321411, Test_Loss: 0.8328191041946411\n",
      "447:Epoch: 10, Train_Loss: 0.8064374327659607, Test_Loss: 0.833482027053833\n",
      "448:Epoch: 10, Train_Loss: 0.8069064021110535, Test_Loss: 0.8337544202804565\n",
      "449:Epoch: 10, Train_Loss: 0.8056418299674988, Test_Loss: 0.8264291882514954 *\n",
      "450:Epoch: 10, Train_Loss: 0.8057011365890503, Test_Loss: 0.8399355411529541\n",
      "451:Epoch: 10, Train_Loss: 0.812094509601593, Test_Loss: 0.8345966339111328 *\n",
      "452:Epoch: 10, Train_Loss: 0.8233727216720581, Test_Loss: 0.8136395215988159 *\n",
      "453:Epoch: 10, Train_Loss: 0.8218625783920288, Test_Loss: 0.817209780216217\n",
      "454:Epoch: 10, Train_Loss: 0.8209198117256165, Test_Loss: 0.8739109039306641\n",
      "1:Epoch: 11, Train_Loss: 0.8265200257301331, Test_Loss: 0.8290020227432251 *\n",
      "2:Epoch: 11, Train_Loss: 0.8237669467926025, Test_Loss: 1.085780382156372\n",
      "3:Epoch: 11, Train_Loss: 0.8131557106971741, Test_Loss: 1.3964006900787354\n",
      "4:Epoch: 11, Train_Loss: 0.81076580286026, Test_Loss: 1.0930383205413818 *\n",
      "5:Epoch: 11, Train_Loss: 0.826921284198761, Test_Loss: 0.8928259611129761 *\n",
      "6:Epoch: 11, Train_Loss: 0.842775285243988, Test_Loss: 0.8378962278366089 *\n",
      "7:Epoch: 11, Train_Loss: 0.8042391538619995, Test_Loss: 0.8086218237876892 *\n",
      "8:Epoch: 11, Train_Loss: 0.8126572966575623, Test_Loss: 0.8781906366348267\n",
      "9:Epoch: 11, Train_Loss: 0.8060861825942993, Test_Loss: 1.3843806982040405\n",
      "10:Epoch: 11, Train_Loss: 0.8301660418510437, Test_Loss: 1.82435941696167\n",
      "11:Epoch: 11, Train_Loss: 0.850414514541626, Test_Loss: 0.9697168469429016 *\n",
      "12:Epoch: 11, Train_Loss: 0.862575888633728, Test_Loss: 0.913849413394928 *\n",
      "13:Epoch: 11, Train_Loss: 0.8227182626724243, Test_Loss: 0.8019298315048218 *\n",
      "14:Epoch: 11, Train_Loss: 0.801519513130188, Test_Loss: 0.8054219484329224\n",
      "15:Epoch: 11, Train_Loss: 0.8794096112251282, Test_Loss: 0.8030350208282471 *\n",
      "16:Epoch: 11, Train_Loss: 0.8093538880348206, Test_Loss: 0.81449955701828\n",
      "17:Epoch: 11, Train_Loss: 0.8060058355331421, Test_Loss: 0.8258109092712402\n",
      "18:Epoch: 11, Train_Loss: 0.8185042142868042, Test_Loss: 0.8311830759048462\n",
      "19:Epoch: 11, Train_Loss: 0.811585009098053, Test_Loss: 0.8027399182319641 *\n",
      "20:Epoch: 11, Train_Loss: 0.927086353302002, Test_Loss: 0.9262000322341919\n",
      "21:Epoch: 11, Train_Loss: 0.8762810826301575, Test_Loss: 1.1960828304290771\n",
      "22:Epoch: 11, Train_Loss: 0.8349061012268066, Test_Loss: 0.9067464470863342 *\n",
      "23:Epoch: 11, Train_Loss: 0.8093132376670837, Test_Loss: 1.0005422830581665\n",
      "24:Epoch: 11, Train_Loss: 0.8148258924484253, Test_Loss: 0.8123372197151184 *\n",
      "25:Epoch: 11, Train_Loss: 0.8075895309448242, Test_Loss: 0.8119118213653564 *\n",
      "26:Epoch: 11, Train_Loss: 0.8001353740692139, Test_Loss: 0.8115376234054565 *\n",
      "27:Epoch: 11, Train_Loss: 0.8090906143188477, Test_Loss: 0.8113903999328613 *\n",
      "28:Epoch: 11, Train_Loss: 0.8156223297119141, Test_Loss: 0.8309637904167175\n",
      "29:Epoch: 11, Train_Loss: 0.8326849341392517, Test_Loss: 5.366209506988525\n",
      "30:Epoch: 11, Train_Loss: 0.8912298679351807, Test_Loss: 1.6428265571594238 *\n",
      "31:Epoch: 11, Train_Loss: 0.8086346983909607, Test_Loss: 0.8067963719367981 *\n",
      "32:Epoch: 11, Train_Loss: 0.8504266738891602, Test_Loss: 0.7987305521965027 *\n",
      "33:Epoch: 11, Train_Loss: 0.8210098743438721, Test_Loss: 0.7993476986885071\n",
      "34:Epoch: 11, Train_Loss: 0.8161920309066772, Test_Loss: 0.8076257705688477\n",
      "35:Epoch: 11, Train_Loss: 0.8936656713485718, Test_Loss: 0.7985763549804688 *\n",
      "36:Epoch: 11, Train_Loss: 1.0352388620376587, Test_Loss: 0.8020457029342651\n",
      "37:Epoch: 11, Train_Loss: 0.8068323135375977, Test_Loss: 0.7959952354431152 *\n",
      "38:Epoch: 11, Train_Loss: 0.8350604772567749, Test_Loss: 0.7975048422813416\n",
      "39:Epoch: 11, Train_Loss: 0.7939310073852539, Test_Loss: 0.7988773584365845\n",
      "40:Epoch: 11, Train_Loss: 0.7935025095939636, Test_Loss: 0.803251326084137\n",
      "41:Epoch: 11, Train_Loss: 0.7945753335952759, Test_Loss: 0.800832986831665 *\n",
      "42:Epoch: 11, Train_Loss: 0.79377281665802, Test_Loss: 0.8144387006759644\n",
      "43:Epoch: 11, Train_Loss: 0.8107770681381226, Test_Loss: 0.8061869144439697 *\n",
      "44:Epoch: 11, Train_Loss: 0.8096081614494324, Test_Loss: 0.7960728406906128 *\n",
      "45:Epoch: 11, Train_Loss: 0.8050270676612854, Test_Loss: 0.7927587032318115 *\n",
      "46:Epoch: 11, Train_Loss: 0.7998208999633789, Test_Loss: 0.793250322341919\n",
      "47:Epoch: 11, Train_Loss: 0.8124989867210388, Test_Loss: 0.7930129766464233 *\n",
      "48:Epoch: 11, Train_Loss: 0.7967380285263062, Test_Loss: 0.7919161319732666 *\n",
      "49:Epoch: 11, Train_Loss: 0.7933403253555298, Test_Loss: 0.7937727570533752\n",
      "50:Epoch: 11, Train_Loss: 0.7907902598381042, Test_Loss: 0.7920838594436646 *\n",
      "51:Epoch: 11, Train_Loss: 0.8182296752929688, Test_Loss: 0.7933943271636963\n",
      "52:Epoch: 11, Train_Loss: 0.8211661577224731, Test_Loss: 0.7947902679443359\n",
      "53:Epoch: 11, Train_Loss: 0.8153188824653625, Test_Loss: 0.7928858399391174 *\n",
      "54:Epoch: 11, Train_Loss: 0.7968291640281677, Test_Loss: 0.7916213274002075 *\n",
      "55:Epoch: 11, Train_Loss: 0.8559125661849976, Test_Loss: 0.7911463975906372 *\n",
      "56:Epoch: 11, Train_Loss: 0.8435289263725281, Test_Loss: 0.7908841967582703 *\n",
      "57:Epoch: 11, Train_Loss: 0.8166368007659912, Test_Loss: 0.7909181118011475\n",
      "58:Epoch: 11, Train_Loss: 0.7994909286499023, Test_Loss: 0.792848527431488\n",
      "59:Epoch: 11, Train_Loss: 0.8150670528411865, Test_Loss: 0.8465164303779602\n",
      "60:Epoch: 11, Train_Loss: 0.7955957651138306, Test_Loss: 1.0572566986083984\n",
      "61:Epoch: 11, Train_Loss: 0.8052548170089722, Test_Loss: 5.95780611038208\n",
      "62:Epoch: 11, Train_Loss: 0.8090903162956238, Test_Loss: 0.7976688146591187 *\n",
      "63:Epoch: 11, Train_Loss: 0.8267775177955627, Test_Loss: 0.7887157201766968 *\n",
      "64:Epoch: 11, Train_Loss: 2.682835102081299, Test_Loss: 0.8251287937164307\n",
      "65:Epoch: 11, Train_Loss: 4.3559956550598145, Test_Loss: 0.8512315154075623\n",
      "66:Epoch: 11, Train_Loss: 0.800457239151001, Test_Loss: 0.8532081842422485\n",
      "67:Epoch: 11, Train_Loss: 0.8003863096237183, Test_Loss: 0.7914568185806274 *\n",
      "68:Epoch: 11, Train_Loss: 0.8135311603546143, Test_Loss: 0.9020504951477051\n",
      "69:Epoch: 11, Train_Loss: 0.9716116189956665, Test_Loss: 0.8258323073387146 *\n",
      "70:Epoch: 11, Train_Loss: 0.8213397264480591, Test_Loss: 0.788192629814148 *\n",
      "71:Epoch: 11, Train_Loss: 0.7952772378921509, Test_Loss: 0.8261035680770874\n",
      "72:Epoch: 11, Train_Loss: 0.7861738204956055, Test_Loss: 0.804048478603363 *\n",
      "73:Epoch: 11, Train_Loss: 0.8525492548942566, Test_Loss: 0.7936220765113831 *\n",
      "74:Epoch: 11, Train_Loss: 0.7964186668395996, Test_Loss: 0.8346949815750122\n",
      "75:Epoch: 11, Train_Loss: 0.8053393959999084, Test_Loss: 0.9013146162033081\n",
      "76:Epoch: 11, Train_Loss: 1.3793278932571411, Test_Loss: 0.8354009389877319 *\n",
      "77:Epoch: 11, Train_Loss: 2.151765823364258, Test_Loss: 0.8905024528503418\n",
      "78:Epoch: 11, Train_Loss: 1.6228153705596924, Test_Loss: 0.8243194222450256 *\n",
      "79:Epoch: 11, Train_Loss: 0.9116999506950378, Test_Loss: 0.8320080041885376\n",
      "80:Epoch: 11, Train_Loss: 1.2144466638565063, Test_Loss: 0.8079820871353149 *\n",
      "81:Epoch: 11, Train_Loss: 3.193755626678467, Test_Loss: 0.8030256628990173 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82:Epoch: 11, Train_Loss: 1.4516679048538208, Test_Loss: 0.8056430816650391\n",
      "83:Epoch: 11, Train_Loss: 0.8357576727867126, Test_Loss: 0.8084006905555725\n",
      "84:Epoch: 11, Train_Loss: 0.8136851787567139, Test_Loss: 0.8077595233917236 *\n",
      "85:Epoch: 11, Train_Loss: 1.7956936359405518, Test_Loss: 0.8070691823959351 *\n",
      "86:Epoch: 11, Train_Loss: 2.280040740966797, Test_Loss: 0.8065280914306641 *\n",
      "87:Epoch: 11, Train_Loss: 1.2221980094909668, Test_Loss: 0.8083595037460327\n",
      "88:Epoch: 11, Train_Loss: 0.8025038242340088, Test_Loss: 0.8050691485404968 *\n",
      "89:Epoch: 11, Train_Loss: 0.7893155217170715, Test_Loss: 0.7874003052711487 *\n",
      "90:Epoch: 11, Train_Loss: 1.324649453163147, Test_Loss: 0.790775716304779\n",
      "91:Epoch: 11, Train_Loss: 1.0888147354125977, Test_Loss: 0.8376558423042297\n",
      "92:Epoch: 11, Train_Loss: 0.829046905040741, Test_Loss: 0.7986354231834412 *\n",
      "93:Epoch: 11, Train_Loss: 0.8615390062332153, Test_Loss: 1.1336824893951416\n",
      "94:Epoch: 11, Train_Loss: 0.9605007171630859, Test_Loss: 1.31355619430542\n",
      "95:Epoch: 11, Train_Loss: 0.9070765972137451, Test_Loss: 1.0233352184295654 *\n",
      "96:Epoch: 11, Train_Loss: 0.8697007298469543, Test_Loss: 0.8297392129898071 *\n",
      "97:Epoch: 11, Train_Loss: 1.0821490287780762, Test_Loss: 0.8087000846862793 *\n",
      "98:Epoch: 11, Train_Loss: 0.8192659616470337, Test_Loss: 0.7852448225021362 *\n",
      "99:Epoch: 11, Train_Loss: 0.8612302541732788, Test_Loss: 0.8453316688537598\n",
      "100:Epoch: 11, Train_Loss: 1.0527883768081665, Test_Loss: 1.4070732593536377\n",
      "Model saved at location ../Saver/model.ckpt at epoch 11\n",
      "101:Epoch: 11, Train_Loss: 1.0686869621276855, Test_Loss: 1.5826575756072998\n",
      "102:Epoch: 11, Train_Loss: 1.1365002393722534, Test_Loss: 0.8490270972251892 *\n",
      "103:Epoch: 11, Train_Loss: 0.9435168504714966, Test_Loss: 0.8598727583885193\n",
      "104:Epoch: 11, Train_Loss: 0.836300790309906, Test_Loss: 0.7814355492591858 *\n",
      "105:Epoch: 11, Train_Loss: 0.8764753937721252, Test_Loss: 0.7825129628181458\n",
      "106:Epoch: 11, Train_Loss: 0.8252854943275452, Test_Loss: 0.7860758900642395\n",
      "107:Epoch: 11, Train_Loss: 0.7885332703590393, Test_Loss: 0.7976425290107727\n",
      "108:Epoch: 11, Train_Loss: 0.7812032699584961, Test_Loss: 0.7997441291809082\n",
      "109:Epoch: 11, Train_Loss: 0.7778595089912415, Test_Loss: 0.8006060719490051\n",
      "110:Epoch: 11, Train_Loss: 0.777030885219574, Test_Loss: 0.7823102474212646 *\n",
      "111:Epoch: 11, Train_Loss: 0.7833135724067688, Test_Loss: 0.9139044284820557\n",
      "112:Epoch: 11, Train_Loss: 0.8062554001808167, Test_Loss: 1.1988651752471924\n",
      "113:Epoch: 11, Train_Loss: 0.8146459460258484, Test_Loss: 1.0066345930099487 *\n",
      "114:Epoch: 11, Train_Loss: 0.8311734199523926, Test_Loss: 0.9690399765968323 *\n",
      "115:Epoch: 11, Train_Loss: 0.8572360277175903, Test_Loss: 0.7948782444000244 *\n",
      "116:Epoch: 11, Train_Loss: 1.16105318069458, Test_Loss: 0.7927213907241821 *\n",
      "117:Epoch: 11, Train_Loss: 0.8165578842163086, Test_Loss: 0.7915343046188354 *\n",
      "118:Epoch: 11, Train_Loss: 0.8289148807525635, Test_Loss: 0.8032941818237305\n",
      "119:Epoch: 11, Train_Loss: 0.8483279347419739, Test_Loss: 0.8694412708282471\n",
      "120:Epoch: 11, Train_Loss: 1.2199335098266602, Test_Loss: 5.994118690490723\n",
      "121:Epoch: 11, Train_Loss: 1.0687217712402344, Test_Loss: 0.991489052772522 *\n",
      "122:Epoch: 11, Train_Loss: 0.7813198566436768, Test_Loss: 0.7905502319335938 *\n",
      "123:Epoch: 11, Train_Loss: 0.7959957122802734, Test_Loss: 0.7795519232749939 *\n",
      "124:Epoch: 11, Train_Loss: 1.2616918087005615, Test_Loss: 0.7825640439987183\n",
      "125:Epoch: 11, Train_Loss: 1.1343413591384888, Test_Loss: 0.7833911776542664\n",
      "126:Epoch: 11, Train_Loss: 0.8799200057983398, Test_Loss: 0.7987419962882996\n",
      "127:Epoch: 11, Train_Loss: 0.7966403961181641, Test_Loss: 0.832397997379303\n",
      "128:Epoch: 11, Train_Loss: 0.8013548254966736, Test_Loss: 0.7828161716461182 *\n",
      "129:Epoch: 11, Train_Loss: 1.4147745370864868, Test_Loss: 0.8030133247375488\n",
      "130:Epoch: 11, Train_Loss: 1.805490255355835, Test_Loss: 0.8041009902954102\n",
      "131:Epoch: 11, Train_Loss: 0.8245489001274109, Test_Loss: 0.8576812744140625\n",
      "132:Epoch: 11, Train_Loss: 0.7925819158554077, Test_Loss: 0.795205295085907 *\n",
      "133:Epoch: 11, Train_Loss: 0.7727262377738953, Test_Loss: 0.7943655848503113 *\n",
      "134:Epoch: 11, Train_Loss: 0.7803394794464111, Test_Loss: 0.8289539217948914\n",
      "135:Epoch: 11, Train_Loss: 1.143487811088562, Test_Loss: 0.7770780920982361 *\n",
      "136:Epoch: 11, Train_Loss: 0.7888325452804565, Test_Loss: 0.7813667058944702\n",
      "137:Epoch: 11, Train_Loss: 0.9004126787185669, Test_Loss: 0.7860598564147949\n",
      "138:Epoch: 11, Train_Loss: 0.9468015432357788, Test_Loss: 0.8009154200553894\n",
      "139:Epoch: 11, Train_Loss: 0.8012415170669556, Test_Loss: 0.785107433795929 *\n",
      "140:Epoch: 11, Train_Loss: 0.7963888645172119, Test_Loss: 0.7945818901062012\n",
      "141:Epoch: 11, Train_Loss: 0.8792561292648315, Test_Loss: 0.8014400005340576\n",
      "142:Epoch: 11, Train_Loss: 0.9388713836669922, Test_Loss: 0.841498613357544\n",
      "143:Epoch: 11, Train_Loss: 0.799365758895874, Test_Loss: 0.8268736600875854 *\n",
      "144:Epoch: 11, Train_Loss: 0.7962219715118408, Test_Loss: 0.8056909441947937 *\n",
      "145:Epoch: 11, Train_Loss: 0.8189823031425476, Test_Loss: 0.793244481086731 *\n",
      "146:Epoch: 11, Train_Loss: 0.919812023639679, Test_Loss: 0.8015710711479187\n",
      "147:Epoch: 11, Train_Loss: 0.8274160027503967, Test_Loss: 0.7888331413269043 *\n",
      "148:Epoch: 11, Train_Loss: 0.7916120886802673, Test_Loss: 0.7858500480651855 *\n",
      "149:Epoch: 11, Train_Loss: 0.7915677428245544, Test_Loss: 0.8044978976249695\n",
      "150:Epoch: 11, Train_Loss: 0.7954329252243042, Test_Loss: 0.8687489628791809\n",
      "151:Epoch: 11, Train_Loss: 1.1795356273651123, Test_Loss: 2.1120400428771973\n",
      "152:Epoch: 11, Train_Loss: 1.0319559574127197, Test_Loss: 4.950866222381592\n",
      "153:Epoch: 11, Train_Loss: 1.1124048233032227, Test_Loss: 0.7840093374252319 *\n",
      "154:Epoch: 11, Train_Loss: 1.1140236854553223, Test_Loss: 0.8171443939208984\n",
      "155:Epoch: 11, Train_Loss: 1.0031862258911133, Test_Loss: 0.838384747505188\n",
      "156:Epoch: 11, Train_Loss: 1.0452158451080322, Test_Loss: 0.7716600894927979 *\n",
      "157:Epoch: 11, Train_Loss: 0.8259442448616028, Test_Loss: 0.7881734371185303\n",
      "158:Epoch: 11, Train_Loss: 0.782179594039917, Test_Loss: 0.8009406328201294\n",
      "159:Epoch: 11, Train_Loss: 0.7830789089202881, Test_Loss: 0.8360252380371094\n",
      "160:Epoch: 11, Train_Loss: 0.7929140329360962, Test_Loss: 0.7929958701133728 *\n",
      "161:Epoch: 11, Train_Loss: 1.0171414613723755, Test_Loss: 0.812709391117096\n",
      "162:Epoch: 11, Train_Loss: 1.2014317512512207, Test_Loss: 0.813353955745697\n",
      "163:Epoch: 11, Train_Loss: 1.208230972290039, Test_Loss: 0.910923182964325\n",
      "164:Epoch: 11, Train_Loss: 2.16780424118042, Test_Loss: 0.8176732659339905 *\n",
      "165:Epoch: 11, Train_Loss: 1.1260080337524414, Test_Loss: 0.8605302572250366\n",
      "166:Epoch: 11, Train_Loss: 1.0725876092910767, Test_Loss: 0.7910206913948059 *\n",
      "167:Epoch: 11, Train_Loss: 0.8051314353942871, Test_Loss: 0.8312830924987793\n",
      "168:Epoch: 11, Train_Loss: 0.7806164622306824, Test_Loss: 0.7716301679611206 *\n",
      "169:Epoch: 11, Train_Loss: 1.14004385471344, Test_Loss: 0.8770278692245483\n",
      "170:Epoch: 11, Train_Loss: 1.7509315013885498, Test_Loss: 0.8886103630065918\n",
      "171:Epoch: 11, Train_Loss: 1.052236557006836, Test_Loss: 0.7715005874633789 *\n",
      "172:Epoch: 11, Train_Loss: 0.8079842925071716, Test_Loss: 0.8389431238174438\n",
      "173:Epoch: 11, Train_Loss: 0.8765367269515991, Test_Loss: 0.8158099055290222 *\n",
      "174:Epoch: 11, Train_Loss: 0.830298662185669, Test_Loss: 0.8218100070953369\n",
      "175:Epoch: 11, Train_Loss: 1.1450483798980713, Test_Loss: 0.817357063293457 *\n",
      "176:Epoch: 11, Train_Loss: 0.9326575994491577, Test_Loss: 0.8014521598815918 *\n",
      "177:Epoch: 11, Train_Loss: 1.004921555519104, Test_Loss: 0.8103744387626648\n",
      "178:Epoch: 11, Train_Loss: 0.9154205322265625, Test_Loss: 0.8147448897361755\n",
      "179:Epoch: 11, Train_Loss: 0.8003410696983337, Test_Loss: 0.8345962166786194\n",
      "180:Epoch: 11, Train_Loss: 0.827468752861023, Test_Loss: 0.9641491770744324\n",
      "181:Epoch: 11, Train_Loss: 0.8214754462242126, Test_Loss: 0.8885028958320618 *\n",
      "182:Epoch: 11, Train_Loss: 0.8071999549865723, Test_Loss: 0.7848309278488159 *\n",
      "183:Epoch: 11, Train_Loss: 0.8043074011802673, Test_Loss: 0.892494797706604\n",
      "184:Epoch: 11, Train_Loss: 0.8127714991569519, Test_Loss: 0.858810305595398 *\n",
      "185:Epoch: 11, Train_Loss: 10.358445167541504, Test_Loss: 0.8144599795341492 *\n",
      "186:Epoch: 11, Train_Loss: 7.701075077056885, Test_Loss: 0.7965861558914185 *\n",
      "187:Epoch: 11, Train_Loss: 1.451322078704834, Test_Loss: 0.8056128025054932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188:Epoch: 11, Train_Loss: 2.3050265312194824, Test_Loss: 0.8473564982414246\n",
      "189:Epoch: 11, Train_Loss: 0.9806602001190186, Test_Loss: 0.8174029588699341 *\n",
      "190:Epoch: 11, Train_Loss: 0.8329502940177917, Test_Loss: 0.865571916103363\n",
      "191:Epoch: 11, Train_Loss: 1.1475646495819092, Test_Loss: 1.1076555252075195\n",
      "192:Epoch: 11, Train_Loss: 7.84584903717041, Test_Loss: 1.2009333372116089\n",
      "193:Epoch: 11, Train_Loss: 1.9150558710098267, Test_Loss: 0.8836306929588318 *\n",
      "194:Epoch: 11, Train_Loss: 0.8142838478088379, Test_Loss: 0.9753724932670593\n",
      "195:Epoch: 11, Train_Loss: 3.5402188301086426, Test_Loss: 0.974831223487854 *\n",
      "196:Epoch: 11, Train_Loss: 2.838367462158203, Test_Loss: 1.1516289710998535\n",
      "197:Epoch: 11, Train_Loss: 1.177651286125183, Test_Loss: 1.3594088554382324\n",
      "198:Epoch: 11, Train_Loss: 0.8148103952407837, Test_Loss: 1.0679526329040527 *\n",
      "199:Epoch: 11, Train_Loss: 0.8113760352134705, Test_Loss: 1.527848243713379\n",
      "200:Epoch: 11, Train_Loss: 0.8228772282600403, Test_Loss: 1.1815085411071777 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 11\n",
      "201:Epoch: 11, Train_Loss: 0.8484759330749512, Test_Loss: 1.3673160076141357\n",
      "202:Epoch: 11, Train_Loss: 0.7818482518196106, Test_Loss: 1.2679328918457031 *\n",
      "203:Epoch: 11, Train_Loss: 0.7603405117988586, Test_Loss: 1.2856601476669312\n",
      "204:Epoch: 11, Train_Loss: 0.7540955543518066, Test_Loss: 0.8858783841133118 *\n",
      "205:Epoch: 11, Train_Loss: 0.7832252979278564, Test_Loss: 0.9603043794631958\n",
      "206:Epoch: 11, Train_Loss: 0.809481143951416, Test_Loss: 0.9787530899047852\n",
      "207:Epoch: 11, Train_Loss: 0.80629962682724, Test_Loss: 0.9553760290145874 *\n",
      "208:Epoch: 11, Train_Loss: 0.9096192121505737, Test_Loss: 0.9120545387268066 *\n",
      "209:Epoch: 11, Train_Loss: 0.8240673542022705, Test_Loss: 0.8250890374183655 *\n",
      "210:Epoch: 11, Train_Loss: 0.8212589025497437, Test_Loss: 1.1474616527557373\n",
      "211:Epoch: 11, Train_Loss: 0.7668074369430542, Test_Loss: 7.382296562194824\n",
      "212:Epoch: 11, Train_Loss: 0.7880709767341614, Test_Loss: 0.9772290587425232 *\n",
      "213:Epoch: 11, Train_Loss: 0.784034252166748, Test_Loss: 0.9891843795776367\n",
      "214:Epoch: 11, Train_Loss: 0.7614086866378784, Test_Loss: 0.961057722568512 *\n",
      "215:Epoch: 11, Train_Loss: 0.7697513103485107, Test_Loss: 0.8556333780288696 *\n",
      "216:Epoch: 11, Train_Loss: 0.7612432837486267, Test_Loss: 0.896204948425293\n",
      "217:Epoch: 11, Train_Loss: 0.7577594518661499, Test_Loss: 1.0442783832550049\n",
      "218:Epoch: 11, Train_Loss: 0.7622719407081604, Test_Loss: 1.035221815109253 *\n",
      "219:Epoch: 11, Train_Loss: 0.7605628967285156, Test_Loss: 0.8177599906921387 *\n",
      "220:Epoch: 11, Train_Loss: 0.7551740407943726, Test_Loss: 0.9040130972862244\n",
      "221:Epoch: 11, Train_Loss: 0.7615242004394531, Test_Loss: 0.9058186411857605\n",
      "222:Epoch: 11, Train_Loss: 0.7802178859710693, Test_Loss: 1.0365536212921143\n",
      "223:Epoch: 11, Train_Loss: 0.7872722744941711, Test_Loss: 0.8489707112312317 *\n",
      "224:Epoch: 11, Train_Loss: 0.8599737286567688, Test_Loss: 0.9312894344329834\n",
      "225:Epoch: 11, Train_Loss: 0.761989951133728, Test_Loss: 0.9417926669120789\n",
      "226:Epoch: 11, Train_Loss: 0.7742394208908081, Test_Loss: 0.7574422955513 *\n",
      "227:Epoch: 11, Train_Loss: 9.234525680541992, Test_Loss: 0.7680655717849731\n",
      "228:Epoch: 11, Train_Loss: 1.1899181604385376, Test_Loss: 0.7664694786071777 *\n",
      "229:Epoch: 11, Train_Loss: 0.768417477607727, Test_Loss: 0.8734570741653442\n",
      "230:Epoch: 11, Train_Loss: 0.8222553730010986, Test_Loss: 0.7744799852371216 *\n",
      "231:Epoch: 11, Train_Loss: 0.8731216788291931, Test_Loss: 0.8373240232467651\n",
      "232:Epoch: 11, Train_Loss: 0.7604280710220337, Test_Loss: 0.8313214778900146 *\n",
      "233:Epoch: 11, Train_Loss: 0.7650148868560791, Test_Loss: 0.9057345390319824\n",
      "234:Epoch: 11, Train_Loss: 0.8446592688560486, Test_Loss: 0.8760435581207275 *\n",
      "235:Epoch: 11, Train_Loss: 0.9421666264533997, Test_Loss: 0.8229680061340332 *\n",
      "236:Epoch: 11, Train_Loss: 0.9001585245132446, Test_Loss: 0.7786129117012024 *\n",
      "237:Epoch: 11, Train_Loss: 0.8512020111083984, Test_Loss: 0.7980091571807861\n",
      "238:Epoch: 11, Train_Loss: 0.7512021660804749, Test_Loss: 0.7773251533508301 *\n",
      "239:Epoch: 11, Train_Loss: 0.8102664351463318, Test_Loss: 0.7683144211769104 *\n",
      "240:Epoch: 11, Train_Loss: 0.8191096782684326, Test_Loss: 0.8055087327957153\n",
      "241:Epoch: 11, Train_Loss: 0.8944971561431885, Test_Loss: 0.8156173825263977\n",
      "242:Epoch: 11, Train_Loss: 0.8254514932632446, Test_Loss: 3.2859630584716797\n",
      "243:Epoch: 11, Train_Loss: 0.820496678352356, Test_Loss: 3.7728209495544434\n",
      "244:Epoch: 11, Train_Loss: 0.7779334187507629, Test_Loss: 0.7491902112960815 *\n",
      "245:Epoch: 11, Train_Loss: 0.7685176134109497, Test_Loss: 0.7454574704170227 *\n",
      "246:Epoch: 11, Train_Loss: 0.8207831382751465, Test_Loss: 0.7924609780311584\n",
      "247:Epoch: 11, Train_Loss: 0.7803207635879517, Test_Loss: 0.7617528438568115 *\n",
      "248:Epoch: 11, Train_Loss: 0.7528372406959534, Test_Loss: 0.7854210734367371\n",
      "249:Epoch: 11, Train_Loss: 0.7616803646087646, Test_Loss: 0.7917545437812805\n",
      "250:Epoch: 11, Train_Loss: 0.7623980045318604, Test_Loss: 0.8478338718414307\n",
      "251:Epoch: 11, Train_Loss: 0.8092910647392273, Test_Loss: 0.7480809688568115 *\n",
      "252:Epoch: 11, Train_Loss: 6.2281599044799805, Test_Loss: 0.7655547261238098\n",
      "253:Epoch: 11, Train_Loss: 0.7686882615089417, Test_Loss: 0.7643189430236816 *\n",
      "254:Epoch: 11, Train_Loss: 0.7485224008560181, Test_Loss: 0.7623372673988342 *\n",
      "255:Epoch: 11, Train_Loss: 0.7830666899681091, Test_Loss: 0.7504702806472778 *\n",
      "256:Epoch: 11, Train_Loss: 0.7521664500236511, Test_Loss: 0.8022828102111816\n",
      "257:Epoch: 11, Train_Loss: 0.7462188601493835, Test_Loss: 0.7925783395767212 *\n",
      "258:Epoch: 11, Train_Loss: 0.7452194690704346, Test_Loss: 0.821695864200592\n",
      "259:Epoch: 11, Train_Loss: 0.7463549971580505, Test_Loss: 0.8427049517631531\n",
      "260:Epoch: 11, Train_Loss: 0.7845994234085083, Test_Loss: 0.7599700689315796 *\n",
      "261:Epoch: 11, Train_Loss: 0.755737841129303, Test_Loss: 0.7677873969078064\n",
      "262:Epoch: 11, Train_Loss: 0.7807007431983948, Test_Loss: 0.7451910972595215 *\n",
      "263:Epoch: 11, Train_Loss: 0.742554783821106, Test_Loss: 0.7440004944801331 *\n",
      "264:Epoch: 11, Train_Loss: 0.7409109473228455, Test_Loss: 0.744162380695343\n",
      "265:Epoch: 11, Train_Loss: 0.7542440295219421, Test_Loss: 0.7523183226585388\n",
      "266:Epoch: 11, Train_Loss: 0.742918848991394, Test_Loss: 0.7495322823524475 *\n",
      "267:Epoch: 11, Train_Loss: 0.7454569935798645, Test_Loss: 0.7445088624954224 *\n",
      "268:Epoch: 11, Train_Loss: 0.7557825446128845, Test_Loss: 0.7470568418502808\n",
      "269:Epoch: 11, Train_Loss: 0.7741044163703918, Test_Loss: 0.7419887185096741 *\n",
      "270:Epoch: 11, Train_Loss: 0.75655597448349, Test_Loss: 0.7426537275314331\n",
      "271:Epoch: 11, Train_Loss: 0.7394880652427673, Test_Loss: 0.759799063205719\n",
      "272:Epoch: 11, Train_Loss: 0.7389634847640991, Test_Loss: 0.7426641583442688 *\n",
      "273:Epoch: 11, Train_Loss: 0.8288393616676331, Test_Loss: 0.7579493522644043\n",
      "274:Epoch: 11, Train_Loss: 0.7845653295516968, Test_Loss: 0.772843062877655\n",
      "275:Epoch: 11, Train_Loss: 0.7777316570281982, Test_Loss: 1.069079875946045\n",
      "276:Epoch: 11, Train_Loss: 0.7564883828163147, Test_Loss: 1.0614577531814575 *\n",
      "277:Epoch: 11, Train_Loss: 0.8617706894874573, Test_Loss: 0.8483647704124451 *\n",
      "278:Epoch: 11, Train_Loss: 0.7913082242012024, Test_Loss: 0.746711790561676 *\n",
      "279:Epoch: 11, Train_Loss: 0.7912079095840454, Test_Loss: 0.7634272575378418\n",
      "280:Epoch: 11, Train_Loss: 0.7855954766273499, Test_Loss: 0.7740901708602905\n",
      "281:Epoch: 11, Train_Loss: 0.9342688918113708, Test_Loss: 0.9709903001785278\n",
      "282:Epoch: 11, Train_Loss: 0.7626489400863647, Test_Loss: 1.6621034145355225\n",
      "283:Epoch: 11, Train_Loss: 0.7521283626556396, Test_Loss: 1.2880464792251587 *\n",
      "284:Epoch: 11, Train_Loss: 0.7392663359642029, Test_Loss: 0.8108010292053223 *\n",
      "285:Epoch: 11, Train_Loss: 0.7363994717597961, Test_Loss: 0.7685950994491577 *\n",
      "286:Epoch: 11, Train_Loss: 0.7357028722763062, Test_Loss: 0.7436938285827637 *\n",
      "287:Epoch: 11, Train_Loss: 0.7361360788345337, Test_Loss: 0.7438820600509644\n",
      "288:Epoch: 11, Train_Loss: 0.7603551149368286, Test_Loss: 0.7477752566337585\n",
      "289:Epoch: 11, Train_Loss: 5.362889766693115, Test_Loss: 0.7492284178733826\n",
      "290:Epoch: 11, Train_Loss: 0.9336240887641907, Test_Loss: 0.7960327863693237\n",
      "291:Epoch: 11, Train_Loss: 0.738966703414917, Test_Loss: 0.737744927406311 *\n",
      "292:Epoch: 11, Train_Loss: 0.7435663342475891, Test_Loss: 0.78523188829422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293:Epoch: 11, Train_Loss: 0.7359868884086609, Test_Loss: 0.8619595170021057\n",
      "294:Epoch: 11, Train_Loss: 0.7344567179679871, Test_Loss: 1.1235634088516235\n",
      "295:Epoch: 11, Train_Loss: 0.734024703502655, Test_Loss: 1.0036699771881104 *\n",
      "296:Epoch: 11, Train_Loss: 0.7327890992164612, Test_Loss: 0.7627257704734802 *\n",
      "297:Epoch: 11, Train_Loss: 0.7326707243919373, Test_Loss: 0.7498188018798828 *\n",
      "298:Epoch: 11, Train_Loss: 0.7330856919288635, Test_Loss: 0.7496458888053894 *\n",
      "299:Epoch: 11, Train_Loss: 0.7834560871124268, Test_Loss: 0.749828040599823\n",
      "300:Epoch: 11, Train_Loss: 0.7930658459663391, Test_Loss: 0.7576910257339478\n",
      "Model saved at location ../Saver/model.ckpt at epoch 11\n",
      "301:Epoch: 11, Train_Loss: 0.8066468834877014, Test_Loss: 1.9402098655700684\n",
      "302:Epoch: 11, Train_Loss: 0.793432354927063, Test_Loss: 4.834312915802002\n",
      "303:Epoch: 11, Train_Loss: 0.734163224697113, Test_Loss: 0.7482406497001648 *\n",
      "304:Epoch: 11, Train_Loss: 0.8298686742782593, Test_Loss: 0.7369557619094849 *\n",
      "305:Epoch: 11, Train_Loss: 0.9598672389984131, Test_Loss: 0.7362948060035706 *\n",
      "306:Epoch: 11, Train_Loss: 0.9624898433685303, Test_Loss: 0.7430159449577332\n",
      "307:Epoch: 11, Train_Loss: 0.9507458209991455, Test_Loss: 0.7357926368713379 *\n",
      "308:Epoch: 11, Train_Loss: 0.7325392961502075, Test_Loss: 0.7357279658317566 *\n",
      "309:Epoch: 11, Train_Loss: 0.730498194694519, Test_Loss: 0.7359819412231445\n",
      "310:Epoch: 11, Train_Loss: 0.7308034300804138, Test_Loss: 0.7317544221878052 *\n",
      "311:Epoch: 11, Train_Loss: 0.7425563335418701, Test_Loss: 0.7346055507659912\n",
      "312:Epoch: 11, Train_Loss: 0.7462006211280823, Test_Loss: 0.7335900068283081 *\n",
      "313:Epoch: 11, Train_Loss: 0.743162989616394, Test_Loss: 0.7446497082710266\n",
      "314:Epoch: 11, Train_Loss: 0.736714243888855, Test_Loss: 0.7466006875038147\n",
      "315:Epoch: 11, Train_Loss: 0.7286187410354614, Test_Loss: 0.7405293583869934 *\n",
      "316:Epoch: 11, Train_Loss: 0.7346984148025513, Test_Loss: 0.7425177097320557\n",
      "317:Epoch: 11, Train_Loss: 0.7481079697608948, Test_Loss: 0.7287939190864563 *\n",
      "318:Epoch: 11, Train_Loss: 0.8968641757965088, Test_Loss: 0.7290538549423218\n",
      "319:Epoch: 11, Train_Loss: 0.9270423650741577, Test_Loss: 0.7290393114089966 *\n",
      "320:Epoch: 11, Train_Loss: 0.9416366219520569, Test_Loss: 0.7297150492668152\n",
      "321:Epoch: 11, Train_Loss: 0.7683478593826294, Test_Loss: 0.7290709018707275 *\n",
      "322:Epoch: 11, Train_Loss: 0.8630549907684326, Test_Loss: 0.7297421097755432\n",
      "323:Epoch: 11, Train_Loss: 0.856542706489563, Test_Loss: 0.729594349861145 *\n",
      "324:Epoch: 11, Train_Loss: 0.7821229100227356, Test_Loss: 0.7329406142234802\n",
      "325:Epoch: 11, Train_Loss: 0.8732315897941589, Test_Loss: 0.7300931215286255 *\n",
      "326:Epoch: 11, Train_Loss: 0.9277660846710205, Test_Loss: 0.7287234663963318 *\n",
      "327:Epoch: 11, Train_Loss: 0.8685800433158875, Test_Loss: 0.7279177308082581 *\n",
      "328:Epoch: 11, Train_Loss: 0.7387422919273376, Test_Loss: 0.7279845476150513\n",
      "329:Epoch: 11, Train_Loss: 2.647061586380005, Test_Loss: 0.7267057299613953 *\n",
      "330:Epoch: 11, Train_Loss: 1.954557180404663, Test_Loss: 0.7263555526733398 *\n",
      "331:Epoch: 11, Train_Loss: 0.762531578540802, Test_Loss: 0.7546296715736389\n",
      "332:Epoch: 11, Train_Loss: 0.7820675373077393, Test_Loss: 0.7687532901763916\n",
      "333:Epoch: 11, Train_Loss: 0.7861018776893616, Test_Loss: 4.189716339111328\n",
      "334:Epoch: 11, Train_Loss: 0.7655428647994995, Test_Loss: 2.6853365898132324 *\n",
      "335:Epoch: 11, Train_Loss: 0.7257475852966309, Test_Loss: 0.7256932854652405 *\n",
      "336:Epoch: 11, Train_Loss: 0.7494800686836243, Test_Loss: 0.7297913432121277\n",
      "337:Epoch: 11, Train_Loss: 0.8602155447006226, Test_Loss: 0.7901448011398315\n",
      "338:Epoch: 11, Train_Loss: 0.8175991773605347, Test_Loss: 0.7945781350135803\n",
      "339:Epoch: 11, Train_Loss: 0.8125439286231995, Test_Loss: 0.7548982501029968 *\n",
      "340:Epoch: 11, Train_Loss: 0.8091293573379517, Test_Loss: 0.7830601930618286\n",
      "341:Epoch: 11, Train_Loss: 0.7723609805107117, Test_Loss: 0.8050661087036133\n",
      "342:Epoch: 11, Train_Loss: 0.7523072957992554, Test_Loss: 0.7267177700996399 *\n",
      "343:Epoch: 11, Train_Loss: 0.7387869358062744, Test_Loss: 0.7509307861328125\n",
      "344:Epoch: 11, Train_Loss: 0.7536754608154297, Test_Loss: 0.7428938746452332 *\n",
      "345:Epoch: 11, Train_Loss: 0.7533160448074341, Test_Loss: 0.7384639382362366 *\n",
      "346:Epoch: 11, Train_Loss: 0.7298053503036499, Test_Loss: 0.7268540859222412 *\n",
      "347:Epoch: 11, Train_Loss: 0.7242854237556458, Test_Loss: 0.8519390821456909\n",
      "348:Epoch: 11, Train_Loss: 0.7673788666725159, Test_Loss: 0.7899481058120728 *\n",
      "349:Epoch: 11, Train_Loss: 0.7735817432403564, Test_Loss: 0.8031378388404846\n",
      "350:Epoch: 11, Train_Loss: 0.7347629070281982, Test_Loss: 0.7911914587020874 *\n",
      "351:Epoch: 11, Train_Loss: 0.7204222679138184, Test_Loss: 0.7593716979026794 *\n",
      "352:Epoch: 11, Train_Loss: 0.7207518815994263, Test_Loss: 0.7530587911605835 *\n",
      "353:Epoch: 11, Train_Loss: 0.720080554485321, Test_Loss: 0.7489391565322876 *\n",
      "354:Epoch: 11, Train_Loss: 0.720701277256012, Test_Loss: 0.7464600801467896 *\n",
      "355:Epoch: 11, Train_Loss: 0.7202077507972717, Test_Loss: 0.7503184676170349\n",
      "356:Epoch: 11, Train_Loss: 0.7212775945663452, Test_Loss: 0.7519314289093018\n",
      "357:Epoch: 11, Train_Loss: 0.7215571999549866, Test_Loss: 0.7506384253501892 *\n",
      "358:Epoch: 11, Train_Loss: 0.7197281718254089, Test_Loss: 0.7445865273475647 *\n",
      "359:Epoch: 11, Train_Loss: 0.7193837761878967, Test_Loss: 0.7584857940673828\n",
      "360:Epoch: 11, Train_Loss: 0.7218433022499084, Test_Loss: 0.7525416016578674 *\n",
      "361:Epoch: 11, Train_Loss: 0.7336781024932861, Test_Loss: 0.7435401082038879 *\n",
      "362:Epoch: 11, Train_Loss: 0.7342450022697449, Test_Loss: 0.7277160882949829 *\n",
      "363:Epoch: 11, Train_Loss: 0.7324414253234863, Test_Loss: 0.7718983292579651\n",
      "364:Epoch: 11, Train_Loss: 0.7437297105789185, Test_Loss: 0.7908586859703064\n",
      "365:Epoch: 11, Train_Loss: 0.7279989123344421, Test_Loss: 0.8433282375335693\n",
      "366:Epoch: 11, Train_Loss: 0.7225951552391052, Test_Loss: 1.3286501169204712\n",
      "367:Epoch: 11, Train_Loss: 0.7200649976730347, Test_Loss: 1.1715160608291626 *\n",
      "368:Epoch: 11, Train_Loss: 0.7268813252449036, Test_Loss: 0.8523797392845154 *\n",
      "369:Epoch: 11, Train_Loss: 0.7489102482795715, Test_Loss: 0.750051736831665 *\n",
      "370:Epoch: 11, Train_Loss: 0.7246513366699219, Test_Loss: 0.7354535460472107 *\n",
      "371:Epoch: 11, Train_Loss: 0.7203825116157532, Test_Loss: 0.7598708271980286\n",
      "372:Epoch: 11, Train_Loss: 0.7168348431587219, Test_Loss: 1.1154426336288452\n",
      "373:Epoch: 11, Train_Loss: 0.7311292886734009, Test_Loss: 1.7565686702728271\n",
      "374:Epoch: 11, Train_Loss: 0.7768340706825256, Test_Loss: 1.1504616737365723 *\n",
      "375:Epoch: 11, Train_Loss: 0.7695047855377197, Test_Loss: 0.8254328966140747 *\n",
      "376:Epoch: 11, Train_Loss: 0.7503455877304077, Test_Loss: 0.731440544128418 *\n",
      "377:Epoch: 11, Train_Loss: 0.7144296765327454, Test_Loss: 0.7199143171310425 *\n",
      "378:Epoch: 11, Train_Loss: 0.7751961946487427, Test_Loss: 0.7149222493171692 *\n",
      "379:Epoch: 11, Train_Loss: 0.7391334176063538, Test_Loss: 0.725542426109314\n",
      "380:Epoch: 11, Train_Loss: 0.7168349623680115, Test_Loss: 0.7379467487335205\n",
      "381:Epoch: 11, Train_Loss: 0.7290624380111694, Test_Loss: 0.7594895958900452\n",
      "382:Epoch: 11, Train_Loss: 0.7364867329597473, Test_Loss: 0.7156241536140442 *\n",
      "383:Epoch: 11, Train_Loss: 0.817907989025116, Test_Loss: 0.7989124059677124\n",
      "384:Epoch: 11, Train_Loss: 0.7919159531593323, Test_Loss: 0.9223139882087708\n",
      "385:Epoch: 11, Train_Loss: 0.7639767527580261, Test_Loss: 1.0092759132385254\n",
      "386:Epoch: 11, Train_Loss: 0.7301381826400757, Test_Loss: 0.9512861967086792 *\n",
      "387:Epoch: 11, Train_Loss: 0.7187249660491943, Test_Loss: 0.7309040427207947 *\n",
      "388:Epoch: 11, Train_Loss: 0.7321900725364685, Test_Loss: 0.727508008480072 *\n",
      "389:Epoch: 11, Train_Loss: 0.7136048078536987, Test_Loss: 0.7272161841392517 *\n",
      "390:Epoch: 11, Train_Loss: 0.7175485491752625, Test_Loss: 0.7269555330276489 *\n",
      "391:Epoch: 11, Train_Loss: 0.7345621585845947, Test_Loss: 0.7382634282112122\n",
      "392:Epoch: 11, Train_Loss: 0.7389551997184753, Test_Loss: 3.184558153152466\n",
      "393:Epoch: 11, Train_Loss: 0.8207513689994812, Test_Loss: 3.6245317459106445\n",
      "394:Epoch: 11, Train_Loss: 0.7134931683540344, Test_Loss: 0.7237024903297424 *\n",
      "395:Epoch: 11, Train_Loss: 0.7787801027297974, Test_Loss: 0.71527498960495 *\n",
      "396:Epoch: 11, Train_Loss: 0.723810076713562, Test_Loss: 0.7142915725708008 *\n",
      "397:Epoch: 11, Train_Loss: 0.7488084435462952, Test_Loss: 0.7237694263458252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398:Epoch: 11, Train_Loss: 0.77286696434021, Test_Loss: 0.7129395008087158 *\n",
      "399:Epoch: 11, Train_Loss: 0.9756302833557129, Test_Loss: 0.7170937657356262\n",
      "400:Epoch: 11, Train_Loss: 0.7274547219276428, Test_Loss: 0.7112873196601868 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 11\n",
      "401:Epoch: 11, Train_Loss: 0.7495066523551941, Test_Loss: 0.71123206615448 *\n",
      "402:Epoch: 11, Train_Loss: 0.7089595198631287, Test_Loss: 0.7138348817825317\n",
      "403:Epoch: 11, Train_Loss: 0.7087111473083496, Test_Loss: 0.7113741636276245 *\n",
      "404:Epoch: 11, Train_Loss: 0.7094312310218811, Test_Loss: 0.7200618386268616\n",
      "405:Epoch: 11, Train_Loss: 0.708166778087616, Test_Loss: 0.7348108887672424\n",
      "406:Epoch: 11, Train_Loss: 0.7223928570747375, Test_Loss: 0.7236613035202026 *\n",
      "407:Epoch: 11, Train_Loss: 0.7230414748191833, Test_Loss: 0.715233325958252 *\n",
      "408:Epoch: 11, Train_Loss: 0.7256836891174316, Test_Loss: 0.7079074382781982 *\n",
      "409:Epoch: 11, Train_Loss: 0.7151011824607849, Test_Loss: 0.707969605922699\n",
      "410:Epoch: 11, Train_Loss: 0.7214857339859009, Test_Loss: 0.7080190181732178\n",
      "411:Epoch: 11, Train_Loss: 0.7196024656295776, Test_Loss: 0.707358717918396 *\n",
      "412:Epoch: 11, Train_Loss: 0.7085769176483154, Test_Loss: 0.7082878947257996\n",
      "413:Epoch: 11, Train_Loss: 0.706063449382782, Test_Loss: 0.7063770890235901 *\n",
      "414:Epoch: 11, Train_Loss: 0.7290045022964478, Test_Loss: 0.7069260478019714\n",
      "415:Epoch: 11, Train_Loss: 0.7354234457015991, Test_Loss: 0.7090135216712952\n",
      "416:Epoch: 11, Train_Loss: 0.7412095665931702, Test_Loss: 0.706807017326355 *\n",
      "417:Epoch: 11, Train_Loss: 0.7054537534713745, Test_Loss: 0.7067951560020447 *\n",
      "418:Epoch: 11, Train_Loss: 0.7538762092590332, Test_Loss: 0.7056646347045898 *\n",
      "419:Epoch: 11, Train_Loss: 0.7587797045707703, Test_Loss: 0.705684244632721\n",
      "420:Epoch: 11, Train_Loss: 0.7439135313034058, Test_Loss: 0.7054665088653564 *\n",
      "421:Epoch: 11, Train_Loss: 0.7080206871032715, Test_Loss: 0.7052584886550903 *\n",
      "422:Epoch: 11, Train_Loss: 0.7411572933197021, Test_Loss: 0.7395135164260864\n",
      "423:Epoch: 11, Train_Loss: 0.7055020928382874, Test_Loss: 0.7420057654380798\n",
      "424:Epoch: 11, Train_Loss: 0.7233572602272034, Test_Loss: 5.378162384033203\n",
      "425:Epoch: 11, Train_Loss: 0.7098095417022705, Test_Loss: 1.4556941986083984 *\n",
      "426:Epoch: 11, Train_Loss: 0.7315768003463745, Test_Loss: 0.7044296860694885 *\n",
      "427:Epoch: 11, Train_Loss: 1.3078635931015015, Test_Loss: 0.7229951024055481\n",
      "428:Epoch: 11, Train_Loss: 4.62975549697876, Test_Loss: 0.7700860500335693\n",
      "429:Epoch: 11, Train_Loss: 1.691448450088501, Test_Loss: 0.7779668569564819\n",
      "430:Epoch: 11, Train_Loss: 0.7181548476219177, Test_Loss: 0.7146741151809692 *\n",
      "431:Epoch: 11, Train_Loss: 0.7063296437263489, Test_Loss: 0.7839234471321106\n",
      "432:Epoch: 11, Train_Loss: 0.8504912257194519, Test_Loss: 0.7741629481315613 *\n",
      "433:Epoch: 11, Train_Loss: 0.7807222604751587, Test_Loss: 0.7042202353477478 *\n",
      "434:Epoch: 11, Train_Loss: 0.7148115634918213, Test_Loss: 0.7423781752586365\n",
      "435:Epoch: 11, Train_Loss: 0.7015577554702759, Test_Loss: 0.717038094997406 *\n",
      "436:Epoch: 11, Train_Loss: 0.7606814503669739, Test_Loss: 0.7126251459121704 *\n",
      "437:Epoch: 11, Train_Loss: 0.7198494076728821, Test_Loss: 0.706965446472168 *\n",
      "438:Epoch: 11, Train_Loss: 0.7216314673423767, Test_Loss: 0.8451616764068604\n",
      "439:Epoch: 11, Train_Loss: 0.9510122537612915, Test_Loss: 0.7412957549095154 *\n",
      "440:Epoch: 11, Train_Loss: 2.0376172065734863, Test_Loss: 0.8074597120285034\n",
      "441:Epoch: 11, Train_Loss: 1.8795268535614014, Test_Loss: 0.7580569982528687 *\n",
      "442:Epoch: 11, Train_Loss: 0.7955319285392761, Test_Loss: 0.7438278794288635 *\n",
      "443:Epoch: 11, Train_Loss: 0.8050523400306702, Test_Loss: 0.7252746820449829 *\n",
      "444:Epoch: 11, Train_Loss: 2.9964563846588135, Test_Loss: 0.725990891456604\n",
      "445:Epoch: 11, Train_Loss: 1.8836334943771362, Test_Loss: 0.7250013947486877 *\n",
      "446:Epoch: 11, Train_Loss: 0.7551887035369873, Test_Loss: 0.7278364300727844\n",
      "447:Epoch: 11, Train_Loss: 0.7368262410163879, Test_Loss: 0.7297406196594238\n",
      "448:Epoch: 11, Train_Loss: 1.2480390071868896, Test_Loss: 0.7281980514526367 *\n",
      "449:Epoch: 11, Train_Loss: 2.1874566078186035, Test_Loss: 0.7178388833999634 *\n",
      "450:Epoch: 11, Train_Loss: 1.5457885265350342, Test_Loss: 0.7336156368255615\n",
      "451:Epoch: 11, Train_Loss: 0.7066848278045654, Test_Loss: 0.7215301990509033 *\n",
      "452:Epoch: 11, Train_Loss: 0.7131369113922119, Test_Loss: 0.7072170376777649 *\n",
      "453:Epoch: 11, Train_Loss: 1.0013998746871948, Test_Loss: 0.7085318565368652\n",
      "454:Epoch: 11, Train_Loss: 1.2219653129577637, Test_Loss: 0.7381827235221863\n",
      "1:Epoch: 12, Train_Loss: 0.7413846254348755, Test_Loss: 0.7260316014289856 *\n",
      "2:Epoch: 12, Train_Loss: 0.7720099687576294, Test_Loss: 0.9203526377677917\n",
      "3:Epoch: 12, Train_Loss: 0.8272167444229126, Test_Loss: 1.1773784160614014\n",
      "4:Epoch: 12, Train_Loss: 0.7798709869384766, Test_Loss: 1.020928978919983 *\n",
      "5:Epoch: 12, Train_Loss: 0.8006007075309753, Test_Loss: 0.7946377992630005 *\n",
      "6:Epoch: 12, Train_Loss: 0.9328054785728455, Test_Loss: 0.7143670320510864 *\n",
      "7:Epoch: 12, Train_Loss: 0.7868494391441345, Test_Loss: 0.7082734704017639 *\n",
      "8:Epoch: 12, Train_Loss: 0.7571347951889038, Test_Loss: 0.7383676171302795\n",
      "9:Epoch: 12, Train_Loss: 0.9275875091552734, Test_Loss: 1.0903820991516113\n",
      "10:Epoch: 12, Train_Loss: 0.8764451742172241, Test_Loss: 1.4249346256256104\n",
      "11:Epoch: 12, Train_Loss: 1.0428011417388916, Test_Loss: 0.9204118847846985 *\n",
      "12:Epoch: 12, Train_Loss: 0.9217666387557983, Test_Loss: 0.7829468846321106 *\n",
      "13:Epoch: 12, Train_Loss: 0.7336807250976562, Test_Loss: 0.7019672393798828 *\n",
      "14:Epoch: 12, Train_Loss: 0.793079674243927, Test_Loss: 0.7045554518699646\n",
      "15:Epoch: 12, Train_Loss: 0.7746032476425171, Test_Loss: 0.6986644864082336 *\n",
      "16:Epoch: 12, Train_Loss: 0.7171801924705505, Test_Loss: 0.7284424901008606\n",
      "17:Epoch: 12, Train_Loss: 0.6979065537452698, Test_Loss: 0.7042596936225891 *\n",
      "18:Epoch: 12, Train_Loss: 0.6938534379005432, Test_Loss: 0.7259379029273987\n",
      "19:Epoch: 12, Train_Loss: 0.6943523287773132, Test_Loss: 0.7041336297988892 *\n",
      "20:Epoch: 12, Train_Loss: 0.6998869776725769, Test_Loss: 0.8513866066932678\n",
      "21:Epoch: 12, Train_Loss: 0.7073049545288086, Test_Loss: 1.054129958152771\n",
      "22:Epoch: 12, Train_Loss: 0.7273263931274414, Test_Loss: 0.8686716556549072 *\n",
      "23:Epoch: 12, Train_Loss: 0.73293137550354, Test_Loss: 1.0574586391448975\n",
      "24:Epoch: 12, Train_Loss: 0.7731406688690186, Test_Loss: 0.7156365513801575 *\n",
      "25:Epoch: 12, Train_Loss: 0.8678687810897827, Test_Loss: 0.7047204971313477 *\n",
      "26:Epoch: 12, Train_Loss: 0.9482542276382446, Test_Loss: 0.703025758266449 *\n",
      "27:Epoch: 12, Train_Loss: 0.7280319333076477, Test_Loss: 0.7042788863182068\n",
      "28:Epoch: 12, Train_Loss: 0.7268590331077576, Test_Loss: 0.8007192015647888\n",
      "29:Epoch: 12, Train_Loss: 1.0638139247894287, Test_Loss: 4.601265907287598\n",
      "30:Epoch: 12, Train_Loss: 1.0113253593444824, Test_Loss: 2.227471113204956 *\n",
      "31:Epoch: 12, Train_Loss: 0.7156865000724792, Test_Loss: 0.7083595991134644 *\n",
      "32:Epoch: 12, Train_Loss: 0.7235908508300781, Test_Loss: 0.7021533846855164 *\n",
      "33:Epoch: 12, Train_Loss: 1.1169829368591309, Test_Loss: 0.7062686085700989\n",
      "34:Epoch: 12, Train_Loss: 1.0678327083587646, Test_Loss: 0.6982444524765015 *\n",
      "35:Epoch: 12, Train_Loss: 0.8570975065231323, Test_Loss: 0.7261998057365417\n",
      "36:Epoch: 12, Train_Loss: 0.7114371061325073, Test_Loss: 0.7646320462226868\n",
      "37:Epoch: 12, Train_Loss: 0.7369429469108582, Test_Loss: 0.7130807042121887 *\n",
      "38:Epoch: 12, Train_Loss: 0.9865328073501587, Test_Loss: 0.728285551071167\n",
      "39:Epoch: 12, Train_Loss: 1.8957719802856445, Test_Loss: 0.7401416301727295\n",
      "40:Epoch: 12, Train_Loss: 0.8897409439086914, Test_Loss: 0.7552193999290466\n",
      "41:Epoch: 12, Train_Loss: 0.7167158722877502, Test_Loss: 0.7688286900520325\n",
      "42:Epoch: 12, Train_Loss: 0.7012003660202026, Test_Loss: 0.7067201137542725 *\n",
      "43:Epoch: 12, Train_Loss: 0.6947596073150635, Test_Loss: 0.7455403208732605\n",
      "44:Epoch: 12, Train_Loss: 1.030273675918579, Test_Loss: 0.7535744309425354\n",
      "45:Epoch: 12, Train_Loss: 0.7656283378601074, Test_Loss: 0.7047126293182373 *\n",
      "46:Epoch: 12, Train_Loss: 0.7059400677680969, Test_Loss: 0.7026653289794922 *\n",
      "47:Epoch: 12, Train_Loss: 1.0051432847976685, Test_Loss: 0.7162278294563293\n",
      "48:Epoch: 12, Train_Loss: 0.7199351191520691, Test_Loss: 0.7183601260185242\n",
      "49:Epoch: 12, Train_Loss: 0.7064959406852722, Test_Loss: 0.7079563736915588 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50:Epoch: 12, Train_Loss: 0.762532114982605, Test_Loss: 0.7361422777175903\n",
      "51:Epoch: 12, Train_Loss: 0.889811098575592, Test_Loss: 0.7493588328361511\n",
      "52:Epoch: 12, Train_Loss: 0.7263612747192383, Test_Loss: 0.7480175495147705 *\n",
      "53:Epoch: 12, Train_Loss: 0.7699849605560303, Test_Loss: 0.7388847470283508 *\n",
      "54:Epoch: 12, Train_Loss: 0.6966031193733215, Test_Loss: 0.7180312275886536 *\n",
      "55:Epoch: 12, Train_Loss: 0.880415678024292, Test_Loss: 0.7080448269844055 *\n",
      "56:Epoch: 12, Train_Loss: 0.7560288310050964, Test_Loss: 0.7121400237083435\n",
      "57:Epoch: 12, Train_Loss: 0.705642580986023, Test_Loss: 0.709309995174408 *\n",
      "58:Epoch: 12, Train_Loss: 0.6960354447364807, Test_Loss: 0.7123848795890808\n",
      "59:Epoch: 12, Train_Loss: 0.7346875667572021, Test_Loss: 0.7795180678367615\n",
      "60:Epoch: 12, Train_Loss: 0.9518807530403137, Test_Loss: 0.760931670665741 *\n",
      "61:Epoch: 12, Train_Loss: 0.957378625869751, Test_Loss: 6.421154975891113\n",
      "62:Epoch: 12, Train_Loss: 0.9329898357391357, Test_Loss: 0.7410639524459839 *\n",
      "63:Epoch: 12, Train_Loss: 1.1221457719802856, Test_Loss: 0.7322660088539124 *\n",
      "64:Epoch: 12, Train_Loss: 0.9495664834976196, Test_Loss: 0.7371664643287659\n",
      "65:Epoch: 12, Train_Loss: 0.9261921644210815, Test_Loss: 0.6981559991836548 *\n",
      "66:Epoch: 12, Train_Loss: 0.7562940120697021, Test_Loss: 0.6996985673904419\n",
      "67:Epoch: 12, Train_Loss: 0.710215151309967, Test_Loss: 0.7024061679840088\n",
      "68:Epoch: 12, Train_Loss: 0.7059301137924194, Test_Loss: 0.7570961713790894\n",
      "69:Epoch: 12, Train_Loss: 0.6905779242515564, Test_Loss: 0.718819797039032 *\n",
      "70:Epoch: 12, Train_Loss: 0.8523843884468079, Test_Loss: 0.7021344900131226 *\n",
      "71:Epoch: 12, Train_Loss: 1.0753816366195679, Test_Loss: 0.7016834616661072 *\n",
      "72:Epoch: 12, Train_Loss: 0.971373438835144, Test_Loss: 0.787947952747345\n",
      "73:Epoch: 12, Train_Loss: 2.196989059448242, Test_Loss: 0.7303084135055542 *\n",
      "74:Epoch: 12, Train_Loss: 1.3094273805618286, Test_Loss: 0.7494125366210938\n",
      "75:Epoch: 12, Train_Loss: 1.1179020404815674, Test_Loss: 0.6847155690193176 *\n",
      "76:Epoch: 12, Train_Loss: 0.7925635576248169, Test_Loss: 0.7539348602294922\n",
      "77:Epoch: 12, Train_Loss: 0.6985531449317932, Test_Loss: 0.6905050277709961 *\n",
      "78:Epoch: 12, Train_Loss: 0.8928815126419067, Test_Loss: 0.7371051907539368\n",
      "79:Epoch: 12, Train_Loss: 1.5026946067810059, Test_Loss: 0.8262296915054321\n",
      "80:Epoch: 12, Train_Loss: 1.4059991836547852, Test_Loss: 0.6988771557807922 *\n",
      "81:Epoch: 12, Train_Loss: 0.7341820001602173, Test_Loss: 0.6964600086212158 *\n",
      "82:Epoch: 12, Train_Loss: 0.7707251310348511, Test_Loss: 0.7127259969711304\n",
      "83:Epoch: 12, Train_Loss: 0.7610875964164734, Test_Loss: 0.7301227450370789\n",
      "84:Epoch: 12, Train_Loss: 1.0130308866500854, Test_Loss: 0.7217512130737305 *\n",
      "85:Epoch: 12, Train_Loss: 0.8442649841308594, Test_Loss: 0.7090160250663757 *\n",
      "86:Epoch: 12, Train_Loss: 0.9499599933624268, Test_Loss: 0.7038392424583435 *\n",
      "87:Epoch: 12, Train_Loss: 0.8167906403541565, Test_Loss: 0.692599892616272 *\n",
      "88:Epoch: 12, Train_Loss: 0.7333299517631531, Test_Loss: 0.7217503786087036\n",
      "89:Epoch: 12, Train_Loss: 0.7216919660568237, Test_Loss: 0.7745574116706848\n",
      "90:Epoch: 12, Train_Loss: 0.7277506589889526, Test_Loss: 0.7870706915855408\n",
      "91:Epoch: 12, Train_Loss: 0.7155625224113464, Test_Loss: 0.6932640671730042 *\n",
      "92:Epoch: 12, Train_Loss: 0.7525094747543335, Test_Loss: 0.8146588206291199\n",
      "93:Epoch: 12, Train_Loss: 0.7273612022399902, Test_Loss: 0.7858279347419739 *\n",
      "94:Epoch: 12, Train_Loss: 0.8706706762313843, Test_Loss: 0.7582523822784424 *\n",
      "95:Epoch: 12, Train_Loss: 16.84206771850586, Test_Loss: 0.7382414937019348 *\n",
      "96:Epoch: 12, Train_Loss: 0.8127377033233643, Test_Loss: 0.7540463209152222\n",
      "97:Epoch: 12, Train_Loss: 2.1753523349761963, Test_Loss: 0.841577410697937\n",
      "98:Epoch: 12, Train_Loss: 1.4216687679290771, Test_Loss: 0.868521511554718\n",
      "99:Epoch: 12, Train_Loss: 0.7422420382499695, Test_Loss: 1.0014724731445312\n",
      "100:Epoch: 12, Train_Loss: 0.9342447519302368, Test_Loss: 0.9518595933914185 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 12\n",
      "101:Epoch: 12, Train_Loss: 5.4908766746521, Test_Loss: 0.8526517748832703 *\n",
      "102:Epoch: 12, Train_Loss: 3.126460313796997, Test_Loss: 1.2851133346557617\n",
      "103:Epoch: 12, Train_Loss: 0.8675185441970825, Test_Loss: 1.003682017326355 *\n",
      "104:Epoch: 12, Train_Loss: 1.4422898292541504, Test_Loss: 0.8634722232818604 *\n",
      "105:Epoch: 12, Train_Loss: 4.340312957763672, Test_Loss: 1.382677674293518\n",
      "106:Epoch: 12, Train_Loss: 1.2905707359313965, Test_Loss: 1.2794911861419678 *\n",
      "107:Epoch: 12, Train_Loss: 0.7911013960838318, Test_Loss: 1.3085918426513672\n",
      "108:Epoch: 12, Train_Loss: 0.7061096429824829, Test_Loss: 1.0423510074615479 *\n",
      "109:Epoch: 12, Train_Loss: 0.7772025465965271, Test_Loss: 1.1783154010772705\n",
      "110:Epoch: 12, Train_Loss: 0.7670690417289734, Test_Loss: 1.161541223526001 *\n",
      "111:Epoch: 12, Train_Loss: 0.7392848134040833, Test_Loss: 0.8711181879043579 *\n",
      "112:Epoch: 12, Train_Loss: 0.6981324553489685, Test_Loss: 1.318965196609497\n",
      "113:Epoch: 12, Train_Loss: 0.6760318875312805, Test_Loss: 0.7907432317733765 *\n",
      "114:Epoch: 12, Train_Loss: 0.6739250421524048, Test_Loss: 0.8410458564758301\n",
      "115:Epoch: 12, Train_Loss: 0.6855063438415527, Test_Loss: 0.6779250502586365 *\n",
      "116:Epoch: 12, Train_Loss: 0.6932557821273804, Test_Loss: 0.6757054328918457 *\n",
      "117:Epoch: 12, Train_Loss: 0.7468603849411011, Test_Loss: 0.67430180311203 *\n",
      "118:Epoch: 12, Train_Loss: 0.7193528413772583, Test_Loss: 0.6794148683547974\n",
      "119:Epoch: 12, Train_Loss: 0.7846252918243408, Test_Loss: 0.7399110198020935\n",
      "120:Epoch: 12, Train_Loss: 0.6823237538337708, Test_Loss: 7.248462200164795\n",
      "121:Epoch: 12, Train_Loss: 0.7111089825630188, Test_Loss: 1.5694551467895508 *\n",
      "122:Epoch: 12, Train_Loss: 0.7078392505645752, Test_Loss: 0.9135221838951111 *\n",
      "123:Epoch: 12, Train_Loss: 0.6762752532958984, Test_Loss: 0.864897608757019 *\n",
      "124:Epoch: 12, Train_Loss: 0.6734671592712402, Test_Loss: 0.8733516931533813\n",
      "125:Epoch: 12, Train_Loss: 0.6708176732063293, Test_Loss: 0.7197571396827698 *\n",
      "126:Epoch: 12, Train_Loss: 0.6705843806266785, Test_Loss: 1.02678382396698\n",
      "127:Epoch: 12, Train_Loss: 0.6702603101730347, Test_Loss: 1.0302989482879639\n",
      "128:Epoch: 12, Train_Loss: 0.6711103916168213, Test_Loss: 0.7984232306480408 *\n",
      "129:Epoch: 12, Train_Loss: 0.6700232625007629, Test_Loss: 0.8244069814682007\n",
      "130:Epoch: 12, Train_Loss: 0.6708792448043823, Test_Loss: 0.800574541091919 *\n",
      "131:Epoch: 12, Train_Loss: 0.6765340566635132, Test_Loss: 0.9334242343902588\n",
      "132:Epoch: 12, Train_Loss: 0.6866594552993774, Test_Loss: 0.8379337787628174 *\n",
      "133:Epoch: 12, Train_Loss: 0.7400484085083008, Test_Loss: 0.7954180836677551 *\n",
      "134:Epoch: 12, Train_Loss: 0.6845845580101013, Test_Loss: 0.8572855591773987\n",
      "135:Epoch: 12, Train_Loss: 0.7413774728775024, Test_Loss: 0.7174600958824158 *\n",
      "136:Epoch: 12, Train_Loss: 6.26173210144043, Test_Loss: 0.6778957843780518 *\n",
      "137:Epoch: 12, Train_Loss: 3.6985764503479004, Test_Loss: 0.7108108401298523\n",
      "138:Epoch: 12, Train_Loss: 0.7093600630760193, Test_Loss: 0.7651284337043762\n",
      "139:Epoch: 12, Train_Loss: 0.7728153467178345, Test_Loss: 0.7594875693321228 *\n",
      "140:Epoch: 12, Train_Loss: 0.854087233543396, Test_Loss: 0.7359771728515625 *\n",
      "141:Epoch: 12, Train_Loss: 0.7237052917480469, Test_Loss: 0.7700276374816895\n",
      "142:Epoch: 12, Train_Loss: 0.6891776919364929, Test_Loss: 0.8083033561706543\n",
      "143:Epoch: 12, Train_Loss: 0.7777782082557678, Test_Loss: 0.7790206670761108 *\n",
      "144:Epoch: 12, Train_Loss: 0.839649498462677, Test_Loss: 0.7711647152900696 *\n",
      "145:Epoch: 12, Train_Loss: 0.8996235728263855, Test_Loss: 0.7122387886047363 *\n",
      "146:Epoch: 12, Train_Loss: 0.7941126823425293, Test_Loss: 0.7177224159240723\n",
      "147:Epoch: 12, Train_Loss: 0.6951723098754883, Test_Loss: 0.7064118385314941 *\n",
      "148:Epoch: 12, Train_Loss: 0.7108808159828186, Test_Loss: 0.7097793221473694\n",
      "149:Epoch: 12, Train_Loss: 0.7512124180793762, Test_Loss: 0.7233001589775085\n",
      "150:Epoch: 12, Train_Loss: 0.801674485206604, Test_Loss: 0.7487358450889587\n",
      "151:Epoch: 12, Train_Loss: 0.7500871419906616, Test_Loss: 1.4096866846084595\n",
      "152:Epoch: 12, Train_Loss: 0.7345117330551147, Test_Loss: 5.733832359313965\n",
      "153:Epoch: 12, Train_Loss: 0.70927494764328, Test_Loss: 0.6813855171203613 *\n",
      "154:Epoch: 12, Train_Loss: 0.6755425930023193, Test_Loss: 0.665777325630188 *\n",
      "155:Epoch: 12, Train_Loss: 0.7155484557151794, Test_Loss: 0.6929025650024414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156:Epoch: 12, Train_Loss: 0.7055336236953735, Test_Loss: 0.6796368360519409 *\n",
      "157:Epoch: 12, Train_Loss: 0.6824830770492554, Test_Loss: 0.6903122067451477\n",
      "158:Epoch: 12, Train_Loss: 0.6718201637268066, Test_Loss: 0.6824547052383423 *\n",
      "159:Epoch: 12, Train_Loss: 0.6743661165237427, Test_Loss: 0.800633430480957\n",
      "160:Epoch: 12, Train_Loss: 0.6895338892936707, Test_Loss: 0.6939602494239807 *\n",
      "161:Epoch: 12, Train_Loss: 5.371835708618164, Test_Loss: 0.66717129945755 *\n",
      "162:Epoch: 12, Train_Loss: 1.7011888027191162, Test_Loss: 0.7049641013145447\n",
      "163:Epoch: 12, Train_Loss: 0.6673877239227295, Test_Loss: 0.6797265410423279 *\n",
      "164:Epoch: 12, Train_Loss: 0.6954194903373718, Test_Loss: 0.6722487807273865 *\n",
      "165:Epoch: 12, Train_Loss: 0.6761491298675537, Test_Loss: 0.7049484848976135\n",
      "166:Epoch: 12, Train_Loss: 0.6684907674789429, Test_Loss: 0.6984372138977051 *\n",
      "167:Epoch: 12, Train_Loss: 0.6679527759552002, Test_Loss: 0.7372264862060547\n",
      "168:Epoch: 12, Train_Loss: 0.6692517399787903, Test_Loss: 0.7718180418014526\n",
      "169:Epoch: 12, Train_Loss: 0.6904299855232239, Test_Loss: 0.6977146863937378 *\n",
      "170:Epoch: 12, Train_Loss: 0.6845561265945435, Test_Loss: 0.6938874125480652 *\n",
      "171:Epoch: 12, Train_Loss: 0.6881824731826782, Test_Loss: 0.6629287600517273 *\n",
      "172:Epoch: 12, Train_Loss: 0.6642667055130005, Test_Loss: 0.6614767909049988 *\n",
      "173:Epoch: 12, Train_Loss: 0.6626394391059875, Test_Loss: 0.6631003022193909\n",
      "174:Epoch: 12, Train_Loss: 0.6746083498001099, Test_Loss: 0.6657050251960754\n",
      "175:Epoch: 12, Train_Loss: 0.6634237766265869, Test_Loss: 0.6646032333374023 *\n",
      "176:Epoch: 12, Train_Loss: 0.6618502736091614, Test_Loss: 0.6642571687698364 *\n",
      "177:Epoch: 12, Train_Loss: 0.6696898341178894, Test_Loss: 0.6635217070579529 *\n",
      "178:Epoch: 12, Train_Loss: 0.6994648575782776, Test_Loss: 0.6625949740409851 *\n",
      "179:Epoch: 12, Train_Loss: 0.6873739361763, Test_Loss: 0.6640031337738037\n",
      "180:Epoch: 12, Train_Loss: 0.6598408818244934, Test_Loss: 0.6748013496398926\n",
      "181:Epoch: 12, Train_Loss: 0.6597331166267395, Test_Loss: 0.6633551716804504 *\n",
      "182:Epoch: 12, Train_Loss: 0.7195785045623779, Test_Loss: 0.6900706887245178\n",
      "183:Epoch: 12, Train_Loss: 0.7513177990913391, Test_Loss: 0.6816221475601196 *\n",
      "184:Epoch: 12, Train_Loss: 0.7024815678596497, Test_Loss: 0.9204948544502258\n",
      "185:Epoch: 12, Train_Loss: 0.7041271924972534, Test_Loss: 1.016113519668579\n",
      "186:Epoch: 12, Train_Loss: 0.7172267436981201, Test_Loss: 0.8098451495170593 *\n",
      "187:Epoch: 12, Train_Loss: 0.7421917915344238, Test_Loss: 0.6744679808616638 *\n",
      "188:Epoch: 12, Train_Loss: 0.7009248733520508, Test_Loss: 0.6725544929504395 *\n",
      "189:Epoch: 12, Train_Loss: 0.7103757262229919, Test_Loss: 0.6793623566627502\n",
      "190:Epoch: 12, Train_Loss: 0.7933570742607117, Test_Loss: 0.7766735553741455\n",
      "191:Epoch: 12, Train_Loss: 0.7084605693817139, Test_Loss: 1.3809447288513184\n",
      "192:Epoch: 12, Train_Loss: 0.6747746467590332, Test_Loss: 1.436061143875122\n",
      "193:Epoch: 12, Train_Loss: 0.6594299077987671, Test_Loss: 0.7150144577026367 *\n",
      "194:Epoch: 12, Train_Loss: 0.6569089293479919, Test_Loss: 0.712898850440979 *\n",
      "195:Epoch: 12, Train_Loss: 0.6564839482307434, Test_Loss: 0.6638668179512024 *\n",
      "196:Epoch: 12, Train_Loss: 0.6562135219573975, Test_Loss: 0.6757981777191162\n",
      "197:Epoch: 12, Train_Loss: 0.6573184728622437, Test_Loss: 0.6720811128616333 *\n",
      "198:Epoch: 12, Train_Loss: 4.798206806182861, Test_Loss: 0.6785115599632263\n",
      "199:Epoch: 12, Train_Loss: 1.5772595405578613, Test_Loss: 0.7145893573760986\n",
      "200:Epoch: 12, Train_Loss: 0.656710684299469, Test_Loss: 0.6733452677726746 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 12\n",
      "201:Epoch: 12, Train_Loss: 0.6668782234191895, Test_Loss: 0.6636782288551331 *\n",
      "202:Epoch: 12, Train_Loss: 0.6586534976959229, Test_Loss: 0.7777377367019653\n",
      "203:Epoch: 12, Train_Loss: 0.6556285619735718, Test_Loss: 1.0586087703704834\n",
      "204:Epoch: 12, Train_Loss: 0.6549932360649109, Test_Loss: 0.8786331415176392 *\n",
      "205:Epoch: 12, Train_Loss: 0.6544526219367981, Test_Loss: 0.7201179265975952 *\n",
      "206:Epoch: 12, Train_Loss: 0.6538478136062622, Test_Loss: 0.6679098606109619 *\n",
      "207:Epoch: 12, Train_Loss: 0.6542882919311523, Test_Loss: 0.6678826212882996 *\n",
      "208:Epoch: 12, Train_Loss: 0.6810091733932495, Test_Loss: 0.6683201193809509\n",
      "209:Epoch: 12, Train_Loss: 0.714942991733551, Test_Loss: 0.6703853607177734\n",
      "210:Epoch: 12, Train_Loss: 0.7177752256393433, Test_Loss: 0.7995771765708923\n",
      "211:Epoch: 12, Train_Loss: 0.7249207496643066, Test_Loss: 5.917629241943359\n",
      "212:Epoch: 12, Train_Loss: 0.6621697545051575, Test_Loss: 0.7661542892456055 *\n",
      "213:Epoch: 12, Train_Loss: 0.6926432847976685, Test_Loss: 0.6633226871490479 *\n",
      "214:Epoch: 12, Train_Loss: 0.8686292767524719, Test_Loss: 0.6562951803207397 *\n",
      "215:Epoch: 12, Train_Loss: 0.8710246086120605, Test_Loss: 0.658958911895752\n",
      "216:Epoch: 12, Train_Loss: 0.8750246167182922, Test_Loss: 0.6593724489212036\n",
      "217:Epoch: 12, Train_Loss: 0.6840532422065735, Test_Loss: 0.6549110412597656 *\n",
      "218:Epoch: 12, Train_Loss: 0.6517291069030762, Test_Loss: 0.6656626462936401\n",
      "219:Epoch: 12, Train_Loss: 0.6516358852386475, Test_Loss: 0.6546626687049866 *\n",
      "220:Epoch: 12, Train_Loss: 0.6578602194786072, Test_Loss: 0.6557255387306213\n",
      "221:Epoch: 12, Train_Loss: 0.6643350720405579, Test_Loss: 0.6591569781303406\n",
      "222:Epoch: 12, Train_Loss: 0.6640729308128357, Test_Loss: 0.6702848672866821\n",
      "223:Epoch: 12, Train_Loss: 0.6606923937797546, Test_Loss: 0.660565972328186 *\n",
      "224:Epoch: 12, Train_Loss: 0.6505768895149231, Test_Loss: 0.6605996489524841\n",
      "225:Epoch: 12, Train_Loss: 0.6513652801513672, Test_Loss: 0.6642187237739563\n",
      "226:Epoch: 12, Train_Loss: 0.667391836643219, Test_Loss: 0.6517595648765564 *\n",
      "227:Epoch: 12, Train_Loss: 0.7709718346595764, Test_Loss: 0.6512113213539124 *\n",
      "228:Epoch: 12, Train_Loss: 0.8426766395568848, Test_Loss: 0.6516390442848206\n",
      "229:Epoch: 12, Train_Loss: 0.8335973620414734, Test_Loss: 0.6555774807929993\n",
      "230:Epoch: 12, Train_Loss: 0.7250890135765076, Test_Loss: 0.6510171890258789 *\n",
      "231:Epoch: 12, Train_Loss: 0.7740730047225952, Test_Loss: 0.65601646900177\n",
      "232:Epoch: 12, Train_Loss: 0.7973403334617615, Test_Loss: 0.6532993912696838 *\n",
      "233:Epoch: 12, Train_Loss: 0.6584991812705994, Test_Loss: 0.6582740545272827\n",
      "234:Epoch: 12, Train_Loss: 0.8172504901885986, Test_Loss: 0.659410297870636\n",
      "235:Epoch: 12, Train_Loss: 0.7649651765823364, Test_Loss: 0.6546722650527954 *\n",
      "236:Epoch: 12, Train_Loss: 0.898209273815155, Test_Loss: 0.6512425541877747 *\n",
      "237:Epoch: 12, Train_Loss: 0.6583710312843323, Test_Loss: 0.6542789340019226\n",
      "238:Epoch: 12, Train_Loss: 1.5855770111083984, Test_Loss: 0.6512588262557983 *\n",
      "239:Epoch: 12, Train_Loss: 2.822665214538574, Test_Loss: 0.6513082385063171\n",
      "240:Epoch: 12, Train_Loss: 0.686770498752594, Test_Loss: 0.6599827408790588\n",
      "241:Epoch: 12, Train_Loss: 0.7134007811546326, Test_Loss: 0.7099258303642273\n",
      "242:Epoch: 12, Train_Loss: 0.6954876184463501, Test_Loss: 2.524517059326172\n",
      "243:Epoch: 12, Train_Loss: 0.6914936900138855, Test_Loss: 4.236937999725342\n",
      "244:Epoch: 12, Train_Loss: 0.6477289795875549, Test_Loss: 0.652143657207489 *\n",
      "245:Epoch: 12, Train_Loss: 0.6560079455375671, Test_Loss: 0.6466302871704102 *\n",
      "246:Epoch: 12, Train_Loss: 0.7707351446151733, Test_Loss: 0.6971947550773621\n",
      "247:Epoch: 12, Train_Loss: 0.7537193894386292, Test_Loss: 0.6860889196395874 *\n",
      "248:Epoch: 12, Train_Loss: 0.7434723973274231, Test_Loss: 0.6965751051902771\n",
      "249:Epoch: 12, Train_Loss: 0.7077656388282776, Test_Loss: 0.6795002818107605 *\n",
      "250:Epoch: 12, Train_Loss: 0.7025041580200195, Test_Loss: 0.7676788568496704\n",
      "251:Epoch: 12, Train_Loss: 0.6727983951568604, Test_Loss: 0.6526005864143372 *\n",
      "252:Epoch: 12, Train_Loss: 0.6705969572067261, Test_Loss: 0.6598913669586182\n",
      "253:Epoch: 12, Train_Loss: 0.6618731617927551, Test_Loss: 0.6689444184303284\n",
      "254:Epoch: 12, Train_Loss: 0.6761831045150757, Test_Loss: 0.6748194694519043\n",
      "255:Epoch: 12, Train_Loss: 0.6559739708900452, Test_Loss: 0.6518984436988831 *\n",
      "256:Epoch: 12, Train_Loss: 0.6453918814659119, Test_Loss: 0.718479335308075\n",
      "257:Epoch: 12, Train_Loss: 0.685296356678009, Test_Loss: 0.7221511602401733\n",
      "258:Epoch: 12, Train_Loss: 0.6982040405273438, Test_Loss: 0.7080166935920715 *\n",
      "259:Epoch: 12, Train_Loss: 0.66810542345047, Test_Loss: 0.7132951021194458\n",
      "260:Epoch: 12, Train_Loss: 0.64444500207901, Test_Loss: 0.6685786843299866 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261:Epoch: 12, Train_Loss: 0.6433464884757996, Test_Loss: 0.6935568451881409\n",
      "262:Epoch: 12, Train_Loss: 0.642738401889801, Test_Loss: 0.6618795394897461 *\n",
      "263:Epoch: 12, Train_Loss: 0.6427192091941833, Test_Loss: 0.6588760614395142 *\n",
      "264:Epoch: 12, Train_Loss: 0.6458385586738586, Test_Loss: 0.6625892519950867\n",
      "265:Epoch: 12, Train_Loss: 0.6459519267082214, Test_Loss: 0.6693233847618103\n",
      "266:Epoch: 12, Train_Loss: 0.6480551362037659, Test_Loss: 0.6651347279548645 *\n",
      "267:Epoch: 12, Train_Loss: 0.6424821615219116, Test_Loss: 0.6640743613243103 *\n",
      "268:Epoch: 12, Train_Loss: 0.6414192318916321, Test_Loss: 0.6660239100456238\n",
      "269:Epoch: 12, Train_Loss: 0.6495983004570007, Test_Loss: 0.6569599509239197 *\n",
      "270:Epoch: 12, Train_Loss: 0.6592430472373962, Test_Loss: 0.6583523154258728\n",
      "271:Epoch: 12, Train_Loss: 0.6633437275886536, Test_Loss: 0.6447519063949585 *\n",
      "272:Epoch: 12, Train_Loss: 0.6618189811706543, Test_Loss: 0.6619454622268677\n",
      "273:Epoch: 12, Train_Loss: 0.6700122356414795, Test_Loss: 0.7183587551116943\n",
      "274:Epoch: 12, Train_Loss: 0.6478110551834106, Test_Loss: 0.6547669172286987 *\n",
      "275:Epoch: 12, Train_Loss: 0.6570702791213989, Test_Loss: 1.1244215965270996\n",
      "276:Epoch: 12, Train_Loss: 0.643561601638794, Test_Loss: 1.1493701934814453\n",
      "277:Epoch: 12, Train_Loss: 0.6478385925292969, Test_Loss: 0.8116112947463989 *\n",
      "278:Epoch: 12, Train_Loss: 0.6711932420730591, Test_Loss: 0.6632803678512573 *\n",
      "279:Epoch: 12, Train_Loss: 0.6580687165260315, Test_Loss: 0.6639471054077148\n",
      "280:Epoch: 12, Train_Loss: 0.6455625891685486, Test_Loss: 0.6586223840713501 *\n",
      "281:Epoch: 12, Train_Loss: 0.6451205015182495, Test_Loss: 0.8068131804466248\n",
      "282:Epoch: 12, Train_Loss: 0.6477680802345276, Test_Loss: 1.4741612672805786\n",
      "283:Epoch: 12, Train_Loss: 0.6829588413238525, Test_Loss: 1.2369627952575684 *\n",
      "284:Epoch: 12, Train_Loss: 0.6788951754570007, Test_Loss: 0.7052258849143982 *\n",
      "285:Epoch: 12, Train_Loss: 0.6898000836372375, Test_Loss: 0.7030747532844543 *\n",
      "286:Epoch: 12, Train_Loss: 0.6384180784225464, Test_Loss: 0.6416194438934326 *\n",
      "287:Epoch: 12, Train_Loss: 0.6874448657035828, Test_Loss: 0.6414141058921814 *\n",
      "288:Epoch: 12, Train_Loss: 0.6854661703109741, Test_Loss: 0.6463761329650879\n",
      "289:Epoch: 12, Train_Loss: 0.6407781839370728, Test_Loss: 0.6488545536994934\n",
      "290:Epoch: 12, Train_Loss: 0.6474756598472595, Test_Loss: 0.697589635848999\n",
      "291:Epoch: 12, Train_Loss: 0.6639041304588318, Test_Loss: 0.6437745094299316 *\n",
      "292:Epoch: 12, Train_Loss: 0.7242968082427979, Test_Loss: 0.666057288646698\n",
      "293:Epoch: 12, Train_Loss: 0.7111525535583496, Test_Loss: 0.7714464068412781\n",
      "294:Epoch: 12, Train_Loss: 0.693187415599823, Test_Loss: 1.0265579223632812\n",
      "295:Epoch: 12, Train_Loss: 0.6653904914855957, Test_Loss: 0.9065548181533813 *\n",
      "296:Epoch: 12, Train_Loss: 0.6433188319206238, Test_Loss: 0.6585139036178589 *\n",
      "297:Epoch: 12, Train_Loss: 0.6564984917640686, Test_Loss: 0.6493005156517029 *\n",
      "298:Epoch: 12, Train_Loss: 0.6364304423332214, Test_Loss: 0.6488277912139893 *\n",
      "299:Epoch: 12, Train_Loss: 0.6409523487091064, Test_Loss: 0.6486356258392334 *\n",
      "300:Epoch: 12, Train_Loss: 0.6504857540130615, Test_Loss: 0.6559879183769226\n",
      "Model saved at location ../Saver/model.ckpt at epoch 12\n",
      "301:Epoch: 12, Train_Loss: 0.656905472278595, Test_Loss: 1.293389081954956\n",
      "302:Epoch: 12, Train_Loss: 0.7433778643608093, Test_Loss: 5.41835880279541\n",
      "303:Epoch: 12, Train_Loss: 0.6389732956886292, Test_Loss: 0.6672578454017639 *\n",
      "304:Epoch: 12, Train_Loss: 0.6997954845428467, Test_Loss: 0.6413730382919312 *\n",
      "305:Epoch: 12, Train_Loss: 0.6379016041755676, Test_Loss: 0.6384669542312622 *\n",
      "306:Epoch: 12, Train_Loss: 0.6631436347961426, Test_Loss: 0.6424847841262817\n",
      "307:Epoch: 12, Train_Loss: 0.6491851806640625, Test_Loss: 0.6388378143310547 *\n",
      "308:Epoch: 12, Train_Loss: 0.9219717979431152, Test_Loss: 0.6377324461936951 *\n",
      "309:Epoch: 12, Train_Loss: 0.6877395510673523, Test_Loss: 0.6419703364372253\n",
      "310:Epoch: 12, Train_Loss: 0.6707614660263062, Test_Loss: 0.6355294585227966 *\n",
      "311:Epoch: 12, Train_Loss: 0.6377415657043457, Test_Loss: 0.6373720169067383\n",
      "312:Epoch: 12, Train_Loss: 0.6330699920654297, Test_Loss: 0.6377620100975037\n",
      "313:Epoch: 12, Train_Loss: 0.6340510845184326, Test_Loss: 0.6481179594993591\n",
      "314:Epoch: 12, Train_Loss: 0.6332892775535583, Test_Loss: 0.6497464179992676\n",
      "315:Epoch: 12, Train_Loss: 0.6437692046165466, Test_Loss: 0.6445996165275574 *\n",
      "316:Epoch: 12, Train_Loss: 0.6440480351448059, Test_Loss: 0.6454291939735413\n",
      "317:Epoch: 12, Train_Loss: 0.6532963514328003, Test_Loss: 0.6327142715454102 *\n",
      "318:Epoch: 12, Train_Loss: 0.6415038704872131, Test_Loss: 0.6328911185264587\n",
      "319:Epoch: 12, Train_Loss: 0.6421859860420227, Test_Loss: 0.6331408619880676\n",
      "320:Epoch: 12, Train_Loss: 0.6490951776504517, Test_Loss: 0.6360101103782654\n",
      "321:Epoch: 12, Train_Loss: 0.6338137984275818, Test_Loss: 0.6318168044090271 *\n",
      "322:Epoch: 12, Train_Loss: 0.630481481552124, Test_Loss: 0.6345095038414001\n",
      "323:Epoch: 12, Train_Loss: 0.6427366137504578, Test_Loss: 0.6326301693916321 *\n",
      "324:Epoch: 12, Train_Loss: 0.6565808057785034, Test_Loss: 0.6371040344238281\n",
      "325:Epoch: 12, Train_Loss: 0.6668483018875122, Test_Loss: 0.6351832151412964 *\n",
      "326:Epoch: 12, Train_Loss: 0.631506085395813, Test_Loss: 0.6326289176940918 *\n",
      "327:Epoch: 12, Train_Loss: 0.6698628664016724, Test_Loss: 0.6311259865760803 *\n",
      "328:Epoch: 12, Train_Loss: 0.6937649250030518, Test_Loss: 0.6331616044044495\n",
      "329:Epoch: 12, Train_Loss: 0.6801812052726746, Test_Loss: 0.6317604780197144 *\n",
      "330:Epoch: 12, Train_Loss: 0.6297533512115479, Test_Loss: 0.6307774186134338 *\n",
      "331:Epoch: 12, Train_Loss: 0.6619523763656616, Test_Loss: 0.6488686203956604\n",
      "332:Epoch: 12, Train_Loss: 0.6332012414932251, Test_Loss: 0.6850559711456299\n",
      "333:Epoch: 12, Train_Loss: 0.6482382416725159, Test_Loss: 3.5965592861175537\n",
      "334:Epoch: 12, Train_Loss: 0.6298890113830566, Test_Loss: 3.135319709777832 *\n",
      "335:Epoch: 12, Train_Loss: 0.6555734872817993, Test_Loss: 0.6301037669181824 *\n",
      "336:Epoch: 12, Train_Loss: 0.6886734366416931, Test_Loss: 0.6293687224388123 *\n",
      "337:Epoch: 12, Train_Loss: 3.2662954330444336, Test_Loss: 0.6871756315231323\n",
      "338:Epoch: 12, Train_Loss: 3.358443260192871, Test_Loss: 0.673911452293396 *\n",
      "339:Epoch: 12, Train_Loss: 0.6450330018997192, Test_Loss: 0.6665236949920654 *\n",
      "340:Epoch: 12, Train_Loss: 0.6276154518127441, Test_Loss: 0.6854305267333984\n",
      "341:Epoch: 12, Train_Loss: 0.7436749339103699, Test_Loss: 0.7304226756095886\n",
      "342:Epoch: 12, Train_Loss: 0.7489715814590454, Test_Loss: 0.6300862431526184 *\n",
      "343:Epoch: 12, Train_Loss: 0.645919680595398, Test_Loss: 0.6519672870635986\n",
      "344:Epoch: 12, Train_Loss: 0.6268636584281921, Test_Loss: 0.6468132734298706 *\n",
      "345:Epoch: 12, Train_Loss: 0.6682685017585754, Test_Loss: 0.6455615758895874 *\n",
      "346:Epoch: 12, Train_Loss: 0.656852126121521, Test_Loss: 0.6324968338012695 *\n",
      "347:Epoch: 12, Train_Loss: 0.6403806209564209, Test_Loss: 0.7112724781036377\n",
      "348:Epoch: 12, Train_Loss: 0.7049751877784729, Test_Loss: 0.6865972876548767 *\n",
      "349:Epoch: 12, Train_Loss: 1.7758979797363281, Test_Loss: 0.7057263255119324\n",
      "350:Epoch: 12, Train_Loss: 2.0173585414886475, Test_Loss: 0.7033430933952332 *\n",
      "351:Epoch: 12, Train_Loss: 0.7597286701202393, Test_Loss: 0.6550734043121338 *\n",
      "352:Epoch: 12, Train_Loss: 0.6959303021430969, Test_Loss: 0.6601331233978271\n",
      "353:Epoch: 12, Train_Loss: 2.3097305297851562, Test_Loss: 0.6431411504745483 *\n",
      "354:Epoch: 12, Train_Loss: 2.2187464237213135, Test_Loss: 0.6388543844223022 *\n",
      "355:Epoch: 12, Train_Loss: 0.671796441078186, Test_Loss: 0.6441048979759216\n",
      "356:Epoch: 12, Train_Loss: 0.653560221195221, Test_Loss: 0.6512162685394287\n",
      "357:Epoch: 12, Train_Loss: 0.8651663661003113, Test_Loss: 0.6477518081665039 *\n",
      "358:Epoch: 12, Train_Loss: 1.9404630661010742, Test_Loss: 0.6384273171424866 *\n",
      "359:Epoch: 12, Train_Loss: 1.7662500143051147, Test_Loss: 0.6357568502426147 *\n",
      "360:Epoch: 12, Train_Loss: 0.6362473964691162, Test_Loss: 0.6258750557899475 *\n",
      "361:Epoch: 12, Train_Loss: 0.6518753170967102, Test_Loss: 0.6284781694412231\n",
      "362:Epoch: 12, Train_Loss: 0.7089093327522278, Test_Loss: 0.6389481425285339\n",
      "363:Epoch: 12, Train_Loss: 1.3375599384307861, Test_Loss: 0.6291562914848328 *\n",
      "364:Epoch: 12, Train_Loss: 0.6845971941947937, Test_Loss: 0.6516355276107788\n",
      "365:Epoch: 12, Train_Loss: 0.707996129989624, Test_Loss: 0.6686993837356567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366:Epoch: 12, Train_Loss: 0.6977691650390625, Test_Loss: 0.9978784322738647\n",
      "367:Epoch: 12, Train_Loss: 0.700310468673706, Test_Loss: 0.9872262477874756 *\n",
      "368:Epoch: 12, Train_Loss: 0.7529463171958923, Test_Loss: 0.7474870681762695 *\n",
      "369:Epoch: 12, Train_Loss: 0.7793481349945068, Test_Loss: 0.6300917863845825 *\n",
      "370:Epoch: 12, Train_Loss: 0.7322680354118347, Test_Loss: 0.6453734040260315\n",
      "371:Epoch: 12, Train_Loss: 0.6625785827636719, Test_Loss: 0.6502561569213867\n",
      "372:Epoch: 12, Train_Loss: 0.7704951763153076, Test_Loss: 0.8156337738037109\n",
      "373:Epoch: 12, Train_Loss: 0.7487946152687073, Test_Loss: 1.1711111068725586\n",
      "374:Epoch: 12, Train_Loss: 0.9190881252288818, Test_Loss: 0.9785759449005127 *\n",
      "375:Epoch: 12, Train_Loss: 0.8295317888259888, Test_Loss: 0.7170677781105042 *\n",
      "376:Epoch: 12, Train_Loss: 0.6496393084526062, Test_Loss: 0.6515785455703735 *\n",
      "377:Epoch: 12, Train_Loss: 0.7399585247039795, Test_Loss: 0.6396611332893372 *\n",
      "378:Epoch: 12, Train_Loss: 0.740952730178833, Test_Loss: 0.6356749534606934 *\n",
      "379:Epoch: 12, Train_Loss: 0.6517404913902283, Test_Loss: 0.6480061411857605\n",
      "380:Epoch: 12, Train_Loss: 0.627106249332428, Test_Loss: 0.6362399458885193 *\n",
      "381:Epoch: 12, Train_Loss: 0.6238659024238586, Test_Loss: 0.666699230670929\n",
      "382:Epoch: 12, Train_Loss: 0.6203929781913757, Test_Loss: 0.6612420082092285 *\n",
      "383:Epoch: 12, Train_Loss: 0.6211892366409302, Test_Loss: 0.7281652688980103\n",
      "384:Epoch: 12, Train_Loss: 0.6278892755508423, Test_Loss: 0.7524855136871338\n",
      "385:Epoch: 12, Train_Loss: 0.6478009819984436, Test_Loss: 1.0319284200668335\n",
      "386:Epoch: 12, Train_Loss: 0.6416506171226501, Test_Loss: 1.1099497079849243\n",
      "387:Epoch: 12, Train_Loss: 0.6888906359672546, Test_Loss: 0.6852335333824158 *\n",
      "388:Epoch: 12, Train_Loss: 0.7380939722061157, Test_Loss: 0.6566736698150635 *\n",
      "389:Epoch: 12, Train_Loss: 0.9425550699234009, Test_Loss: 0.653639554977417 *\n",
      "390:Epoch: 12, Train_Loss: 0.6385653018951416, Test_Loss: 0.6506909728050232 *\n",
      "391:Epoch: 12, Train_Loss: 0.6557072401046753, Test_Loss: 0.7546150088310242\n",
      "392:Epoch: 12, Train_Loss: 0.8581277132034302, Test_Loss: 2.2534267902374268\n",
      "393:Epoch: 12, Train_Loss: 0.8805981278419495, Test_Loss: 4.522418022155762\n",
      "394:Epoch: 12, Train_Loss: 0.7711459994316101, Test_Loss: 0.6462080478668213 *\n",
      "395:Epoch: 12, Train_Loss: 0.6542885303497314, Test_Loss: 0.6302018761634827 *\n",
      "396:Epoch: 12, Train_Loss: 0.9225392937660217, Test_Loss: 0.6432650685310364\n",
      "397:Epoch: 12, Train_Loss: 1.0136168003082275, Test_Loss: 0.625335693359375 *\n",
      "398:Epoch: 12, Train_Loss: 0.8076484203338623, Test_Loss: 0.6568081974983215\n",
      "399:Epoch: 12, Train_Loss: 0.6406826972961426, Test_Loss: 0.6793204545974731\n",
      "400:Epoch: 12, Train_Loss: 0.646304726600647, Test_Loss: 0.6624720692634583 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 12\n",
      "401:Epoch: 12, Train_Loss: 0.6977472901344299, Test_Loss: 0.6394270658493042 *\n",
      "402:Epoch: 12, Train_Loss: 1.6581993103027344, Test_Loss: 0.659712553024292\n",
      "403:Epoch: 12, Train_Loss: 1.0701693296432495, Test_Loss: 0.6684103608131409\n",
      "404:Epoch: 12, Train_Loss: 0.632086455821991, Test_Loss: 0.716413140296936\n",
      "405:Epoch: 12, Train_Loss: 0.6448474526405334, Test_Loss: 0.6299154758453369 *\n",
      "406:Epoch: 12, Train_Loss: 0.6210153102874756, Test_Loss: 0.6524177193641663\n",
      "407:Epoch: 12, Train_Loss: 0.8270139694213867, Test_Loss: 0.6806538105010986\n",
      "408:Epoch: 12, Train_Loss: 0.8578581809997559, Test_Loss: 0.6280766725540161 *\n",
      "409:Epoch: 12, Train_Loss: 0.623343288898468, Test_Loss: 0.6304206252098083\n",
      "410:Epoch: 12, Train_Loss: 0.8930438756942749, Test_Loss: 0.6526229381561279\n",
      "411:Epoch: 12, Train_Loss: 0.6283764839172363, Test_Loss: 0.6776375770568848\n",
      "412:Epoch: 12, Train_Loss: 0.6302410364151001, Test_Loss: 0.628320038318634 *\n",
      "413:Epoch: 12, Train_Loss: 0.6736972332000732, Test_Loss: 0.6530631184577942\n",
      "414:Epoch: 12, Train_Loss: 0.7819839715957642, Test_Loss: 0.6526476740837097 *\n",
      "415:Epoch: 12, Train_Loss: 0.6693321466445923, Test_Loss: 0.6958359479904175\n",
      "416:Epoch: 12, Train_Loss: 0.737098217010498, Test_Loss: 0.6867207884788513 *\n",
      "417:Epoch: 12, Train_Loss: 0.615685760974884, Test_Loss: 0.6602318286895752 *\n",
      "418:Epoch: 12, Train_Loss: 0.754317581653595, Test_Loss: 0.6246960163116455 *\n",
      "419:Epoch: 12, Train_Loss: 0.6532031297683716, Test_Loss: 0.6552867293357849\n",
      "420:Epoch: 12, Train_Loss: 0.6508162021636963, Test_Loss: 0.6494765877723694 *\n",
      "421:Epoch: 12, Train_Loss: 0.6281368732452393, Test_Loss: 0.6388139128684998 *\n",
      "422:Epoch: 12, Train_Loss: 0.6483119130134583, Test_Loss: 0.7425938248634338\n",
      "423:Epoch: 12, Train_Loss: 0.6927008628845215, Test_Loss: 0.7163529396057129 *\n",
      "424:Epoch: 12, Train_Loss: 0.8432387709617615, Test_Loss: 4.934214115142822\n",
      "425:Epoch: 12, Train_Loss: 0.7827582359313965, Test_Loss: 1.959756851196289 *\n",
      "426:Epoch: 12, Train_Loss: 1.0200843811035156, Test_Loss: 0.6611664891242981 *\n",
      "427:Epoch: 12, Train_Loss: 0.9126739501953125, Test_Loss: 0.6594025492668152 *\n",
      "428:Epoch: 12, Train_Loss: 0.8227363228797913, Test_Loss: 0.6433740258216858 *\n",
      "429:Epoch: 12, Train_Loss: 0.6991558074951172, Test_Loss: 0.6169962286949158 *\n",
      "430:Epoch: 12, Train_Loss: 0.6563037037849426, Test_Loss: 0.6284682750701904\n",
      "431:Epoch: 12, Train_Loss: 0.6388435363769531, Test_Loss: 0.693683922290802\n",
      "432:Epoch: 12, Train_Loss: 0.6192672252655029, Test_Loss: 0.6565369367599487 *\n",
      "433:Epoch: 12, Train_Loss: 0.6908584237098694, Test_Loss: 0.6298041939735413 *\n",
      "434:Epoch: 12, Train_Loss: 0.9757489562034607, Test_Loss: 0.6505516767501831\n",
      "435:Epoch: 12, Train_Loss: 0.9641308188438416, Test_Loss: 0.7114272117614746\n",
      "436:Epoch: 12, Train_Loss: 1.901413083076477, Test_Loss: 0.7371817827224731\n",
      "437:Epoch: 12, Train_Loss: 1.501930832862854, Test_Loss: 0.665667712688446 *\n",
      "438:Epoch: 12, Train_Loss: 1.0288763046264648, Test_Loss: 0.6251955628395081 *\n",
      "439:Epoch: 12, Train_Loss: 0.8057056665420532, Test_Loss: 0.6777434349060059\n",
      "440:Epoch: 12, Train_Loss: 0.6228851675987244, Test_Loss: 0.6413459777832031 *\n",
      "441:Epoch: 12, Train_Loss: 0.7131749391555786, Test_Loss: 0.6373974680900574 *\n",
      "442:Epoch: 12, Train_Loss: 1.224992275238037, Test_Loss: 0.7753632664680481\n",
      "443:Epoch: 12, Train_Loss: 1.5163929462432861, Test_Loss: 0.6794918179512024 *\n",
      "444:Epoch: 12, Train_Loss: 0.6391260027885437, Test_Loss: 0.6307189464569092 *\n",
      "445:Epoch: 12, Train_Loss: 0.6747471690177917, Test_Loss: 0.6462547183036804\n",
      "446:Epoch: 12, Train_Loss: 0.703230619430542, Test_Loss: 0.6480410099029541\n",
      "447:Epoch: 12, Train_Loss: 0.8334200382232666, Test_Loss: 0.6466192603111267 *\n",
      "448:Epoch: 12, Train_Loss: 0.7491384148597717, Test_Loss: 0.6531840562820435\n",
      "449:Epoch: 12, Train_Loss: 0.8954210877418518, Test_Loss: 0.651187002658844 *\n",
      "450:Epoch: 12, Train_Loss: 0.7773822546005249, Test_Loss: 0.634623646736145 *\n",
      "451:Epoch: 12, Train_Loss: 0.8311507701873779, Test_Loss: 0.6559013724327087\n",
      "452:Epoch: 12, Train_Loss: 0.640185534954071, Test_Loss: 0.6527136564254761 *\n",
      "453:Epoch: 12, Train_Loss: 0.6374337077140808, Test_Loss: 0.7108510732650757\n",
      "454:Epoch: 12, Train_Loss: 0.6329705119132996, Test_Loss: 0.6309666037559509 *\n",
      "1:Epoch: 13, Train_Loss: 0.694471001625061, Test_Loss: 0.6524746417999268 *\n",
      "2:Epoch: 13, Train_Loss: 0.6364045739173889, Test_Loss: 0.7179412245750427\n",
      "3:Epoch: 13, Train_Loss: 0.6567209959030151, Test_Loss: 0.7268483638763428\n",
      "4:Epoch: 13, Train_Loss: 16.448156356811523, Test_Loss: 0.7820744514465332\n",
      "5:Epoch: 13, Train_Loss: 0.676295816898346, Test_Loss: 0.6403840780258179 *\n",
      "6:Epoch: 13, Train_Loss: 2.0786995887756348, Test_Loss: 0.6948502063751221\n",
      "7:Epoch: 13, Train_Loss: 1.6157033443450928, Test_Loss: 0.7021290063858032\n",
      "8:Epoch: 13, Train_Loss: 0.6311607360839844, Test_Loss: 0.7445614337921143\n",
      "9:Epoch: 13, Train_Loss: 0.7479065656661987, Test_Loss: 0.8706344366073608\n",
      "10:Epoch: 13, Train_Loss: 3.500678062438965, Test_Loss: 0.8248398900032043 *\n",
      "11:Epoch: 13, Train_Loss: 5.183011054992676, Test_Loss: 1.1204078197479248\n",
      "12:Epoch: 13, Train_Loss: 0.6480203866958618, Test_Loss: 0.9842321872711182 *\n",
      "13:Epoch: 13, Train_Loss: 0.9134764075279236, Test_Loss: 0.8389344811439514 *\n",
      "14:Epoch: 13, Train_Loss: 3.7269532680511475, Test_Loss: 1.6471405029296875\n",
      "15:Epoch: 13, Train_Loss: 1.5280590057373047, Test_Loss: 1.814399242401123\n",
      "16:Epoch: 13, Train_Loss: 1.126585602760315, Test_Loss: 1.7131291627883911 *\n",
      "17:Epoch: 13, Train_Loss: 0.6265078186988831, Test_Loss: 1.4037160873413086 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:Epoch: 13, Train_Loss: 0.7464818954467773, Test_Loss: 1.9690985679626465\n",
      "19:Epoch: 13, Train_Loss: 0.7485113143920898, Test_Loss: 1.3362064361572266 *\n",
      "20:Epoch: 13, Train_Loss: 0.6050006151199341, Test_Loss: 0.9558945298194885 *\n",
      "21:Epoch: 13, Train_Loss: 0.6215478181838989, Test_Loss: 1.4868450164794922\n",
      "22:Epoch: 13, Train_Loss: 0.6092150211334229, Test_Loss: 0.7773258686065674 *\n",
      "23:Epoch: 13, Train_Loss: 0.6160329580307007, Test_Loss: 0.8487789034843445\n",
      "24:Epoch: 13, Train_Loss: 0.625710666179657, Test_Loss: 0.6105031371116638 *\n",
      "25:Epoch: 13, Train_Loss: 0.6333268880844116, Test_Loss: 0.6013753414154053 *\n",
      "26:Epoch: 13, Train_Loss: 0.7116692066192627, Test_Loss: 0.6016674637794495\n",
      "27:Epoch: 13, Train_Loss: 0.6608296036720276, Test_Loss: 0.6073967218399048\n",
      "28:Epoch: 13, Train_Loss: 0.7333192229270935, Test_Loss: 0.7855849862098694\n",
      "29:Epoch: 13, Train_Loss: 0.6104217171669006, Test_Loss: 4.14351749420166\n",
      "30:Epoch: 13, Train_Loss: 0.6359983682632446, Test_Loss: 3.3937902450561523 *\n",
      "31:Epoch: 13, Train_Loss: 0.6211097836494446, Test_Loss: 0.7522943019866943 *\n",
      "32:Epoch: 13, Train_Loss: 0.6263601183891296, Test_Loss: 0.7607314586639404\n",
      "33:Epoch: 13, Train_Loss: 0.6076830625534058, Test_Loss: 0.8239556550979614\n",
      "34:Epoch: 13, Train_Loss: 0.6043597459793091, Test_Loss: 0.6248290538787842 *\n",
      "35:Epoch: 13, Train_Loss: 0.6014798283576965, Test_Loss: 0.9016842246055603\n",
      "36:Epoch: 13, Train_Loss: 0.6000849008560181, Test_Loss: 0.9553021192550659\n",
      "37:Epoch: 13, Train_Loss: 0.6029921770095825, Test_Loss: 0.7746065855026245 *\n",
      "38:Epoch: 13, Train_Loss: 0.5997927188873291, Test_Loss: 0.7073383331298828 *\n",
      "39:Epoch: 13, Train_Loss: 0.6000909209251404, Test_Loss: 0.7283627986907959\n",
      "40:Epoch: 13, Train_Loss: 0.6128948330879211, Test_Loss: 0.767682671546936\n",
      "41:Epoch: 13, Train_Loss: 0.6196268200874329, Test_Loss: 0.8700844049453735\n",
      "42:Epoch: 13, Train_Loss: 0.667287290096283, Test_Loss: 0.669135332107544 *\n",
      "43:Epoch: 13, Train_Loss: 0.6193526983261108, Test_Loss: 0.7423115968704224\n",
      "44:Epoch: 13, Train_Loss: 0.6462762355804443, Test_Loss: 0.7181801199913025 *\n",
      "45:Epoch: 13, Train_Loss: 2.671722412109375, Test_Loss: 0.6007532477378845 *\n",
      "46:Epoch: 13, Train_Loss: 7.007380485534668, Test_Loss: 0.6279508471488953\n",
      "47:Epoch: 13, Train_Loss: 0.6163280010223389, Test_Loss: 0.6504275798797607\n",
      "48:Epoch: 13, Train_Loss: 0.6578711271286011, Test_Loss: 0.7110562324523926\n",
      "49:Epoch: 13, Train_Loss: 0.7273095846176147, Test_Loss: 0.6416006684303284 *\n",
      "50:Epoch: 13, Train_Loss: 0.6925065517425537, Test_Loss: 0.6742966175079346\n",
      "51:Epoch: 13, Train_Loss: 0.6121997237205505, Test_Loss: 0.7045608758926392\n",
      "52:Epoch: 13, Train_Loss: 0.6648829579353333, Test_Loss: 0.7161712646484375\n",
      "53:Epoch: 13, Train_Loss: 0.7459174394607544, Test_Loss: 0.7072707414627075 *\n",
      "54:Epoch: 13, Train_Loss: 0.8674657344818115, Test_Loss: 0.6745762228965759 *\n",
      "55:Epoch: 13, Train_Loss: 0.71897953748703, Test_Loss: 0.6367579698562622 *\n",
      "56:Epoch: 13, Train_Loss: 0.6404013633728027, Test_Loss: 0.6314405798912048 *\n",
      "57:Epoch: 13, Train_Loss: 0.6183743476867676, Test_Loss: 0.6380669474601746\n",
      "58:Epoch: 13, Train_Loss: 0.6753171682357788, Test_Loss: 0.6437397599220276\n",
      "59:Epoch: 13, Train_Loss: 0.6948091983795166, Test_Loss: 0.7031760215759277\n",
      "60:Epoch: 13, Train_Loss: 0.7010594010353088, Test_Loss: 0.627044141292572 *\n",
      "61:Epoch: 13, Train_Loss: 0.6484871506690979, Test_Loss: 6.344168663024902\n",
      "62:Epoch: 13, Train_Loss: 0.643796980381012, Test_Loss: 0.9775882959365845 *\n",
      "63:Epoch: 13, Train_Loss: 0.5991610884666443, Test_Loss: 0.5966314673423767 *\n",
      "64:Epoch: 13, Train_Loss: 0.6223196387290955, Test_Loss: 0.6126307845115662\n",
      "65:Epoch: 13, Train_Loss: 0.6393646597862244, Test_Loss: 0.6064367294311523 *\n",
      "66:Epoch: 13, Train_Loss: 0.6144596934318542, Test_Loss: 0.6111418604850769\n",
      "67:Epoch: 13, Train_Loss: 0.604985773563385, Test_Loss: 0.6048316955566406 *\n",
      "68:Epoch: 13, Train_Loss: 0.6078065037727356, Test_Loss: 0.7283469438552856\n",
      "69:Epoch: 13, Train_Loss: 0.6062731146812439, Test_Loss: 0.6621071100234985 *\n",
      "70:Epoch: 13, Train_Loss: 2.8103575706481934, Test_Loss: 0.5949468016624451 *\n",
      "71:Epoch: 13, Train_Loss: 4.1284918785095215, Test_Loss: 0.6290659308433533\n",
      "72:Epoch: 13, Train_Loss: 0.5944815278053284, Test_Loss: 0.6104391813278198 *\n",
      "73:Epoch: 13, Train_Loss: 0.618005096912384, Test_Loss: 0.5999903082847595 *\n",
      "74:Epoch: 13, Train_Loss: 0.6146758794784546, Test_Loss: 0.6058703064918518\n",
      "75:Epoch: 13, Train_Loss: 0.5983588099479675, Test_Loss: 0.6262896060943604\n",
      "76:Epoch: 13, Train_Loss: 0.6035656332969666, Test_Loss: 0.6382330060005188\n",
      "77:Epoch: 13, Train_Loss: 0.5999082922935486, Test_Loss: 0.6934588551521301\n",
      "78:Epoch: 13, Train_Loss: 0.6084445118904114, Test_Loss: 0.6530925631523132 *\n",
      "79:Epoch: 13, Train_Loss: 0.6220822334289551, Test_Loss: 0.6231409311294556 *\n",
      "80:Epoch: 13, Train_Loss: 0.6173561811447144, Test_Loss: 0.5951559543609619 *\n",
      "81:Epoch: 13, Train_Loss: 0.600898265838623, Test_Loss: 0.5927175283432007 *\n",
      "82:Epoch: 13, Train_Loss: 0.5926032662391663, Test_Loss: 0.5923109650611877 *\n",
      "83:Epoch: 13, Train_Loss: 0.5933210849761963, Test_Loss: 0.5924650430679321\n",
      "84:Epoch: 13, Train_Loss: 0.6064153909683228, Test_Loss: 0.5931692123413086\n",
      "85:Epoch: 13, Train_Loss: 0.5957823395729065, Test_Loss: 0.5931154489517212 *\n",
      "86:Epoch: 13, Train_Loss: 0.5940971970558167, Test_Loss: 0.5937548875808716\n",
      "87:Epoch: 13, Train_Loss: 0.6231294274330139, Test_Loss: 0.593815267086029\n",
      "88:Epoch: 13, Train_Loss: 0.6270914673805237, Test_Loss: 0.5926536321640015 *\n",
      "89:Epoch: 13, Train_Loss: 0.59239262342453, Test_Loss: 0.5997166037559509\n",
      "90:Epoch: 13, Train_Loss: 0.5900937914848328, Test_Loss: 0.6059772372245789\n",
      "91:Epoch: 13, Train_Loss: 0.6351953744888306, Test_Loss: 0.6086307764053345\n",
      "92:Epoch: 13, Train_Loss: 0.6952618956565857, Test_Loss: 0.6067770719528198 *\n",
      "93:Epoch: 13, Train_Loss: 0.6312731504440308, Test_Loss: 0.773922324180603\n",
      "94:Epoch: 13, Train_Loss: 0.6432467103004456, Test_Loss: 0.930018424987793\n",
      "95:Epoch: 13, Train_Loss: 0.6350553035736084, Test_Loss: 0.774205207824707 *\n",
      "96:Epoch: 13, Train_Loss: 0.6888121366500854, Test_Loss: 0.6332248449325562 *\n",
      "97:Epoch: 13, Train_Loss: 0.6307094097137451, Test_Loss: 0.594559371471405 *\n",
      "98:Epoch: 13, Train_Loss: 0.6655701994895935, Test_Loss: 0.6103624701499939\n",
      "99:Epoch: 13, Train_Loss: 0.6653438210487366, Test_Loss: 0.7000815868377686\n",
      "100:Epoch: 13, Train_Loss: 0.6874059438705444, Test_Loss: 1.0820953845977783\n",
      "Model saved at location ../Saver/model.ckpt at epoch 13\n",
      "101:Epoch: 13, Train_Loss: 0.6086081266403198, Test_Loss: 1.3029124736785889\n",
      "102:Epoch: 13, Train_Loss: 0.5914680361747742, Test_Loss: 0.7860312461853027 *\n",
      "103:Epoch: 13, Train_Loss: 0.5905020236968994, Test_Loss: 0.6536265015602112 *\n",
      "104:Epoch: 13, Train_Loss: 0.5878787636756897, Test_Loss: 0.5918170213699341 *\n",
      "105:Epoch: 13, Train_Loss: 0.5868777632713318, Test_Loss: 0.6183474659919739\n",
      "106:Epoch: 13, Train_Loss: 0.5898691415786743, Test_Loss: 0.606050968170166 *\n",
      "107:Epoch: 13, Train_Loss: 3.079178810119629, Test_Loss: 0.6145481467247009\n",
      "108:Epoch: 13, Train_Loss: 3.090960741043091, Test_Loss: 0.6250643134117126\n",
      "109:Epoch: 13, Train_Loss: 0.587220311164856, Test_Loss: 0.6352726221084595\n",
      "110:Epoch: 13, Train_Loss: 0.5992985367774963, Test_Loss: 0.59256911277771 *\n",
      "111:Epoch: 13, Train_Loss: 0.5902326703071594, Test_Loss: 0.7085556983947754\n",
      "112:Epoch: 13, Train_Loss: 0.5885610580444336, Test_Loss: 0.9608200788497925\n",
      "113:Epoch: 13, Train_Loss: 0.586206316947937, Test_Loss: 0.6872317790985107 *\n",
      "114:Epoch: 13, Train_Loss: 0.5861409902572632, Test_Loss: 0.7979636788368225\n",
      "115:Epoch: 13, Train_Loss: 0.585015058517456, Test_Loss: 0.5996464490890503 *\n",
      "116:Epoch: 13, Train_Loss: 0.5852518081665039, Test_Loss: 0.5998874306678772\n",
      "117:Epoch: 13, Train_Loss: 0.5965636968612671, Test_Loss: 0.6001450419425964\n",
      "118:Epoch: 13, Train_Loss: 0.6447668671607971, Test_Loss: 0.6009373068809509\n",
      "119:Epoch: 13, Train_Loss: 0.6326045989990234, Test_Loss: 0.6366317272186279\n",
      "120:Epoch: 13, Train_Loss: 0.6712216734886169, Test_Loss: 5.037985801696777\n",
      "121:Epoch: 13, Train_Loss: 0.6113595366477966, Test_Loss: 1.5566434860229492 *\n",
      "122:Epoch: 13, Train_Loss: 0.591253936290741, Test_Loss: 0.5946983695030212 *\n",
      "123:Epoch: 13, Train_Loss: 0.78679358959198, Test_Loss: 0.5872352719306946 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124:Epoch: 13, Train_Loss: 0.7918735146522522, Test_Loss: 0.586142897605896 *\n",
      "125:Epoch: 13, Train_Loss: 0.7999261021614075, Test_Loss: 0.5941372513771057\n",
      "126:Epoch: 13, Train_Loss: 0.664467990398407, Test_Loss: 0.5883032083511353 *\n",
      "127:Epoch: 13, Train_Loss: 0.5837062001228333, Test_Loss: 0.5986379981040955\n",
      "128:Epoch: 13, Train_Loss: 0.5832481980323792, Test_Loss: 0.5866976976394653 *\n",
      "129:Epoch: 13, Train_Loss: 0.5864677429199219, Test_Loss: 0.5873509049415588\n",
      "130:Epoch: 13, Train_Loss: 0.5925816297531128, Test_Loss: 0.5916829705238342\n",
      "131:Epoch: 13, Train_Loss: 0.6012397408485413, Test_Loss: 0.6025732755661011\n",
      "132:Epoch: 13, Train_Loss: 0.5926773548126221, Test_Loss: 0.590553343296051 *\n",
      "133:Epoch: 13, Train_Loss: 0.5823560953140259, Test_Loss: 0.5994955897331238\n",
      "134:Epoch: 13, Train_Loss: 0.58245450258255, Test_Loss: 0.5944774150848389 *\n",
      "135:Epoch: 13, Train_Loss: 0.5940631031990051, Test_Loss: 0.5884416103363037 *\n",
      "136:Epoch: 13, Train_Loss: 0.6560173034667969, Test_Loss: 0.583132803440094 *\n",
      "137:Epoch: 13, Train_Loss: 0.7813537120819092, Test_Loss: 0.5846543908119202\n",
      "138:Epoch: 13, Train_Loss: 0.7567458152770996, Test_Loss: 0.5864952206611633\n",
      "139:Epoch: 13, Train_Loss: 0.6922057867050171, Test_Loss: 0.584808886051178 *\n",
      "140:Epoch: 13, Train_Loss: 0.6864476203918457, Test_Loss: 0.587093710899353\n",
      "141:Epoch: 13, Train_Loss: 0.7374719977378845, Test_Loss: 0.5847021341323853 *\n",
      "142:Epoch: 13, Train_Loss: 0.6254096627235413, Test_Loss: 0.5890859365463257\n",
      "143:Epoch: 13, Train_Loss: 0.7443434000015259, Test_Loss: 0.5904165506362915\n",
      "144:Epoch: 13, Train_Loss: 0.7064391374588013, Test_Loss: 0.5897107124328613 *\n",
      "145:Epoch: 13, Train_Loss: 0.8624559044837952, Test_Loss: 0.5856030583381653 *\n",
      "146:Epoch: 13, Train_Loss: 0.5905055403709412, Test_Loss: 0.5844132900238037 *\n",
      "147:Epoch: 13, Train_Loss: 0.7552183866500854, Test_Loss: 0.582712709903717 *\n",
      "148:Epoch: 13, Train_Loss: 3.4849071502685547, Test_Loss: 0.5841647982597351\n",
      "149:Epoch: 13, Train_Loss: 0.6875417828559875, Test_Loss: 0.5882720351219177\n",
      "150:Epoch: 13, Train_Loss: 0.644044041633606, Test_Loss: 0.638637900352478\n",
      "151:Epoch: 13, Train_Loss: 0.636527955532074, Test_Loss: 0.7765508890151978\n",
      "152:Epoch: 13, Train_Loss: 0.6237547397613525, Test_Loss: 5.940561294555664\n",
      "153:Epoch: 13, Train_Loss: 0.5837477445602417, Test_Loss: 0.5953537821769714 *\n",
      "154:Epoch: 13, Train_Loss: 0.5871365070343018, Test_Loss: 0.5805395245552063 *\n",
      "155:Epoch: 13, Train_Loss: 0.672713577747345, Test_Loss: 0.6096513271331787\n",
      "156:Epoch: 13, Train_Loss: 0.7119375467300415, Test_Loss: 0.6253771185874939\n",
      "157:Epoch: 13, Train_Loss: 0.6842557787895203, Test_Loss: 0.6260141134262085\n",
      "158:Epoch: 13, Train_Loss: 0.6436788439750671, Test_Loss: 0.5830798745155334 *\n",
      "159:Epoch: 13, Train_Loss: 0.645744800567627, Test_Loss: 0.6967150568962097\n",
      "160:Epoch: 13, Train_Loss: 0.6092269420623779, Test_Loss: 0.6175991296768188 *\n",
      "161:Epoch: 13, Train_Loss: 0.6116544604301453, Test_Loss: 0.5804488062858582 *\n",
      "162:Epoch: 13, Train_Loss: 0.584932804107666, Test_Loss: 0.6051740646362305\n",
      "163:Epoch: 13, Train_Loss: 0.6084870100021362, Test_Loss: 0.6102123856544495\n",
      "164:Epoch: 13, Train_Loss: 0.5912281274795532, Test_Loss: 0.5801360011100769 *\n",
      "165:Epoch: 13, Train_Loss: 0.5768104195594788, Test_Loss: 0.6244757771492004\n",
      "166:Epoch: 13, Train_Loss: 0.6079626083374023, Test_Loss: 0.6607130765914917\n",
      "167:Epoch: 13, Train_Loss: 0.637870192527771, Test_Loss: 0.6241273880004883 *\n",
      "168:Epoch: 13, Train_Loss: 0.6167150735855103, Test_Loss: 0.6323963403701782\n",
      "169:Epoch: 13, Train_Loss: 0.5763825178146362, Test_Loss: 0.6098417043685913 *\n",
      "170:Epoch: 13, Train_Loss: 0.575899064540863, Test_Loss: 0.6454249024391174\n",
      "171:Epoch: 13, Train_Loss: 0.5754160284996033, Test_Loss: 0.5953658223152161 *\n",
      "172:Epoch: 13, Train_Loss: 0.5750657320022583, Test_Loss: 0.5887677073478699 *\n",
      "173:Epoch: 13, Train_Loss: 0.5790092349052429, Test_Loss: 0.5939714312553406\n",
      "174:Epoch: 13, Train_Loss: 0.5806131362915039, Test_Loss: 0.6029706597328186\n",
      "175:Epoch: 13, Train_Loss: 0.5826708674430847, Test_Loss: 0.5980244278907776 *\n",
      "176:Epoch: 13, Train_Loss: 0.5766875147819519, Test_Loss: 0.5971283316612244 *\n",
      "177:Epoch: 13, Train_Loss: 0.574365496635437, Test_Loss: 0.5898861885070801 *\n",
      "178:Epoch: 13, Train_Loss: 0.5771772265434265, Test_Loss: 0.5919853448867798\n",
      "179:Epoch: 13, Train_Loss: 0.5885007977485657, Test_Loss: 0.5972397923469543\n",
      "180:Epoch: 13, Train_Loss: 0.5944151878356934, Test_Loss: 0.5793653726577759 *\n",
      "181:Epoch: 13, Train_Loss: 0.6072496771812439, Test_Loss: 0.5871637463569641\n",
      "182:Epoch: 13, Train_Loss: 0.6020888686180115, Test_Loss: 0.6448702216148376\n",
      "183:Epoch: 13, Train_Loss: 0.5824703574180603, Test_Loss: 0.5946171879768372 *\n",
      "184:Epoch: 13, Train_Loss: 0.5924350023269653, Test_Loss: 0.9246416091918945\n",
      "185:Epoch: 13, Train_Loss: 0.5743245482444763, Test_Loss: 1.1287145614624023\n",
      "186:Epoch: 13, Train_Loss: 0.5761449337005615, Test_Loss: 0.8088201880455017 *\n",
      "187:Epoch: 13, Train_Loss: 0.5961237549781799, Test_Loss: 0.6258050799369812 *\n",
      "188:Epoch: 13, Train_Loss: 0.5941141247749329, Test_Loss: 0.6054075360298157 *\n",
      "189:Epoch: 13, Train_Loss: 0.5741809010505676, Test_Loss: 0.5823273658752441 *\n",
      "190:Epoch: 13, Train_Loss: 0.5770103931427002, Test_Loss: 0.6537243723869324\n",
      "191:Epoch: 13, Train_Loss: 0.5764895081520081, Test_Loss: 1.1334331035614014\n",
      "192:Epoch: 13, Train_Loss: 0.6121922135353088, Test_Loss: 1.251413106918335\n",
      "193:Epoch: 13, Train_Loss: 0.609347939491272, Test_Loss: 0.6498426198959351 *\n",
      "194:Epoch: 13, Train_Loss: 0.6252794861793518, Test_Loss: 0.6801525950431824\n",
      "195:Epoch: 13, Train_Loss: 0.5784275531768799, Test_Loss: 0.572874128818512 *\n",
      "196:Epoch: 13, Train_Loss: 0.5861186981201172, Test_Loss: 0.5778133273124695\n",
      "197:Epoch: 13, Train_Loss: 0.6437952518463135, Test_Loss: 0.5788850784301758\n",
      "198:Epoch: 13, Train_Loss: 0.57310551404953, Test_Loss: 0.5840470790863037\n",
      "199:Epoch: 13, Train_Loss: 0.5797425508499146, Test_Loss: 0.6063154339790344\n",
      "200:Epoch: 13, Train_Loss: 0.5953271985054016, Test_Loss: 0.594441831111908 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 13\n",
      "201:Epoch: 13, Train_Loss: 0.6220656633377075, Test_Loss: 0.5748148560523987 *\n",
      "202:Epoch: 13, Train_Loss: 0.6784763336181641, Test_Loss: 0.7081640362739563\n",
      "203:Epoch: 13, Train_Loss: 0.640116810798645, Test_Loss: 0.986248254776001\n",
      "204:Epoch: 13, Train_Loss: 0.6022799015045166, Test_Loss: 0.7085031270980835 *\n",
      "205:Epoch: 13, Train_Loss: 0.5747381448745728, Test_Loss: 0.7269982695579529\n",
      "206:Epoch: 13, Train_Loss: 0.5941253304481506, Test_Loss: 0.5854407548904419 *\n",
      "207:Epoch: 13, Train_Loss: 0.5706836581230164, Test_Loss: 0.585286021232605 *\n",
      "208:Epoch: 13, Train_Loss: 0.5743029117584229, Test_Loss: 0.5851803421974182 *\n",
      "209:Epoch: 13, Train_Loss: 0.5805906057357788, Test_Loss: 0.5878804326057434\n",
      "210:Epoch: 13, Train_Loss: 0.5904644131660461, Test_Loss: 0.629960834980011\n",
      "211:Epoch: 13, Train_Loss: 0.6414381265640259, Test_Loss: 5.763176918029785\n",
      "212:Epoch: 13, Train_Loss: 0.6246759295463562, Test_Loss: 0.7954376935958862 *\n",
      "213:Epoch: 13, Train_Loss: 0.616125226020813, Test_Loss: 0.5783128142356873 *\n",
      "214:Epoch: 13, Train_Loss: 0.5891693234443665, Test_Loss: 0.5713440775871277 *\n",
      "215:Epoch: 13, Train_Loss: 0.5969051122665405, Test_Loss: 0.5729855298995972\n",
      "216:Epoch: 13, Train_Loss: 0.5800279974937439, Test_Loss: 0.5774420499801636\n",
      "217:Epoch: 13, Train_Loss: 0.7482996582984924, Test_Loss: 0.5702354907989502 *\n",
      "218:Epoch: 13, Train_Loss: 0.7348814010620117, Test_Loss: 0.5778042078018188\n",
      "219:Epoch: 13, Train_Loss: 0.5734755396842957, Test_Loss: 0.5693249702453613 *\n",
      "220:Epoch: 13, Train_Loss: 0.6025187373161316, Test_Loss: 0.5687451958656311 *\n",
      "221:Epoch: 13, Train_Loss: 0.5670661330223083, Test_Loss: 0.5729842782020569\n",
      "222:Epoch: 13, Train_Loss: 0.5669078230857849, Test_Loss: 0.5857222676277161\n",
      "223:Epoch: 13, Train_Loss: 0.5672289729118347, Test_Loss: 0.5741722583770752 *\n",
      "224:Epoch: 13, Train_Loss: 0.5711825489997864, Test_Loss: 0.5835940837860107\n",
      "225:Epoch: 13, Train_Loss: 0.5785180926322937, Test_Loss: 0.5815221071243286 *\n",
      "226:Epoch: 13, Train_Loss: 0.5864331126213074, Test_Loss: 0.5666086673736572 *\n",
      "227:Epoch: 13, Train_Loss: 0.5766852498054504, Test_Loss: 0.566832959651947\n",
      "228:Epoch: 13, Train_Loss: 0.5802708864212036, Test_Loss: 0.566368818283081 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229:Epoch: 13, Train_Loss: 0.5868723392486572, Test_Loss: 0.568851888179779\n",
      "230:Epoch: 13, Train_Loss: 0.5658198595046997, Test_Loss: 0.5654715895652771 *\n",
      "231:Epoch: 13, Train_Loss: 0.5666327476501465, Test_Loss: 0.5680785179138184\n",
      "232:Epoch: 13, Train_Loss: 0.5657945871353149, Test_Loss: 0.5656443238258362 *\n",
      "233:Epoch: 13, Train_Loss: 0.5967651009559631, Test_Loss: 0.568132221698761\n",
      "234:Epoch: 13, Train_Loss: 0.5931026935577393, Test_Loss: 0.5692153573036194\n",
      "235:Epoch: 13, Train_Loss: 0.5765263438224792, Test_Loss: 0.566838264465332 *\n",
      "236:Epoch: 13, Train_Loss: 0.5860865116119385, Test_Loss: 0.565122127532959 *\n",
      "237:Epoch: 13, Train_Loss: 0.628153920173645, Test_Loss: 0.565917432308197\n",
      "238:Epoch: 13, Train_Loss: 0.6159836053848267, Test_Loss: 0.5644700527191162 *\n",
      "239:Epoch: 13, Train_Loss: 0.5728709101676941, Test_Loss: 0.564939558506012\n",
      "240:Epoch: 13, Train_Loss: 0.586176872253418, Test_Loss: 0.5709477663040161\n",
      "241:Epoch: 13, Train_Loss: 0.5763240456581116, Test_Loss: 0.6250395178794861\n",
      "242:Epoch: 13, Train_Loss: 0.577966034412384, Test_Loss: 1.7665281295776367\n",
      "243:Epoch: 13, Train_Loss: 0.5699053406715393, Test_Loss: 4.812431335449219\n",
      "244:Epoch: 13, Train_Loss: 0.5912538170814514, Test_Loss: 0.5688072443008423 *\n",
      "245:Epoch: 13, Train_Loss: 0.6135804057121277, Test_Loss: 0.5631514191627502 *\n",
      "246:Epoch: 13, Train_Loss: 2.837982654571533, Test_Loss: 0.6119093298912048\n",
      "247:Epoch: 13, Train_Loss: 3.6421046257019043, Test_Loss: 0.603553831577301 *\n",
      "248:Epoch: 13, Train_Loss: 0.5792036652565002, Test_Loss: 0.6126253604888916\n",
      "249:Epoch: 13, Train_Loss: 0.5643889307975769, Test_Loss: 0.5831226706504822 *\n",
      "250:Epoch: 13, Train_Loss: 0.6314283609390259, Test_Loss: 0.6843581199645996\n",
      "251:Epoch: 13, Train_Loss: 0.7224183082580566, Test_Loss: 0.5785381197929382 *\n",
      "252:Epoch: 13, Train_Loss: 0.5876649618148804, Test_Loss: 0.5700120329856873 *\n",
      "253:Epoch: 13, Train_Loss: 0.5659894943237305, Test_Loss: 0.5919047594070435\n",
      "254:Epoch: 13, Train_Loss: 0.5771486759185791, Test_Loss: 0.5884997844696045 *\n",
      "255:Epoch: 13, Train_Loss: 0.6210615634918213, Test_Loss: 0.5664920806884766 *\n",
      "256:Epoch: 13, Train_Loss: 0.5709134340286255, Test_Loss: 0.6226378679275513\n",
      "257:Epoch: 13, Train_Loss: 0.5717306137084961, Test_Loss: 0.6206371784210205 *\n",
      "258:Epoch: 13, Train_Loss: 1.4601666927337646, Test_Loss: 0.6202589869499207 *\n",
      "259:Epoch: 13, Train_Loss: 1.7906689643859863, Test_Loss: 0.6224111318588257\n",
      "260:Epoch: 13, Train_Loss: 0.970400333404541, Test_Loss: 0.5876758098602295 *\n",
      "261:Epoch: 13, Train_Loss: 0.6501667499542236, Test_Loss: 0.6265792846679688\n",
      "262:Epoch: 13, Train_Loss: 1.6004774570465088, Test_Loss: 0.5711669921875 *\n",
      "263:Epoch: 13, Train_Loss: 2.47939133644104, Test_Loss: 0.563623309135437 *\n",
      "264:Epoch: 13, Train_Loss: 0.7205061912536621, Test_Loss: 0.5735293030738831\n",
      "265:Epoch: 13, Train_Loss: 0.5758557915687561, Test_Loss: 0.5929449200630188\n",
      "266:Epoch: 13, Train_Loss: 0.6267588138580322, Test_Loss: 0.5858044624328613 *\n",
      "267:Epoch: 13, Train_Loss: 1.534148931503296, Test_Loss: 0.5770198702812195 *\n",
      "268:Epoch: 13, Train_Loss: 1.5947651863098145, Test_Loss: 0.563809871673584 *\n",
      "269:Epoch: 13, Train_Loss: 0.5831710696220398, Test_Loss: 0.5674368143081665\n",
      "270:Epoch: 13, Train_Loss: 0.5738829374313354, Test_Loss: 0.5621339082717896 *\n",
      "271:Epoch: 13, Train_Loss: 0.5678554177284241, Test_Loss: 0.5893696546554565\n",
      "272:Epoch: 13, Train_Loss: 1.1994612216949463, Test_Loss: 0.5662916302680969 *\n",
      "273:Epoch: 13, Train_Loss: 0.7230729460716248, Test_Loss: 0.5681833624839783\n",
      "274:Epoch: 13, Train_Loss: 0.6681888103485107, Test_Loss: 0.6061689257621765\n",
      "275:Epoch: 13, Train_Loss: 0.6010186076164246, Test_Loss: 0.747780442237854\n",
      "276:Epoch: 13, Train_Loss: 0.6381273865699768, Test_Loss: 0.7723803520202637\n",
      "277:Epoch: 13, Train_Loss: 0.6438928842544556, Test_Loss: 0.7032235860824585 *\n",
      "278:Epoch: 13, Train_Loss: 0.6829261779785156, Test_Loss: 0.5724097490310669 *\n",
      "279:Epoch: 13, Train_Loss: 0.6765828132629395, Test_Loss: 0.5824651122093201\n",
      "280:Epoch: 13, Train_Loss: 0.5835244059562683, Test_Loss: 0.5653966665267944 *\n",
      "281:Epoch: 13, Train_Loss: 0.6277982592582703, Test_Loss: 0.6077520847320557\n",
      "282:Epoch: 13, Train_Loss: 0.6324624419212341, Test_Loss: 0.7525799870491028\n",
      "283:Epoch: 13, Train_Loss: 0.6797655820846558, Test_Loss: 0.8160346746444702\n",
      "284:Epoch: 13, Train_Loss: 0.6721100211143494, Test_Loss: 0.6266133785247803 *\n",
      "285:Epoch: 13, Train_Loss: 0.6137055158615112, Test_Loss: 0.6022836565971375 *\n",
      "286:Epoch: 13, Train_Loss: 0.6655197143554688, Test_Loss: 0.6040564775466919\n",
      "287:Epoch: 13, Train_Loss: 0.7308690547943115, Test_Loss: 0.6139276623725891\n",
      "288:Epoch: 13, Train_Loss: 0.6287256479263306, Test_Loss: 0.6460638046264648\n",
      "289:Epoch: 13, Train_Loss: 0.568740963935852, Test_Loss: 0.6834145784378052\n",
      "290:Epoch: 13, Train_Loss: 0.5624728798866272, Test_Loss: 0.645513653755188 *\n",
      "291:Epoch: 13, Train_Loss: 0.5615970492362976, Test_Loss: 0.6640969514846802\n",
      "292:Epoch: 13, Train_Loss: 0.5557751655578613, Test_Loss: 0.6090301871299744 *\n",
      "293:Epoch: 13, Train_Loss: 0.5675942301750183, Test_Loss: 0.7916012406349182\n",
      "294:Epoch: 13, Train_Loss: 0.6014464497566223, Test_Loss: 0.8725103139877319\n",
      "295:Epoch: 13, Train_Loss: 0.56883305311203, Test_Loss: 1.3050017356872559\n",
      "296:Epoch: 13, Train_Loss: 0.6078152060508728, Test_Loss: 0.7594321966171265 *\n",
      "297:Epoch: 13, Train_Loss: 0.7129608392715454, Test_Loss: 0.6665968894958496 *\n",
      "298:Epoch: 13, Train_Loss: 0.7537358999252319, Test_Loss: 0.6596540808677673 *\n",
      "299:Epoch: 13, Train_Loss: 0.5613075494766235, Test_Loss: 0.6606206893920898\n",
      "300:Epoch: 13, Train_Loss: 0.6391901969909668, Test_Loss: 0.7544540166854858\n",
      "Model saved at location ../Saver/model.ckpt at epoch 13\n",
      "301:Epoch: 13, Train_Loss: 0.6637191772460938, Test_Loss: 0.9000244140625\n",
      "302:Epoch: 13, Train_Loss: 0.8584029674530029, Test_Loss: 5.9281697273254395\n",
      "303:Epoch: 13, Train_Loss: 0.7544517517089844, Test_Loss: 0.6730234026908875 *\n",
      "304:Epoch: 13, Train_Loss: 0.598112165927887, Test_Loss: 0.5946550965309143 *\n",
      "305:Epoch: 13, Train_Loss: 0.7126219868659973, Test_Loss: 0.5884431004524231 *\n",
      "306:Epoch: 13, Train_Loss: 0.8545516133308411, Test_Loss: 0.5696755051612854 *\n",
      "307:Epoch: 13, Train_Loss: 0.718682587146759, Test_Loss: 0.5871343016624451\n",
      "308:Epoch: 13, Train_Loss: 0.5822615027427673, Test_Loss: 0.657148003578186\n",
      "309:Epoch: 13, Train_Loss: 0.5675841569900513, Test_Loss: 0.6578188538551331\n",
      "310:Epoch: 13, Train_Loss: 0.6142330765724182, Test_Loss: 0.5644447207450867 *\n",
      "311:Epoch: 13, Train_Loss: 1.1784778833389282, Test_Loss: 0.5928466320037842\n",
      "312:Epoch: 13, Train_Loss: 1.0520429611206055, Test_Loss: 0.6120010018348694\n",
      "313:Epoch: 13, Train_Loss: 0.5707148909568787, Test_Loss: 0.7122335433959961\n",
      "314:Epoch: 13, Train_Loss: 0.6008251309394836, Test_Loss: 0.5939616560935974 *\n",
      "315:Epoch: 13, Train_Loss: 0.5709779262542725, Test_Loss: 0.6257138252258301\n",
      "316:Epoch: 13, Train_Loss: 0.6250630617141724, Test_Loss: 0.6359855532646179\n",
      "317:Epoch: 13, Train_Loss: 0.847891628742218, Test_Loss: 0.5735687613487244 *\n",
      "318:Epoch: 13, Train_Loss: 0.5567705035209656, Test_Loss: 0.572561502456665 *\n",
      "319:Epoch: 13, Train_Loss: 0.763187050819397, Test_Loss: 0.6193147897720337\n",
      "320:Epoch: 13, Train_Loss: 0.591193675994873, Test_Loss: 0.6663590669631958\n",
      "321:Epoch: 13, Train_Loss: 0.586003303527832, Test_Loss: 0.5672044157981873 *\n",
      "322:Epoch: 13, Train_Loss: 0.5976552367210388, Test_Loss: 0.596921980381012\n",
      "323:Epoch: 13, Train_Loss: 0.6622225046157837, Test_Loss: 0.590282678604126 *\n",
      "324:Epoch: 13, Train_Loss: 0.6380748152732849, Test_Loss: 0.6557033061981201\n",
      "325:Epoch: 13, Train_Loss: 0.7009432315826416, Test_Loss: 0.671594500541687\n",
      "326:Epoch: 13, Train_Loss: 0.6092309951782227, Test_Loss: 0.6527552604675293 *\n",
      "327:Epoch: 13, Train_Loss: 0.5891395807266235, Test_Loss: 0.5704002976417542 *\n",
      "328:Epoch: 13, Train_Loss: 0.6279404163360596, Test_Loss: 0.5920702815055847\n",
      "329:Epoch: 13, Train_Loss: 0.6046663522720337, Test_Loss: 0.5995537042617798\n",
      "330:Epoch: 13, Train_Loss: 0.6112634539604187, Test_Loss: 0.5735040903091431 *\n",
      "331:Epoch: 13, Train_Loss: 0.5683596730232239, Test_Loss: 0.6229656934738159\n",
      "332:Epoch: 13, Train_Loss: 0.604944109916687, Test_Loss: 0.6868563294410706\n",
      "333:Epoch: 13, Train_Loss: 0.7791684865951538, Test_Loss: 3.035482883453369\n",
      "334:Epoch: 13, Train_Loss: 0.8110929727554321, Test_Loss: 3.471886157989502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335:Epoch: 13, Train_Loss: 0.921790361404419, Test_Loss: 0.592698872089386 *\n",
      "336:Epoch: 13, Train_Loss: 0.8417671322822571, Test_Loss: 0.6070246696472168\n",
      "337:Epoch: 13, Train_Loss: 0.7629595994949341, Test_Loss: 0.6084136962890625\n",
      "338:Epoch: 13, Train_Loss: 0.7051506042480469, Test_Loss: 0.5536461472511292 *\n",
      "339:Epoch: 13, Train_Loss: 0.616459310054779, Test_Loss: 0.5665821433067322\n",
      "340:Epoch: 13, Train_Loss: 0.5691973567008972, Test_Loss: 0.6132707595825195\n",
      "341:Epoch: 13, Train_Loss: 0.5638651847839355, Test_Loss: 0.6157064437866211\n",
      "342:Epoch: 13, Train_Loss: 0.6164481043815613, Test_Loss: 0.5615097880363464 *\n",
      "343:Epoch: 13, Train_Loss: 0.8366093635559082, Test_Loss: 0.5858228802680969\n",
      "344:Epoch: 13, Train_Loss: 0.9933565855026245, Test_Loss: 0.5899730920791626\n",
      "345:Epoch: 13, Train_Loss: 1.371360421180725, Test_Loss: 0.6889409422874451\n",
      "346:Epoch: 13, Train_Loss: 1.6220862865447998, Test_Loss: 0.6002283096313477 *\n",
      "347:Epoch: 13, Train_Loss: 0.8641523122787476, Test_Loss: 0.6213525533676147\n",
      "348:Epoch: 13, Train_Loss: 0.8427451848983765, Test_Loss: 0.6003396511077881 *\n",
      "349:Epoch: 13, Train_Loss: 0.5588626861572266, Test_Loss: 0.6146908402442932\n",
      "350:Epoch: 13, Train_Loss: 0.5825477838516235, Test_Loss: 0.5501702427864075 *\n",
      "351:Epoch: 13, Train_Loss: 0.8614131212234497, Test_Loss: 0.6402129530906677\n",
      "352:Epoch: 13, Train_Loss: 1.5609095096588135, Test_Loss: 0.6659209132194519\n",
      "353:Epoch: 13, Train_Loss: 0.6049048900604248, Test_Loss: 0.58566814661026 *\n",
      "354:Epoch: 13, Train_Loss: 0.5842908620834351, Test_Loss: 0.6349518299102783\n",
      "355:Epoch: 13, Train_Loss: 0.6255208253860474, Test_Loss: 0.6268668174743652 *\n",
      "356:Epoch: 13, Train_Loss: 0.6689325571060181, Test_Loss: 0.6012440323829651 *\n",
      "357:Epoch: 13, Train_Loss: 0.8425986170768738, Test_Loss: 0.6117739081382751\n",
      "358:Epoch: 13, Train_Loss: 0.7753564119338989, Test_Loss: 0.6370241045951843\n",
      "359:Epoch: 13, Train_Loss: 0.7540462017059326, Test_Loss: 0.6339179277420044 *\n",
      "360:Epoch: 13, Train_Loss: 0.8684747815132141, Test_Loss: 0.6449927091598511\n",
      "361:Epoch: 13, Train_Loss: 0.5837676525115967, Test_Loss: 0.5869232416152954 *\n",
      "362:Epoch: 13, Train_Loss: 0.5894793272018433, Test_Loss: 0.6585875749588013\n",
      "363:Epoch: 13, Train_Loss: 0.5772510170936584, Test_Loss: 0.6137325763702393 *\n",
      "364:Epoch: 13, Train_Loss: 0.640177845954895, Test_Loss: 0.5652799010276794 *\n",
      "365:Epoch: 13, Train_Loss: 0.5583505034446716, Test_Loss: 0.6489921808242798\n",
      "366:Epoch: 13, Train_Loss: 0.6030659675598145, Test_Loss: 0.6356801390647888 *\n",
      "367:Epoch: 13, Train_Loss: 16.020009994506836, Test_Loss: 0.6712836027145386\n",
      "368:Epoch: 13, Train_Loss: 0.6910411715507507, Test_Loss: 0.6114638447761536 *\n",
      "369:Epoch: 13, Train_Loss: 1.8554692268371582, Test_Loss: 0.5797106027603149 *\n",
      "370:Epoch: 13, Train_Loss: 1.8485006093978882, Test_Loss: 0.609455943107605\n",
      "371:Epoch: 13, Train_Loss: 0.5575025081634521, Test_Loss: 0.5764544606208801 *\n",
      "372:Epoch: 13, Train_Loss: 0.5985662937164307, Test_Loss: 0.6416566371917725\n",
      "373:Epoch: 13, Train_Loss: 2.2666053771972656, Test_Loss: 0.7513997554779053\n",
      "374:Epoch: 13, Train_Loss: 6.833390235900879, Test_Loss: 0.909000039100647\n",
      "375:Epoch: 13, Train_Loss: 0.7156381607055664, Test_Loss: 0.7101885080337524 *\n",
      "376:Epoch: 13, Train_Loss: 0.6232578158378601, Test_Loss: 0.8016589283943176\n",
      "377:Epoch: 13, Train_Loss: 4.854143142700195, Test_Loss: 0.862540602684021\n",
      "378:Epoch: 13, Train_Loss: 0.8874032497406006, Test_Loss: 1.1285996437072754\n",
      "379:Epoch: 13, Train_Loss: 0.7035131454467773, Test_Loss: 1.6038107872009277\n",
      "380:Epoch: 13, Train_Loss: 0.5627281665802002, Test_Loss: 0.8362840414047241 *\n",
      "381:Epoch: 13, Train_Loss: 0.6119482517242432, Test_Loss: 2.1354823112487793\n",
      "382:Epoch: 13, Train_Loss: 0.7636987566947937, Test_Loss: 1.0717723369598389 *\n",
      "383:Epoch: 13, Train_Loss: 0.5788472890853882, Test_Loss: 1.6404751539230347\n",
      "384:Epoch: 13, Train_Loss: 0.5814977884292603, Test_Loss: 2.3490407466888428\n",
      "385:Epoch: 13, Train_Loss: 0.5398464202880859, Test_Loss: 1.336680293083191 *\n",
      "386:Epoch: 13, Train_Loss: 0.5421605706214905, Test_Loss: 0.7427052855491638 *\n",
      "387:Epoch: 13, Train_Loss: 0.5503919124603271, Test_Loss: 0.6043851971626282 *\n",
      "388:Epoch: 13, Train_Loss: 0.5830210447311401, Test_Loss: 0.6213861107826233\n",
      "389:Epoch: 13, Train_Loss: 0.6782753467559814, Test_Loss: 0.6043612957000732 *\n",
      "390:Epoch: 13, Train_Loss: 0.6968114972114563, Test_Loss: 0.578775942325592 *\n",
      "391:Epoch: 13, Train_Loss: 0.643668532371521, Test_Loss: 0.6150826215744019\n",
      "392:Epoch: 13, Train_Loss: 0.5728855133056641, Test_Loss: 1.7320818901062012\n",
      "393:Epoch: 13, Train_Loss: 0.5804646611213684, Test_Loss: 6.2494988441467285\n",
      "394:Epoch: 13, Train_Loss: 0.5643646717071533, Test_Loss: 0.8858004808425903 *\n",
      "395:Epoch: 13, Train_Loss: 0.5899177193641663, Test_Loss: 1.1854698657989502\n",
      "396:Epoch: 13, Train_Loss: 0.5455915331840515, Test_Loss: 1.1275217533111572 *\n",
      "397:Epoch: 13, Train_Loss: 0.5494011640548706, Test_Loss: 0.7495095729827881 *\n",
      "398:Epoch: 13, Train_Loss: 0.5443463325500488, Test_Loss: 1.0154798030853271\n",
      "399:Epoch: 13, Train_Loss: 0.546483039855957, Test_Loss: 1.416151523590088\n",
      "400:Epoch: 13, Train_Loss: 0.5465726256370544, Test_Loss: 1.0418161153793335 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 13\n",
      "401:Epoch: 13, Train_Loss: 0.543541669845581, Test_Loss: 0.6087070107460022 *\n",
      "402:Epoch: 13, Train_Loss: 0.5418317914009094, Test_Loss: 0.7388947010040283\n",
      "403:Epoch: 13, Train_Loss: 0.5596534609794617, Test_Loss: 0.7837890386581421\n",
      "404:Epoch: 13, Train_Loss: 0.585492730140686, Test_Loss: 1.1372039318084717\n",
      "405:Epoch: 13, Train_Loss: 0.5889031291007996, Test_Loss: 0.8163579106330872 *\n",
      "406:Epoch: 13, Train_Loss: 0.6893522143363953, Test_Loss: 1.09488844871521\n",
      "407:Epoch: 13, Train_Loss: 0.6031425595283508, Test_Loss: 0.9067223072052002 *\n",
      "408:Epoch: 13, Train_Loss: 0.9032554030418396, Test_Loss: 0.5461122393608093 *\n",
      "409:Epoch: 13, Train_Loss: 8.005609512329102, Test_Loss: 0.5681126117706299\n",
      "410:Epoch: 13, Train_Loss: 0.5872738361358643, Test_Loss: 0.5578678846359253 *\n",
      "411:Epoch: 13, Train_Loss: 0.6318862438201904, Test_Loss: 0.9200341701507568\n",
      "412:Epoch: 13, Train_Loss: 0.7506686449050903, Test_Loss: 0.5885246396064758 *\n",
      "413:Epoch: 13, Train_Loss: 0.8107795715332031, Test_Loss: 0.748956024646759\n",
      "414:Epoch: 13, Train_Loss: 0.6412944197654724, Test_Loss: 0.6777138710021973 *\n",
      "415:Epoch: 13, Train_Loss: 0.661583662033081, Test_Loss: 0.8073781728744507\n",
      "416:Epoch: 13, Train_Loss: 0.7103133797645569, Test_Loss: 0.7805516123771667 *\n",
      "417:Epoch: 13, Train_Loss: 0.74955153465271, Test_Loss: 0.6822438836097717 *\n",
      "418:Epoch: 13, Train_Loss: 0.6250274181365967, Test_Loss: 0.5776067972183228 *\n",
      "419:Epoch: 13, Train_Loss: 0.5790034532546997, Test_Loss: 0.6225500106811523\n",
      "420:Epoch: 13, Train_Loss: 0.5403200387954712, Test_Loss: 0.6046893000602722 *\n",
      "421:Epoch: 13, Train_Loss: 0.601609468460083, Test_Loss: 0.5752249956130981 *\n",
      "422:Epoch: 13, Train_Loss: 0.5843445062637329, Test_Loss: 0.6462295055389404\n",
      "423:Epoch: 13, Train_Loss: 0.720914363861084, Test_Loss: 0.5861243009567261 *\n",
      "424:Epoch: 13, Train_Loss: 0.6023247241973877, Test_Loss: 4.009278774261475\n",
      "425:Epoch: 13, Train_Loss: 0.6069130897521973, Test_Loss: 2.8019847869873047 *\n",
      "426:Epoch: 13, Train_Loss: 0.5440406203269958, Test_Loss: 0.5408919453620911 *\n",
      "427:Epoch: 13, Train_Loss: 0.5663019418716431, Test_Loss: 0.5383573770523071 *\n",
      "428:Epoch: 13, Train_Loss: 0.5969151854515076, Test_Loss: 0.5625300407409668\n",
      "429:Epoch: 13, Train_Loss: 0.5589475631713867, Test_Loss: 0.5538477301597595 *\n",
      "430:Epoch: 13, Train_Loss: 0.5408329367637634, Test_Loss: 0.5532304048538208 *\n",
      "431:Epoch: 13, Train_Loss: 0.5489944815635681, Test_Loss: 0.5939386487007141\n",
      "432:Epoch: 13, Train_Loss: 0.54522705078125, Test_Loss: 0.6126538515090942\n",
      "433:Epoch: 13, Train_Loss: 1.297080397605896, Test_Loss: 0.5362836122512817 *\n",
      "434:Epoch: 13, Train_Loss: 5.563267707824707, Test_Loss: 0.5611875057220459\n",
      "435:Epoch: 13, Train_Loss: 0.5372331142425537, Test_Loss: 0.5521035194396973 *\n",
      "436:Epoch: 13, Train_Loss: 0.5350018739700317, Test_Loss: 0.548953652381897 *\n",
      "437:Epoch: 13, Train_Loss: 0.5379459857940674, Test_Loss: 0.5372338891029358 *\n",
      "438:Epoch: 13, Train_Loss: 0.5344642996788025, Test_Loss: 0.601166844367981\n",
      "439:Epoch: 13, Train_Loss: 0.5330237746238708, Test_Loss: 0.5786664485931396 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440:Epoch: 13, Train_Loss: 0.533862829208374, Test_Loss: 0.6251221299171448\n",
      "441:Epoch: 13, Train_Loss: 0.5409793853759766, Test_Loss: 0.6184975504875183 *\n",
      "442:Epoch: 13, Train_Loss: 0.597003161907196, Test_Loss: 0.5594273209571838 *\n",
      "443:Epoch: 13, Train_Loss: 0.5482641458511353, Test_Loss: 0.5462437272071838 *\n",
      "444:Epoch: 13, Train_Loss: 0.5351981520652771, Test_Loss: 0.533674955368042 *\n",
      "445:Epoch: 13, Train_Loss: 0.5308457612991333, Test_Loss: 0.5324115753173828 *\n",
      "446:Epoch: 13, Train_Loss: 0.5334581136703491, Test_Loss: 0.5335239171981812\n",
      "447:Epoch: 13, Train_Loss: 0.546829104423523, Test_Loss: 0.5347434878349304\n",
      "448:Epoch: 13, Train_Loss: 0.5386044383049011, Test_Loss: 0.5348972082138062\n",
      "449:Epoch: 13, Train_Loss: 0.5380139946937561, Test_Loss: 0.5312472581863403 *\n",
      "450:Epoch: 13, Train_Loss: 0.5520449280738831, Test_Loss: 0.5359340310096741\n",
      "451:Epoch: 13, Train_Loss: 0.5700039863586426, Test_Loss: 0.5331249237060547 *\n",
      "452:Epoch: 13, Train_Loss: 0.5388522148132324, Test_Loss: 0.5360807776451111\n",
      "453:Epoch: 13, Train_Loss: 0.5295096039772034, Test_Loss: 0.5411632061004639\n",
      "454:Epoch: 13, Train_Loss: 0.5388326644897461, Test_Loss: 0.5414679050445557\n",
      "1:Epoch: 14, Train_Loss: 0.6362157464027405, Test_Loss: 0.5621652007102966 *\n",
      "2:Epoch: 14, Train_Loss: 0.5843914747238159, Test_Loss: 0.6066518425941467\n",
      "3:Epoch: 14, Train_Loss: 0.5981438159942627, Test_Loss: 0.9052109718322754\n",
      "4:Epoch: 14, Train_Loss: 0.5500028133392334, Test_Loss: 0.8636484146118164 *\n",
      "5:Epoch: 14, Train_Loss: 0.6084613800048828, Test_Loss: 0.6147887706756592 *\n",
      "6:Epoch: 14, Train_Loss: 0.5538578033447266, Test_Loss: 0.5338442325592041 *\n",
      "7:Epoch: 14, Train_Loss: 0.5921536684036255, Test_Loss: 0.5499229431152344\n",
      "8:Epoch: 14, Train_Loss: 0.5495961308479309, Test_Loss: 0.5937433242797852\n",
      "9:Epoch: 14, Train_Loss: 0.6717432141304016, Test_Loss: 0.8937458992004395\n",
      "10:Epoch: 14, Train_Loss: 0.5525501370429993, Test_Loss: 1.3541984558105469\n",
      "11:Epoch: 14, Train_Loss: 0.5301886200904846, Test_Loss: 0.9512662887573242 *\n",
      "12:Epoch: 14, Train_Loss: 0.530036211013794, Test_Loss: 0.6114190816879272 *\n",
      "13:Epoch: 14, Train_Loss: 0.5271621942520142, Test_Loss: 0.5437151789665222 *\n",
      "14:Epoch: 14, Train_Loss: 0.5266227126121521, Test_Loss: 0.5451960563659668\n",
      "15:Epoch: 14, Train_Loss: 0.5300023555755615, Test_Loss: 0.5386346578598022 *\n",
      "16:Epoch: 14, Train_Loss: 1.3180261850357056, Test_Loss: 0.5438081622123718\n",
      "17:Epoch: 14, Train_Loss: 4.592772006988525, Test_Loss: 0.5484809875488281\n",
      "18:Epoch: 14, Train_Loss: 0.5314814448356628, Test_Loss: 0.5819315910339355\n",
      "19:Epoch: 14, Train_Loss: 0.5368955731391907, Test_Loss: 0.5302921533584595 *\n",
      "20:Epoch: 14, Train_Loss: 0.5326308608055115, Test_Loss: 0.6076706647872925\n",
      "21:Epoch: 14, Train_Loss: 0.528774082660675, Test_Loss: 0.7039788961410522\n",
      "22:Epoch: 14, Train_Loss: 0.5266560912132263, Test_Loss: 0.8418747782707214\n",
      "23:Epoch: 14, Train_Loss: 0.5264353156089783, Test_Loss: 0.8170363306999207 *\n",
      "24:Epoch: 14, Train_Loss: 0.5249780416488647, Test_Loss: 0.551558256149292 *\n",
      "25:Epoch: 14, Train_Loss: 0.5250281691551208, Test_Loss: 0.5432650446891785 *\n",
      "26:Epoch: 14, Train_Loss: 0.5267173051834106, Test_Loss: 0.5431724786758423 *\n",
      "27:Epoch: 14, Train_Loss: 0.5849904417991638, Test_Loss: 0.5430125594139099 *\n",
      "28:Epoch: 14, Train_Loss: 0.5652434825897217, Test_Loss: 0.5826011300086975\n",
      "29:Epoch: 14, Train_Loss: 0.6096862554550171, Test_Loss: 2.8203611373901367\n",
      "30:Epoch: 14, Train_Loss: 0.5571445226669312, Test_Loss: 3.568337917327881\n",
      "31:Epoch: 14, Train_Loss: 0.5280588269233704, Test_Loss: 0.5385518670082092 *\n",
      "32:Epoch: 14, Train_Loss: 0.7009017467498779, Test_Loss: 0.5294299125671387 *\n",
      "33:Epoch: 14, Train_Loss: 0.7546994686126709, Test_Loss: 0.5287905335426331 *\n",
      "34:Epoch: 14, Train_Loss: 0.7508646845817566, Test_Loss: 0.5366002321243286\n",
      "35:Epoch: 14, Train_Loss: 0.6602279543876648, Test_Loss: 0.528927206993103 *\n",
      "36:Epoch: 14, Train_Loss: 0.5253703594207764, Test_Loss: 0.5406479835510254\n",
      "37:Epoch: 14, Train_Loss: 0.5232076048851013, Test_Loss: 0.5315686464309692 *\n",
      "38:Epoch: 14, Train_Loss: 0.5257643461227417, Test_Loss: 0.5303781628608704 *\n",
      "39:Epoch: 14, Train_Loss: 0.5354733467102051, Test_Loss: 0.535913348197937\n",
      "40:Epoch: 14, Train_Loss: 0.5413397550582886, Test_Loss: 0.5309562683105469 *\n",
      "41:Epoch: 14, Train_Loss: 0.5400577187538147, Test_Loss: 0.5489133596420288\n",
      "42:Epoch: 14, Train_Loss: 0.5257619619369507, Test_Loss: 0.5358628034591675 *\n",
      "43:Epoch: 14, Train_Loss: 0.522235631942749, Test_Loss: 0.5265175104141235 *\n",
      "44:Epoch: 14, Train_Loss: 0.535870373249054, Test_Loss: 0.5431477427482605\n",
      "45:Epoch: 14, Train_Loss: 0.5636128187179565, Test_Loss: 0.5235698819160461 *\n",
      "46:Epoch: 14, Train_Loss: 0.710934042930603, Test_Loss: 0.5229732990264893 *\n",
      "47:Epoch: 14, Train_Loss: 0.692642331123352, Test_Loss: 0.5259928703308105\n",
      "48:Epoch: 14, Train_Loss: 0.6733575463294983, Test_Loss: 0.5301946997642517\n",
      "49:Epoch: 14, Train_Loss: 0.5824688076972961, Test_Loss: 0.5259755253791809 *\n",
      "50:Epoch: 14, Train_Loss: 0.6502354145050049, Test_Loss: 0.527760922908783\n",
      "51:Epoch: 14, Train_Loss: 0.6120783090591431, Test_Loss: 0.5283036828041077\n",
      "52:Epoch: 14, Train_Loss: 0.6301257014274597, Test_Loss: 0.5350842475891113\n",
      "53:Epoch: 14, Train_Loss: 0.6395992040634155, Test_Loss: 0.5323070287704468 *\n",
      "54:Epoch: 14, Train_Loss: 0.8161457777023315, Test_Loss: 0.5282678604125977 *\n",
      "55:Epoch: 14, Train_Loss: 0.5324405431747437, Test_Loss: 0.5225996971130371 *\n",
      "56:Epoch: 14, Train_Loss: 0.528630793094635, Test_Loss: 0.5276768803596497\n",
      "57:Epoch: 14, Train_Loss: 3.3284213542938232, Test_Loss: 0.5250828266143799 *\n",
      "58:Epoch: 14, Train_Loss: 0.8907055854797363, Test_Loss: 0.5252563953399658\n",
      "59:Epoch: 14, Train_Loss: 0.5713245868682861, Test_Loss: 0.5690925121307373\n",
      "60:Epoch: 14, Train_Loss: 0.5711561441421509, Test_Loss: 0.5623732805252075 *\n",
      "61:Epoch: 14, Train_Loss: 0.545767068862915, Test_Loss: 5.066995620727539\n",
      "62:Epoch: 14, Train_Loss: 0.5389024615287781, Test_Loss: 1.4075572490692139 *\n",
      "63:Epoch: 14, Train_Loss: 0.5351718068122864, Test_Loss: 0.5214144587516785 *\n",
      "64:Epoch: 14, Train_Loss: 0.58991938829422, Test_Loss: 0.5390662550926208\n",
      "65:Epoch: 14, Train_Loss: 0.6797126531600952, Test_Loss: 0.5738373398780823\n",
      "66:Epoch: 14, Train_Loss: 0.634297251701355, Test_Loss: 0.5724678635597229 *\n",
      "67:Epoch: 14, Train_Loss: 0.5981513261795044, Test_Loss: 0.531135082244873 *\n",
      "68:Epoch: 14, Train_Loss: 0.5993478894233704, Test_Loss: 0.5937181711196899\n",
      "69:Epoch: 14, Train_Loss: 0.553553581237793, Test_Loss: 0.5851689577102661 *\n",
      "70:Epoch: 14, Train_Loss: 0.5521859526634216, Test_Loss: 0.5222472548484802 *\n",
      "71:Epoch: 14, Train_Loss: 0.5255476832389832, Test_Loss: 0.5492647886276245\n",
      "72:Epoch: 14, Train_Loss: 0.5590736865997314, Test_Loss: 0.5407352447509766 *\n",
      "73:Epoch: 14, Train_Loss: 0.5436318516731262, Test_Loss: 0.529047966003418 *\n",
      "74:Epoch: 14, Train_Loss: 0.5181015133857727, Test_Loss: 0.5258427262306213 *\n",
      "75:Epoch: 14, Train_Loss: 0.5317213535308838, Test_Loss: 0.6380676031112671\n",
      "76:Epoch: 14, Train_Loss: 0.5698716044425964, Test_Loss: 0.5589724183082581 *\n",
      "77:Epoch: 14, Train_Loss: 0.5671924948692322, Test_Loss: 0.6092134714126587\n",
      "78:Epoch: 14, Train_Loss: 0.5192267894744873, Test_Loss: 0.5627750754356384 *\n",
      "79:Epoch: 14, Train_Loss: 0.5167005062103271, Test_Loss: 0.5701400637626648\n",
      "80:Epoch: 14, Train_Loss: 0.5170213580131531, Test_Loss: 0.5414553284645081 *\n",
      "81:Epoch: 14, Train_Loss: 0.5162150263786316, Test_Loss: 0.5396658778190613 *\n",
      "82:Epoch: 14, Train_Loss: 0.5166082382202148, Test_Loss: 0.5378262996673584 *\n",
      "83:Epoch: 14, Train_Loss: 0.5179474949836731, Test_Loss: 0.5410631895065308\n",
      "84:Epoch: 14, Train_Loss: 0.51723712682724, Test_Loss: 0.5432427525520325\n",
      "85:Epoch: 14, Train_Loss: 0.5184476971626282, Test_Loss: 0.5423434376716614 *\n",
      "86:Epoch: 14, Train_Loss: 0.5163403749465942, Test_Loss: 0.5327390432357788 *\n",
      "87:Epoch: 14, Train_Loss: 0.516457200050354, Test_Loss: 0.548202633857727\n",
      "88:Epoch: 14, Train_Loss: 0.5180291533470154, Test_Loss: 0.5399888753890991 *\n",
      "89:Epoch: 14, Train_Loss: 0.5315777659416199, Test_Loss: 0.5311750769615173 *\n",
      "90:Epoch: 14, Train_Loss: 0.5321775674819946, Test_Loss: 0.5281205773353577 *\n",
      "91:Epoch: 14, Train_Loss: 0.5328935980796814, Test_Loss: 0.5778422355651855\n",
      "92:Epoch: 14, Train_Loss: 0.5367408394813538, Test_Loss: 0.5627204775810242 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93:Epoch: 14, Train_Loss: 0.5284927487373352, Test_Loss: 0.7519795894622803\n",
      "94:Epoch: 14, Train_Loss: 0.5170117616653442, Test_Loss: 1.0595065355300903\n",
      "95:Epoch: 14, Train_Loss: 0.5174601078033447, Test_Loss: 0.8544002771377563 *\n",
      "96:Epoch: 14, Train_Loss: 0.5361267328262329, Test_Loss: 0.6166560649871826 *\n",
      "97:Epoch: 14, Train_Loss: 0.5449665188789368, Test_Loss: 0.5425018072128296 *\n",
      "98:Epoch: 14, Train_Loss: 0.5151042938232422, Test_Loss: 0.5242160558700562 *\n",
      "99:Epoch: 14, Train_Loss: 0.5196738839149475, Test_Loss: 0.5815973877906799\n",
      "100:Epoch: 14, Train_Loss: 0.5164539217948914, Test_Loss: 0.956983208656311\n",
      "Model saved at location ../Saver/model.ckpt at epoch 14\n",
      "101:Epoch: 14, Train_Loss: 0.5488066077232361, Test_Loss: 1.2678651809692383\n",
      "102:Epoch: 14, Train_Loss: 0.5608576536178589, Test_Loss: 0.7697839140892029 *\n",
      "103:Epoch: 14, Train_Loss: 0.5730771422386169, Test_Loss: 0.6316258907318115 *\n",
      "104:Epoch: 14, Train_Loss: 0.5319162607192993, Test_Loss: 0.5183025598526001 *\n",
      "105:Epoch: 14, Train_Loss: 0.5145906209945679, Test_Loss: 0.5185559391975403\n",
      "106:Epoch: 14, Train_Loss: 0.595180869102478, Test_Loss: 0.5135723352432251 *\n",
      "107:Epoch: 14, Train_Loss: 0.5196107029914856, Test_Loss: 0.5286659002304077\n",
      "108:Epoch: 14, Train_Loss: 0.520000696182251, Test_Loss: 0.5325028300285339\n",
      "109:Epoch: 14, Train_Loss: 0.5320876836776733, Test_Loss: 0.5460898280143738\n",
      "110:Epoch: 14, Train_Loss: 0.5241996645927429, Test_Loss: 0.5164362192153931 *\n",
      "111:Epoch: 14, Train_Loss: 0.6424252986907959, Test_Loss: 0.6402467489242554\n",
      "112:Epoch: 14, Train_Loss: 0.5829214453697205, Test_Loss: 0.8750757575035095\n",
      "113:Epoch: 14, Train_Loss: 0.5458125472068787, Test_Loss: 0.6479026079177856 *\n",
      "114:Epoch: 14, Train_Loss: 0.5215299129486084, Test_Loss: 0.7794458270072937\n",
      "115:Epoch: 14, Train_Loss: 0.5311435461044312, Test_Loss: 0.5325425267219543 *\n",
      "116:Epoch: 14, Train_Loss: 0.5183572173118591, Test_Loss: 0.5289199352264404 *\n",
      "117:Epoch: 14, Train_Loss: 0.5143475532531738, Test_Loss: 0.5286135077476501 *\n",
      "118:Epoch: 14, Train_Loss: 0.5242496728897095, Test_Loss: 0.5291365385055542\n",
      "119:Epoch: 14, Train_Loss: 0.5316444039344788, Test_Loss: 0.5679242014884949\n",
      "120:Epoch: 14, Train_Loss: 0.5533819794654846, Test_Loss: 4.19618034362793\n",
      "121:Epoch: 14, Train_Loss: 0.6053085327148438, Test_Loss: 2.1762266159057617 *\n",
      "122:Epoch: 14, Train_Loss: 0.5260204672813416, Test_Loss: 0.5222454071044922 *\n",
      "123:Epoch: 14, Train_Loss: 0.5593182444572449, Test_Loss: 0.5148710608482361 *\n",
      "124:Epoch: 14, Train_Loss: 0.5376019477844238, Test_Loss: 0.5146604776382446 *\n",
      "125:Epoch: 14, Train_Loss: 0.5286592841148376, Test_Loss: 0.5254217982292175\n",
      "126:Epoch: 14, Train_Loss: 0.6079434156417847, Test_Loss: 0.5135200619697571 *\n",
      "127:Epoch: 14, Train_Loss: 0.7520314455032349, Test_Loss: 0.5224331617355347\n",
      "128:Epoch: 14, Train_Loss: 0.5195667743682861, Test_Loss: 0.5108742117881775 *\n",
      "129:Epoch: 14, Train_Loss: 0.547911524772644, Test_Loss: 0.51274174451828\n",
      "130:Epoch: 14, Train_Loss: 0.5091252326965332, Test_Loss: 0.5147938132286072\n",
      "131:Epoch: 14, Train_Loss: 0.508703887462616, Test_Loss: 0.5142204761505127 *\n",
      "132:Epoch: 14, Train_Loss: 0.5093945860862732, Test_Loss: 0.5215038061141968\n",
      "133:Epoch: 14, Train_Loss: 0.5086979866027832, Test_Loss: 0.5331019759178162\n",
      "134:Epoch: 14, Train_Loss: 0.523283839225769, Test_Loss: 0.5208157896995544 *\n",
      "135:Epoch: 14, Train_Loss: 0.5258734226226807, Test_Loss: 0.5170198678970337 *\n",
      "136:Epoch: 14, Train_Loss: 0.5193958878517151, Test_Loss: 0.5093614459037781 *\n",
      "137:Epoch: 14, Train_Loss: 0.5184915065765381, Test_Loss: 0.5082810521125793 *\n",
      "138:Epoch: 14, Train_Loss: 0.5270616412162781, Test_Loss: 0.510044515132904\n",
      "139:Epoch: 14, Train_Loss: 0.5108102560043335, Test_Loss: 0.5089927315711975 *\n",
      "140:Epoch: 14, Train_Loss: 0.50887131690979, Test_Loss: 0.5090160965919495\n",
      "141:Epoch: 14, Train_Loss: 0.5068839192390442, Test_Loss: 0.5089747905731201 *\n",
      "142:Epoch: 14, Train_Loss: 0.5365703701972961, Test_Loss: 0.5094177722930908\n",
      "143:Epoch: 14, Train_Loss: 0.5352989435195923, Test_Loss: 0.5100908875465393\n",
      "144:Epoch: 14, Train_Loss: 0.5308544635772705, Test_Loss: 0.5098399519920349 *\n",
      "145:Epoch: 14, Train_Loss: 0.5135955214500427, Test_Loss: 0.5080880522727966 *\n",
      "146:Epoch: 14, Train_Loss: 0.5630801916122437, Test_Loss: 0.5080627202987671 *\n",
      "147:Epoch: 14, Train_Loss: 0.5548810958862305, Test_Loss: 0.507095456123352 *\n",
      "148:Epoch: 14, Train_Loss: 0.5301531553268433, Test_Loss: 0.5076180696487427\n",
      "149:Epoch: 14, Train_Loss: 0.5192864537239075, Test_Loss: 0.5085769295692444\n",
      "150:Epoch: 14, Train_Loss: 0.5309494137763977, Test_Loss: 0.5508992671966553\n",
      "151:Epoch: 14, Train_Loss: 0.5125335454940796, Test_Loss: 0.551613450050354\n",
      "152:Epoch: 14, Train_Loss: 0.5200145840644836, Test_Loss: 5.818568706512451\n",
      "153:Epoch: 14, Train_Loss: 0.5293697118759155, Test_Loss: 0.5978870987892151 *\n",
      "154:Epoch: 14, Train_Loss: 0.5437871217727661, Test_Loss: 0.5068278312683105 *\n",
      "155:Epoch: 14, Train_Loss: 2.471958637237549, Test_Loss: 0.5371545553207397\n",
      "156:Epoch: 14, Train_Loss: 3.9618473052978516, Test_Loss: 0.5654757022857666\n",
      "157:Epoch: 14, Train_Loss: 0.5120480060577393, Test_Loss: 0.5668299794197083\n",
      "158:Epoch: 14, Train_Loss: 0.5159544348716736, Test_Loss: 0.5104360580444336 *\n",
      "159:Epoch: 14, Train_Loss: 0.534131646156311, Test_Loss: 0.601571798324585\n",
      "160:Epoch: 14, Train_Loss: 0.6730772256851196, Test_Loss: 0.5576528310775757 *\n",
      "161:Epoch: 14, Train_Loss: 0.5336655378341675, Test_Loss: 0.5065122842788696 *\n",
      "162:Epoch: 14, Train_Loss: 0.5126057267189026, Test_Loss: 0.538305401802063\n",
      "163:Epoch: 14, Train_Loss: 0.5047077536582947, Test_Loss: 0.5269635319709778 *\n",
      "164:Epoch: 14, Train_Loss: 0.5731388926506042, Test_Loss: 0.5079852938652039 *\n",
      "165:Epoch: 14, Train_Loss: 0.5139291286468506, Test_Loss: 0.5352538228034973\n",
      "166:Epoch: 14, Train_Loss: 0.522121012210846, Test_Loss: 0.6064733266830444\n",
      "167:Epoch: 14, Train_Loss: 1.1045607328414917, Test_Loss: 0.5425544381141663 *\n",
      "168:Epoch: 14, Train_Loss: 1.726591944694519, Test_Loss: 0.5864618420600891\n",
      "169:Epoch: 14, Train_Loss: 1.212317705154419, Test_Loss: 0.5422858595848083 *\n",
      "170:Epoch: 14, Train_Loss: 0.5919321179389954, Test_Loss: 0.5633777976036072\n",
      "171:Epoch: 14, Train_Loss: 0.9939528107643127, Test_Loss: 0.5262425541877747 *\n",
      "172:Epoch: 14, Train_Loss: 2.7455029487609863, Test_Loss: 0.5195938944816589 *\n",
      "173:Epoch: 14, Train_Loss: 1.0812358856201172, Test_Loss: 0.5231797099113464\n",
      "174:Epoch: 14, Train_Loss: 0.5459319949150085, Test_Loss: 0.5365284085273743\n",
      "175:Epoch: 14, Train_Loss: 0.5364387035369873, Test_Loss: 0.5337246060371399 *\n",
      "176:Epoch: 14, Train_Loss: 1.3022470474243164, Test_Loss: 0.5339137315750122\n",
      "177:Epoch: 14, Train_Loss: 1.6226110458374023, Test_Loss: 0.5135014057159424 *\n",
      "178:Epoch: 14, Train_Loss: 0.811254620552063, Test_Loss: 0.5157445669174194\n",
      "179:Epoch: 14, Train_Loss: 0.5102383494377136, Test_Loss: 0.5087997317314148 *\n",
      "180:Epoch: 14, Train_Loss: 0.5085158944129944, Test_Loss: 0.5093119740486145\n",
      "181:Epoch: 14, Train_Loss: 1.0533287525177002, Test_Loss: 0.5111079216003418\n",
      "182:Epoch: 14, Train_Loss: 0.7498602867126465, Test_Loss: 0.528610110282898\n",
      "183:Epoch: 14, Train_Loss: 0.5561959147453308, Test_Loss: 0.5279136896133423 *\n",
      "184:Epoch: 14, Train_Loss: 0.5540661215782166, Test_Loss: 0.7057570219039917\n",
      "185:Epoch: 14, Train_Loss: 0.6207455396652222, Test_Loss: 0.8100998401641846\n",
      "186:Epoch: 14, Train_Loss: 0.5689267516136169, Test_Loss: 0.7264319062232971 *\n",
      "187:Epoch: 14, Train_Loss: 0.600345253944397, Test_Loss: 0.5515673756599426 *\n",
      "188:Epoch: 14, Train_Loss: 0.6790413856506348, Test_Loss: 0.5098705887794495 *\n",
      "189:Epoch: 14, Train_Loss: 0.5168221592903137, Test_Loss: 0.5066037774085999 *\n",
      "190:Epoch: 14, Train_Loss: 0.5855883955955505, Test_Loss: 0.5619283318519592\n",
      "191:Epoch: 14, Train_Loss: 0.6887677907943726, Test_Loss: 0.7686582207679749\n",
      "192:Epoch: 14, Train_Loss: 0.6513254046440125, Test_Loss: 0.8083081245422363\n",
      "193:Epoch: 14, Train_Loss: 0.6566591858863831, Test_Loss: 0.6246099472045898 *\n",
      "194:Epoch: 14, Train_Loss: 0.6585301160812378, Test_Loss: 0.5723947286605835 *\n",
      "195:Epoch: 14, Train_Loss: 0.5765354037284851, Test_Loss: 0.5128300189971924 *\n",
      "196:Epoch: 14, Train_Loss: 0.6318637132644653, Test_Loss: 0.5210030674934387\n",
      "197:Epoch: 14, Train_Loss: 0.5567945241928101, Test_Loss: 0.5155375599861145 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198:Epoch: 14, Train_Loss: 0.5170701742172241, Test_Loss: 0.5812348127365112\n",
      "199:Epoch: 14, Train_Loss: 0.5023554563522339, Test_Loss: 0.5304120182991028 *\n",
      "200:Epoch: 14, Train_Loss: 0.5013382434844971, Test_Loss: 0.5632388591766357\n",
      "Model saved at location ../Saver/model.ckpt at epoch 14\n",
      "201:Epoch: 14, Train_Loss: 0.49980607628822327, Test_Loss: 0.5269176363945007 *\n",
      "202:Epoch: 14, Train_Loss: 0.5085592269897461, Test_Loss: 0.7903966903686523\n",
      "203:Epoch: 14, Train_Loss: 0.5270410180091858, Test_Loss: 0.8830381631851196\n",
      "204:Epoch: 14, Train_Loss: 0.528876543045044, Test_Loss: 0.8375825881958008 *\n",
      "205:Epoch: 14, Train_Loss: 0.5425639152526855, Test_Loss: 1.0491929054260254\n",
      "206:Epoch: 14, Train_Loss: 0.600872278213501, Test_Loss: 0.6446747183799744 *\n",
      "207:Epoch: 14, Train_Loss: 0.7619804739952087, Test_Loss: 0.6424784660339355 *\n",
      "208:Epoch: 14, Train_Loss: 0.5206566452980042, Test_Loss: 0.6414461135864258 *\n",
      "209:Epoch: 14, Train_Loss: 0.5627993941307068, Test_Loss: 0.6660894155502319\n",
      "210:Epoch: 14, Train_Loss: 0.5386903285980225, Test_Loss: 0.7965739369392395\n",
      "211:Epoch: 14, Train_Loss: 0.8445699214935303, Test_Loss: 5.625865936279297\n",
      "212:Epoch: 14, Train_Loss: 0.6610360741615295, Test_Loss: 1.0599123239517212 *\n",
      "213:Epoch: 14, Train_Loss: 0.5177900195121765, Test_Loss: 0.5350282192230225 *\n",
      "214:Epoch: 14, Train_Loss: 0.5506907105445862, Test_Loss: 0.5142987966537476 *\n",
      "215:Epoch: 14, Train_Loss: 0.8525122404098511, Test_Loss: 0.5241672396659851\n",
      "216:Epoch: 14, Train_Loss: 0.7163755893707275, Test_Loss: 0.5082888603210449 *\n",
      "217:Epoch: 14, Train_Loss: 0.5810967683792114, Test_Loss: 0.5671895742416382\n",
      "218:Epoch: 14, Train_Loss: 0.5117469429969788, Test_Loss: 0.6273660659790039\n",
      "219:Epoch: 14, Train_Loss: 0.5572178363800049, Test_Loss: 0.5249512195587158 *\n",
      "220:Epoch: 14, Train_Loss: 0.9166412949562073, Test_Loss: 0.5408130288124084\n",
      "221:Epoch: 14, Train_Loss: 1.0961261987686157, Test_Loss: 0.5507306456565857\n",
      "222:Epoch: 14, Train_Loss: 0.5425782203674316, Test_Loss: 0.628565788269043\n",
      "223:Epoch: 14, Train_Loss: 0.5294527411460876, Test_Loss: 0.5818531513214111 *\n",
      "224:Epoch: 14, Train_Loss: 0.5072141289710999, Test_Loss: 0.5683469772338867 *\n",
      "225:Epoch: 14, Train_Loss: 0.5067300796508789, Test_Loss: 0.6258834600448608\n",
      "226:Epoch: 14, Train_Loss: 0.7899525761604309, Test_Loss: 0.5406531691551208 *\n",
      "227:Epoch: 14, Train_Loss: 0.512793242931366, Test_Loss: 0.5261169672012329 *\n",
      "228:Epoch: 14, Train_Loss: 0.6249526739120483, Test_Loss: 0.5347588658332825\n",
      "229:Epoch: 14, Train_Loss: 0.6171378493309021, Test_Loss: 0.5766534209251404\n",
      "230:Epoch: 14, Train_Loss: 0.5360453128814697, Test_Loss: 0.5351155996322632 *\n",
      "231:Epoch: 14, Train_Loss: 0.5267441272735596, Test_Loss: 0.5203834176063538 *\n",
      "232:Epoch: 14, Train_Loss: 0.5991560816764832, Test_Loss: 0.5492880940437317\n",
      "233:Epoch: 14, Train_Loss: 0.6233854293823242, Test_Loss: 0.5758047699928284\n",
      "234:Epoch: 14, Train_Loss: 0.546768844127655, Test_Loss: 0.5639420747756958 *\n",
      "235:Epoch: 14, Train_Loss: 0.5899958610534668, Test_Loss: 0.5788453817367554\n",
      "236:Epoch: 14, Train_Loss: 0.5134029388427734, Test_Loss: 0.532853901386261 *\n",
      "237:Epoch: 14, Train_Loss: 0.6443594098091125, Test_Loss: 0.5361347198486328\n",
      "238:Epoch: 14, Train_Loss: 0.5499891638755798, Test_Loss: 0.5439544916152954\n",
      "239:Epoch: 14, Train_Loss: 0.543191134929657, Test_Loss: 0.5244859457015991 *\n",
      "240:Epoch: 14, Train_Loss: 0.5109608769416809, Test_Loss: 0.5352957248687744\n",
      "241:Epoch: 14, Train_Loss: 0.5333916544914246, Test_Loss: 0.6062500476837158\n",
      "242:Epoch: 14, Train_Loss: 0.8357025384902954, Test_Loss: 1.0408430099487305\n",
      "243:Epoch: 14, Train_Loss: 0.7803398966789246, Test_Loss: 5.449526309967041\n",
      "244:Epoch: 14, Train_Loss: 0.8697798252105713, Test_Loss: 0.513335645198822 *\n",
      "245:Epoch: 14, Train_Loss: 0.7786465883255005, Test_Loss: 0.5902342200279236\n",
      "246:Epoch: 14, Train_Loss: 0.7190773487091064, Test_Loss: 0.5766093134880066 *\n",
      "247:Epoch: 14, Train_Loss: 0.7348225712776184, Test_Loss: 0.49659815430641174 *\n",
      "248:Epoch: 14, Train_Loss: 0.5596693158149719, Test_Loss: 0.5045403242111206\n",
      "249:Epoch: 14, Train_Loss: 0.5053823590278625, Test_Loss: 0.5143214464187622\n",
      "250:Epoch: 14, Train_Loss: 0.5064511299133301, Test_Loss: 0.5771291255950928\n",
      "251:Epoch: 14, Train_Loss: 0.5342810750007629, Test_Loss: 0.5173965692520142 *\n",
      "252:Epoch: 14, Train_Loss: 0.7399412989616394, Test_Loss: 0.5092170238494873 *\n",
      "253:Epoch: 14, Train_Loss: 0.9187146425247192, Test_Loss: 0.5211213231086731\n",
      "254:Epoch: 14, Train_Loss: 0.979529619216919, Test_Loss: 0.601070761680603\n",
      "255:Epoch: 14, Train_Loss: 1.8057315349578857, Test_Loss: 0.5360837578773499 *\n",
      "256:Epoch: 14, Train_Loss: 0.8173621892929077, Test_Loss: 0.568057656288147\n",
      "257:Epoch: 14, Train_Loss: 0.7837214469909668, Test_Loss: 0.5034645795822144 *\n",
      "258:Epoch: 14, Train_Loss: 0.5180714726448059, Test_Loss: 0.5656700730323792\n",
      "259:Epoch: 14, Train_Loss: 0.5017024874687195, Test_Loss: 0.5016405582427979 *\n",
      "260:Epoch: 14, Train_Loss: 0.8274559378623962, Test_Loss: 0.5597197413444519\n",
      "261:Epoch: 14, Train_Loss: 1.5201547145843506, Test_Loss: 0.6244634985923767\n",
      "262:Epoch: 14, Train_Loss: 0.6663970947265625, Test_Loss: 0.4954635202884674 *\n",
      "263:Epoch: 14, Train_Loss: 0.5445153117179871, Test_Loss: 0.5440440773963928\n",
      "264:Epoch: 14, Train_Loss: 0.5709301829338074, Test_Loss: 0.5285797715187073 *\n",
      "265:Epoch: 14, Train_Loss: 0.5719606280326843, Test_Loss: 0.5379109978675842\n",
      "266:Epoch: 14, Train_Loss: 0.8256562352180481, Test_Loss: 0.5328453779220581 *\n",
      "267:Epoch: 14, Train_Loss: 0.662866473197937, Test_Loss: 0.5280853509902954 *\n",
      "268:Epoch: 14, Train_Loss: 0.7192783951759338, Test_Loss: 0.5298463106155396\n",
      "269:Epoch: 14, Train_Loss: 0.7044212818145752, Test_Loss: 0.5274133682250977 *\n",
      "270:Epoch: 14, Train_Loss: 0.5140982270240784, Test_Loss: 0.5201878547668457 *\n",
      "271:Epoch: 14, Train_Loss: 0.5220613479614258, Test_Loss: 0.5804634690284729\n",
      "272:Epoch: 14, Train_Loss: 0.5222495198249817, Test_Loss: 0.5507736802101135 *\n",
      "273:Epoch: 14, Train_Loss: 0.5291810035705566, Test_Loss: 0.4961070120334625 *\n",
      "274:Epoch: 14, Train_Loss: 0.5109037756919861, Test_Loss: 0.640397310256958\n",
      "275:Epoch: 14, Train_Loss: 0.5485130548477173, Test_Loss: 0.6054531335830688 *\n",
      "276:Epoch: 14, Train_Loss: 11.762978553771973, Test_Loss: 0.5242457985877991 *\n",
      "277:Epoch: 14, Train_Loss: 5.007983207702637, Test_Loss: 0.5315008163452148\n",
      "278:Epoch: 14, Train_Loss: 1.3201817274093628, Test_Loss: 0.5470055341720581\n",
      "279:Epoch: 14, Train_Loss: 1.7097878456115723, Test_Loss: 0.6001976132392883\n",
      "280:Epoch: 14, Train_Loss: 0.647362232208252, Test_Loss: 0.5315911173820496 *\n",
      "281:Epoch: 14, Train_Loss: 0.5580155849456787, Test_Loss: 0.6419631242752075\n",
      "282:Epoch: 14, Train_Loss: 1.05527925491333, Test_Loss: 0.7591156959533691\n",
      "283:Epoch: 14, Train_Loss: 6.452365398406982, Test_Loss: 0.8738908767700195\n",
      "284:Epoch: 14, Train_Loss: 1.3457489013671875, Test_Loss: 0.7204701900482178 *\n",
      "285:Epoch: 14, Train_Loss: 0.5992767810821533, Test_Loss: 0.9211122393608093\n",
      "286:Epoch: 14, Train_Loss: 3.1927459239959717, Test_Loss: 0.8290634155273438 *\n",
      "287:Epoch: 14, Train_Loss: 2.1748015880584717, Test_Loss: 1.4996769428253174\n",
      "288:Epoch: 14, Train_Loss: 0.9407748579978943, Test_Loss: 1.8283460140228271\n",
      "289:Epoch: 14, Train_Loss: 0.5280121564865112, Test_Loss: 1.5043538808822632 *\n",
      "290:Epoch: 14, Train_Loss: 0.5109747648239136, Test_Loss: 1.7026090621948242\n",
      "291:Epoch: 14, Train_Loss: 0.5887077450752258, Test_Loss: 1.5773284435272217 *\n",
      "292:Epoch: 14, Train_Loss: 0.5804170966148376, Test_Loss: 1.864661455154419\n",
      "293:Epoch: 14, Train_Loss: 0.5241317749023438, Test_Loss: 1.440239667892456 *\n",
      "294:Epoch: 14, Train_Loss: 0.488676518201828, Test_Loss: 2.468620777130127\n",
      "295:Epoch: 14, Train_Loss: 0.4854450821876526, Test_Loss: 0.7992820739746094 *\n",
      "296:Epoch: 14, Train_Loss: 0.5255414843559265, Test_Loss: 0.7355601191520691 *\n",
      "297:Epoch: 14, Train_Loss: 0.5430201888084412, Test_Loss: 0.6262936592102051 *\n",
      "298:Epoch: 14, Train_Loss: 0.5379536151885986, Test_Loss: 0.5361968278884888 *\n",
      "299:Epoch: 14, Train_Loss: 0.5903174877166748, Test_Loss: 0.5010268688201904 *\n",
      "300:Epoch: 14, Train_Loss: 0.5269249081611633, Test_Loss: 0.5192247629165649\n",
      "Model saved at location ../Saver/model.ckpt at epoch 14\n",
      "301:Epoch: 14, Train_Loss: 0.5374450087547302, Test_Loss: 0.6768298745155334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302:Epoch: 14, Train_Loss: 0.49992963671684265, Test_Loss: 6.732116222381592\n",
      "303:Epoch: 14, Train_Loss: 0.502666175365448, Test_Loss: 0.8161172866821289 *\n",
      "304:Epoch: 14, Train_Loss: 0.5239030718803406, Test_Loss: 0.8317109942436218\n",
      "305:Epoch: 14, Train_Loss: 0.4900246858596802, Test_Loss: 0.7244963645935059 *\n",
      "306:Epoch: 14, Train_Loss: 0.4918793737888336, Test_Loss: 0.6716083884239197 *\n",
      "307:Epoch: 14, Train_Loss: 0.4854781925678253, Test_Loss: 0.5866076946258545 *\n",
      "308:Epoch: 14, Train_Loss: 0.48499220609664917, Test_Loss: 0.9241794943809509\n",
      "309:Epoch: 14, Train_Loss: 0.4886852204799652, Test_Loss: 0.8410248756408691 *\n",
      "310:Epoch: 14, Train_Loss: 0.4855354130268097, Test_Loss: 0.5273611545562744 *\n",
      "311:Epoch: 14, Train_Loss: 0.48415085673332214, Test_Loss: 0.6065232157707214\n",
      "312:Epoch: 14, Train_Loss: 0.4937111735343933, Test_Loss: 0.617500901222229\n",
      "313:Epoch: 14, Train_Loss: 0.5135700106620789, Test_Loss: 0.820224404335022\n",
      "314:Epoch: 14, Train_Loss: 0.5163292288780212, Test_Loss: 0.6328446865081787 *\n",
      "315:Epoch: 14, Train_Loss: 0.6240418553352356, Test_Loss: 0.7566019892692566\n",
      "316:Epoch: 14, Train_Loss: 0.5228362083435059, Test_Loss: 0.8089420795440674\n",
      "317:Epoch: 14, Train_Loss: 0.593664288520813, Test_Loss: 0.49270567297935486 *\n",
      "318:Epoch: 14, Train_Loss: 8.535317420959473, Test_Loss: 0.5071494579315186\n",
      "319:Epoch: 14, Train_Loss: 0.7184419631958008, Test_Loss: 0.5167694091796875\n",
      "320:Epoch: 14, Train_Loss: 0.5027361512184143, Test_Loss: 0.6999098062515259\n",
      "321:Epoch: 14, Train_Loss: 0.5560736060142517, Test_Loss: 0.5574169754981995 *\n",
      "322:Epoch: 14, Train_Loss: 0.6332176923751831, Test_Loss: 0.6077229976654053\n",
      "323:Epoch: 14, Train_Loss: 0.5115640759468079, Test_Loss: 0.5865012407302856 *\n",
      "324:Epoch: 14, Train_Loss: 0.5180586576461792, Test_Loss: 0.7062283158302307\n",
      "325:Epoch: 14, Train_Loss: 0.5864102840423584, Test_Loss: 0.6754295229911804 *\n",
      "326:Epoch: 14, Train_Loss: 0.7103263139724731, Test_Loss: 0.6522648930549622 *\n",
      "327:Epoch: 14, Train_Loss: 0.619067907333374, Test_Loss: 0.5574216246604919 *\n",
      "328:Epoch: 14, Train_Loss: 0.5548551082611084, Test_Loss: 0.5755358934402466\n",
      "329:Epoch: 14, Train_Loss: 0.48421627283096313, Test_Loss: 0.5506948828697205 *\n",
      "330:Epoch: 14, Train_Loss: 0.544148325920105, Test_Loss: 0.526415228843689 *\n",
      "331:Epoch: 14, Train_Loss: 0.5324366092681885, Test_Loss: 0.5583224296569824\n",
      "332:Epoch: 14, Train_Loss: 0.6999512910842896, Test_Loss: 0.5835877060890198\n",
      "333:Epoch: 14, Train_Loss: 0.5254629254341125, Test_Loss: 2.2978036403656006\n",
      "334:Epoch: 14, Train_Loss: 0.5242303609848022, Test_Loss: 4.414300441741943\n",
      "335:Epoch: 14, Train_Loss: 0.5028697848320007, Test_Loss: 0.4931916892528534 *\n",
      "336:Epoch: 14, Train_Loss: 0.5030887126922607, Test_Loss: 0.4828169643878937 *\n",
      "337:Epoch: 14, Train_Loss: 0.5415956974029541, Test_Loss: 0.5186399221420288\n",
      "338:Epoch: 14, Train_Loss: 0.5098854303359985, Test_Loss: 0.4896218776702881 *\n",
      "339:Epoch: 14, Train_Loss: 0.5013977289199829, Test_Loss: 0.5054123401641846\n",
      "340:Epoch: 14, Train_Loss: 0.503307044506073, Test_Loss: 0.5145407915115356\n",
      "341:Epoch: 14, Train_Loss: 0.49629920721054077, Test_Loss: 0.5741888284683228\n",
      "342:Epoch: 14, Train_Loss: 0.5624845027923584, Test_Loss: 0.4889112114906311 *\n",
      "343:Epoch: 14, Train_Loss: 5.941930770874023, Test_Loss: 0.49291276931762695\n",
      "344:Epoch: 14, Train_Loss: 0.49713850021362305, Test_Loss: 0.5037062168121338\n",
      "345:Epoch: 14, Train_Loss: 0.4822562038898468, Test_Loss: 0.5144686698913574\n",
      "346:Epoch: 14, Train_Loss: 0.4909718930721283, Test_Loss: 0.48684626817703247 *\n",
      "347:Epoch: 14, Train_Loss: 0.483523964881897, Test_Loss: 0.5398988723754883\n",
      "348:Epoch: 14, Train_Loss: 0.47964152693748474, Test_Loss: 0.524336040019989 *\n",
      "349:Epoch: 14, Train_Loss: 0.48201364278793335, Test_Loss: 0.5450188517570496\n",
      "350:Epoch: 14, Train_Loss: 0.48095816373825073, Test_Loss: 0.5408387780189514 *\n",
      "351:Epoch: 14, Train_Loss: 0.5264298319816589, Test_Loss: 0.5021578669548035 *\n",
      "352:Epoch: 14, Train_Loss: 0.4918673038482666, Test_Loss: 0.5298553705215454\n",
      "353:Epoch: 14, Train_Loss: 0.4998849928379059, Test_Loss: 0.4803374409675598 *\n",
      "354:Epoch: 14, Train_Loss: 0.4787071645259857, Test_Loss: 0.48162442445755005\n",
      "355:Epoch: 14, Train_Loss: 0.47841301560401917, Test_Loss: 0.4830009341239929\n",
      "356:Epoch: 14, Train_Loss: 0.4904251992702484, Test_Loss: 0.4912700355052948\n",
      "357:Epoch: 14, Train_Loss: 0.4853348433971405, Test_Loss: 0.48453351855278015 *\n",
      "358:Epoch: 14, Train_Loss: 0.48836377263069153, Test_Loss: 0.483246386051178 *\n",
      "359:Epoch: 14, Train_Loss: 0.4927704930305481, Test_Loss: 0.4820283353328705 *\n",
      "360:Epoch: 14, Train_Loss: 0.5064608454704285, Test_Loss: 0.47928255796432495 *\n",
      "361:Epoch: 14, Train_Loss: 0.4936058819293976, Test_Loss: 0.4832838177680969\n",
      "362:Epoch: 14, Train_Loss: 0.4784703552722931, Test_Loss: 0.48874297738075256\n",
      "363:Epoch: 14, Train_Loss: 0.4778159260749817, Test_Loss: 0.4811236262321472 *\n",
      "364:Epoch: 14, Train_Loss: 0.5777847766876221, Test_Loss: 0.5092822313308716\n",
      "365:Epoch: 14, Train_Loss: 0.5030710101127625, Test_Loss: 0.49970757961273193 *\n",
      "366:Epoch: 14, Train_Loss: 0.5170400738716125, Test_Loss: 0.764975905418396\n",
      "367:Epoch: 14, Train_Loss: 0.5045655965805054, Test_Loss: 0.7744081020355225\n",
      "368:Epoch: 14, Train_Loss: 0.5967168211936951, Test_Loss: 0.6064713001251221 *\n",
      "369:Epoch: 14, Train_Loss: 0.5253656506538391, Test_Loss: 0.48796340823173523 *\n",
      "370:Epoch: 14, Train_Loss: 0.5271534323692322, Test_Loss: 0.50123530626297\n",
      "371:Epoch: 14, Train_Loss: 0.538634181022644, Test_Loss: 0.4951419234275818 *\n",
      "372:Epoch: 14, Train_Loss: 0.7062245607376099, Test_Loss: 0.626517653465271\n",
      "373:Epoch: 14, Train_Loss: 0.506313681602478, Test_Loss: 1.0996938943862915\n",
      "374:Epoch: 14, Train_Loss: 0.48563089966773987, Test_Loss: 1.0073288679122925 *\n",
      "375:Epoch: 14, Train_Loss: 0.4750690758228302, Test_Loss: 0.5412409901618958 *\n",
      "376:Epoch: 14, Train_Loss: 0.47558173537254333, Test_Loss: 0.5205220580101013 *\n",
      "377:Epoch: 14, Train_Loss: 0.4751647114753723, Test_Loss: 0.48226308822631836 *\n",
      "378:Epoch: 14, Train_Loss: 0.4746297299861908, Test_Loss: 0.4868524968624115\n",
      "379:Epoch: 14, Train_Loss: 0.5225180983543396, Test_Loss: 0.48920923471450806\n",
      "380:Epoch: 14, Train_Loss: 5.062516212463379, Test_Loss: 0.48948410153388977\n",
      "381:Epoch: 14, Train_Loss: 0.6055907607078552, Test_Loss: 0.5370103120803833\n",
      "382:Epoch: 14, Train_Loss: 0.4782041907310486, Test_Loss: 0.48264050483703613 *\n",
      "383:Epoch: 14, Train_Loss: 0.4784977436065674, Test_Loss: 0.5008488297462463\n",
      "384:Epoch: 14, Train_Loss: 0.47680655121803284, Test_Loss: 0.6063770055770874\n",
      "385:Epoch: 14, Train_Loss: 0.47658786177635193, Test_Loss: 0.8500310182571411\n",
      "386:Epoch: 14, Train_Loss: 0.4756928086280823, Test_Loss: 0.7813838720321655 *\n",
      "387:Epoch: 14, Train_Loss: 0.4731464087963104, Test_Loss: 0.5029587745666504 *\n",
      "388:Epoch: 14, Train_Loss: 0.4733131229877472, Test_Loss: 0.4925917088985443 *\n",
      "389:Epoch: 14, Train_Loss: 0.4740523099899292, Test_Loss: 0.4924331307411194 *\n",
      "390:Epoch: 14, Train_Loss: 0.5117193460464478, Test_Loss: 0.4932001233100891\n",
      "391:Epoch: 14, Train_Loss: 0.5008547306060791, Test_Loss: 0.5121273994445801\n",
      "392:Epoch: 14, Train_Loss: 0.5438166856765747, Test_Loss: 1.0335392951965332\n",
      "393:Epoch: 14, Train_Loss: 0.5089414119720459, Test_Loss: 5.181734561920166\n",
      "394:Epoch: 14, Train_Loss: 0.4797995388507843, Test_Loss: 0.5078127384185791 *\n",
      "395:Epoch: 14, Train_Loss: 0.5964295268058777, Test_Loss: 0.48108971118927 *\n",
      "396:Epoch: 14, Train_Loss: 0.6970008015632629, Test_Loss: 0.479093074798584 *\n",
      "397:Epoch: 14, Train_Loss: 0.7033064961433411, Test_Loss: 0.4850255846977234\n",
      "398:Epoch: 14, Train_Loss: 0.6774107217788696, Test_Loss: 0.4785541594028473 *\n",
      "399:Epoch: 14, Train_Loss: 0.47462520003318787, Test_Loss: 0.4762406349182129 *\n",
      "400:Epoch: 14, Train_Loss: 0.47158119082450867, Test_Loss: 0.48080670833587646\n",
      "Model saved at location ../Saver/model.ckpt at epoch 14\n",
      "401:Epoch: 14, Train_Loss: 0.47235754132270813, Test_Loss: 0.4732450544834137 *\n",
      "402:Epoch: 14, Train_Loss: 0.48278722167015076, Test_Loss: 0.47550341486930847\n",
      "403:Epoch: 14, Train_Loss: 0.4884675443172455, Test_Loss: 0.476645827293396\n",
      "404:Epoch: 14, Train_Loss: 0.4876841902732849, Test_Loss: 0.49198079109191895\n",
      "405:Epoch: 14, Train_Loss: 0.48007991909980774, Test_Loss: 0.4834997355937958 *\n",
      "406:Epoch: 14, Train_Loss: 0.4705910086631775, Test_Loss: 0.4784674346446991 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407:Epoch: 14, Train_Loss: 0.47690683603286743, Test_Loss: 0.4864612817764282\n",
      "408:Epoch: 14, Train_Loss: 0.4884912669658661, Test_Loss: 0.47099441289901733 *\n",
      "409:Epoch: 14, Train_Loss: 0.6348679065704346, Test_Loss: 0.47125232219696045\n",
      "410:Epoch: 14, Train_Loss: 0.6450225710868835, Test_Loss: 0.47156623005867004\n",
      "411:Epoch: 14, Train_Loss: 0.659623920917511, Test_Loss: 0.4774370491504669\n",
      "412:Epoch: 14, Train_Loss: 0.5094082355499268, Test_Loss: 0.4716871380805969 *\n",
      "413:Epoch: 14, Train_Loss: 0.614656925201416, Test_Loss: 0.4750344157218933\n",
      "414:Epoch: 14, Train_Loss: 0.606295108795166, Test_Loss: 0.47341296076774597 *\n",
      "415:Epoch: 14, Train_Loss: 0.539379358291626, Test_Loss: 0.4773362874984741\n",
      "416:Epoch: 14, Train_Loss: 0.6178705096244812, Test_Loss: 0.4771117866039276 *\n",
      "417:Epoch: 14, Train_Loss: 0.7100641131401062, Test_Loss: 0.4753888249397278 *\n",
      "418:Epoch: 14, Train_Loss: 0.5737996101379395, Test_Loss: 0.47193798422813416 *\n",
      "419:Epoch: 14, Train_Loss: 0.4834728240966797, Test_Loss: 0.4726150631904602\n",
      "420:Epoch: 14, Train_Loss: 2.575956106185913, Test_Loss: 0.4704577922821045 *\n",
      "421:Epoch: 14, Train_Loss: 1.4943382740020752, Test_Loss: 0.47112858295440674\n",
      "422:Epoch: 14, Train_Loss: 0.5075481534004211, Test_Loss: 0.48672083020210266\n",
      "423:Epoch: 14, Train_Loss: 0.5263627171516418, Test_Loss: 0.5267789959907532\n",
      "424:Epoch: 14, Train_Loss: 0.5134007930755615, Test_Loss: 3.3077597618103027\n",
      "425:Epoch: 14, Train_Loss: 0.4995739161968231, Test_Loss: 3.069420576095581 *\n",
      "426:Epoch: 14, Train_Loss: 0.47337159514427185, Test_Loss: 0.4699636995792389 *\n",
      "427:Epoch: 14, Train_Loss: 0.508131206035614, Test_Loss: 0.4696725904941559 *\n",
      "428:Epoch: 14, Train_Loss: 0.6185102462768555, Test_Loss: 0.5309079885482788\n",
      "429:Epoch: 14, Train_Loss: 0.5637845396995544, Test_Loss: 0.5197473168373108 *\n",
      "430:Epoch: 14, Train_Loss: 0.552092432975769, Test_Loss: 0.5110310316085815 *\n",
      "431:Epoch: 14, Train_Loss: 0.5342221856117249, Test_Loss: 0.5194607377052307\n",
      "432:Epoch: 14, Train_Loss: 0.5093368291854858, Test_Loss: 0.556203305721283\n",
      "433:Epoch: 14, Train_Loss: 0.4969080984592438, Test_Loss: 0.47188088297843933 *\n",
      "434:Epoch: 14, Train_Loss: 0.48328328132629395, Test_Loss: 0.4894205927848816\n",
      "435:Epoch: 14, Train_Loss: 0.5026680827140808, Test_Loss: 0.4871216118335724 *\n",
      "436:Epoch: 14, Train_Loss: 0.4956638514995575, Test_Loss: 0.49526068568229675\n",
      "437:Epoch: 14, Train_Loss: 0.4729273319244385, Test_Loss: 0.4743349850177765 *\n",
      "438:Epoch: 14, Train_Loss: 0.4706697165966034, Test_Loss: 0.5703756809234619\n",
      "439:Epoch: 14, Train_Loss: 0.5137796998023987, Test_Loss: 0.5452508926391602 *\n",
      "440:Epoch: 14, Train_Loss: 0.5181017518043518, Test_Loss: 0.5357135534286499 *\n",
      "441:Epoch: 14, Train_Loss: 0.4773077666759491, Test_Loss: 0.5284806489944458 *\n",
      "442:Epoch: 14, Train_Loss: 0.46572941541671753, Test_Loss: 0.49612489342689514 *\n",
      "443:Epoch: 14, Train_Loss: 0.46621251106262207, Test_Loss: 0.5145598649978638\n",
      "444:Epoch: 14, Train_Loss: 0.4654178321361542, Test_Loss: 0.489557683467865 *\n",
      "445:Epoch: 14, Train_Loss: 0.4657643139362335, Test_Loss: 0.4879910945892334 *\n",
      "446:Epoch: 14, Train_Loss: 0.46635597944259644, Test_Loss: 0.4912998378276825\n",
      "447:Epoch: 14, Train_Loss: 0.46702226996421814, Test_Loss: 0.49838703870773315\n",
      "448:Epoch: 14, Train_Loss: 0.47076231241226196, Test_Loss: 0.4938047230243683 *\n",
      "449:Epoch: 14, Train_Loss: 0.4653058648109436, Test_Loss: 0.48727765679359436 *\n",
      "450:Epoch: 14, Train_Loss: 0.46508026123046875, Test_Loss: 0.49256590008735657\n",
      "451:Epoch: 14, Train_Loss: 0.46704065799713135, Test_Loss: 0.4885937571525574 *\n",
      "452:Epoch: 14, Train_Loss: 0.4799802303314209, Test_Loss: 0.48801618814468384 *\n",
      "453:Epoch: 14, Train_Loss: 0.48302161693573, Test_Loss: 0.4707552194595337 *\n",
      "454:Epoch: 14, Train_Loss: 0.4827854633331299, Test_Loss: 0.49883556365966797\n",
      "1:Epoch: 15, Train_Loss: 0.48946812748908997, Test_Loss: 0.5428608059883118 *\n",
      "2:Epoch: 15, Train_Loss: 0.47297701239585876, Test_Loss: 0.5183747410774231 *\n",
      "3:Epoch: 15, Train_Loss: 0.4673507511615753, Test_Loss: 1.013916015625\n",
      "4:Epoch: 15, Train_Loss: 0.4646678566932678, Test_Loss: 0.9449645280838013 *\n",
      "5:Epoch: 15, Train_Loss: 0.47423237562179565, Test_Loss: 0.6061970591545105 *\n",
      "6:Epoch: 15, Train_Loss: 0.4899446964263916, Test_Loss: 0.49288463592529297 *\n",
      "7:Epoch: 15, Train_Loss: 0.46837320923805237, Test_Loss: 0.48598146438598633 *\n",
      "8:Epoch: 15, Train_Loss: 0.4673323333263397, Test_Loss: 0.49386751651763916\n",
      "9:Epoch: 15, Train_Loss: 0.4648900330066681, Test_Loss: 0.711531400680542\n",
      "10:Epoch: 15, Train_Loss: 0.4797447621822357, Test_Loss: 1.1357903480529785\n",
      "11:Epoch: 15, Train_Loss: 0.5185956358909607, Test_Loss: 0.9028494358062744 *\n",
      "12:Epoch: 15, Train_Loss: 0.5164927244186401, Test_Loss: 0.5568809509277344 *\n",
      "13:Epoch: 15, Train_Loss: 0.4908255934715271, Test_Loss: 0.495657742023468 *\n",
      "14:Epoch: 15, Train_Loss: 0.4622318148612976, Test_Loss: 0.4681410491466522 *\n",
      "15:Epoch: 15, Train_Loss: 0.5260835289955139, Test_Loss: 0.4633375108242035 *\n",
      "16:Epoch: 15, Train_Loss: 0.48245978355407715, Test_Loss: 0.47225818037986755\n",
      "17:Epoch: 15, Train_Loss: 0.46581608057022095, Test_Loss: 0.47856608033180237\n",
      "18:Epoch: 15, Train_Loss: 0.47806990146636963, Test_Loss: 0.5125354528427124\n",
      "19:Epoch: 15, Train_Loss: 0.48353633284568787, Test_Loss: 0.4705124795436859 *\n",
      "20:Epoch: 15, Train_Loss: 0.5725741386413574, Test_Loss: 0.5257070064544678\n",
      "21:Epoch: 15, Train_Loss: 0.5417031645774841, Test_Loss: 0.5838096141815186\n",
      "22:Epoch: 15, Train_Loss: 0.5106058120727539, Test_Loss: 0.846620500087738\n",
      "23:Epoch: 15, Train_Loss: 0.4776439070701599, Test_Loss: 0.747164249420166 *\n",
      "24:Epoch: 15, Train_Loss: 0.4680899679660797, Test_Loss: 0.48955467343330383 *\n",
      "25:Epoch: 15, Train_Loss: 0.47886666655540466, Test_Loss: 0.4778369069099426 *\n",
      "26:Epoch: 15, Train_Loss: 0.4628290832042694, Test_Loss: 0.47730541229248047 *\n",
      "27:Epoch: 15, Train_Loss: 0.46759912371635437, Test_Loss: 0.4773661494255066\n",
      "28:Epoch: 15, Train_Loss: 0.48337024450302124, Test_Loss: 0.5065327882766724\n",
      "29:Epoch: 15, Train_Loss: 0.48797884583473206, Test_Loss: 2.0473897457122803\n",
      "30:Epoch: 15, Train_Loss: 0.5680543780326843, Test_Loss: 4.221471309661865\n",
      "31:Epoch: 15, Train_Loss: 0.46392208337783813, Test_Loss: 0.47443217039108276 *\n",
      "32:Epoch: 15, Train_Loss: 0.527866542339325, Test_Loss: 0.46491849422454834 *\n",
      "33:Epoch: 15, Train_Loss: 0.4751463830471039, Test_Loss: 0.4644448161125183 *\n",
      "34:Epoch: 15, Train_Loss: 0.493740439414978, Test_Loss: 0.47085824608802795\n",
      "35:Epoch: 15, Train_Loss: 0.5353605151176453, Test_Loss: 0.4641781747341156 *\n",
      "36:Epoch: 15, Train_Loss: 0.7076376080513, Test_Loss: 0.46676069498062134\n",
      "37:Epoch: 15, Train_Loss: 0.4790962338447571, Test_Loss: 0.46357059478759766 *\n",
      "38:Epoch: 15, Train_Loss: 0.498952716588974, Test_Loss: 0.4616684913635254 *\n",
      "39:Epoch: 15, Train_Loss: 0.4593343436717987, Test_Loss: 0.46390199661254883\n",
      "40:Epoch: 15, Train_Loss: 0.45901036262512207, Test_Loss: 0.46387964487075806 *\n",
      "41:Epoch: 15, Train_Loss: 0.45959392189979553, Test_Loss: 0.4750111401081085\n",
      "42:Epoch: 15, Train_Loss: 0.45866936445236206, Test_Loss: 0.4780242443084717\n",
      "43:Epoch: 15, Train_Loss: 0.4720366895198822, Test_Loss: 0.46784090995788574 *\n",
      "44:Epoch: 15, Train_Loss: 0.4717705249786377, Test_Loss: 0.4755687415599823\n",
      "45:Epoch: 15, Train_Loss: 0.474025160074234, Test_Loss: 0.45864415168762207 *\n",
      "46:Epoch: 15, Train_Loss: 0.4659052789211273, Test_Loss: 0.45884764194488525\n",
      "47:Epoch: 15, Train_Loss: 0.4710887670516968, Test_Loss: 0.45923373103141785\n",
      "48:Epoch: 15, Train_Loss: 0.46805065870285034, Test_Loss: 0.4615006744861603\n",
      "49:Epoch: 15, Train_Loss: 0.45899587869644165, Test_Loss: 0.4590406119823456 *\n",
      "50:Epoch: 15, Train_Loss: 0.45718687772750854, Test_Loss: 0.45939210057258606\n",
      "51:Epoch: 15, Train_Loss: 0.48097220063209534, Test_Loss: 0.4583178758621216 *\n",
      "52:Epoch: 15, Train_Loss: 0.48556676506996155, Test_Loss: 0.46147578954696655\n",
      "53:Epoch: 15, Train_Loss: 0.49304425716400146, Test_Loss: 0.45979225635528564 *\n",
      "54:Epoch: 15, Train_Loss: 0.45723143219947815, Test_Loss: 0.4591633081436157 *\n",
      "55:Epoch: 15, Train_Loss: 0.5050770044326782, Test_Loss: 0.45759886503219604 *\n",
      "56:Epoch: 15, Train_Loss: 0.5077824592590332, Test_Loss: 0.45804741978645325\n",
      "57:Epoch: 15, Train_Loss: 0.49019691348075867, Test_Loss: 0.45748960971832275 *\n",
      "58:Epoch: 15, Train_Loss: 0.46238115429878235, Test_Loss: 0.4575898051261902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59:Epoch: 15, Train_Loss: 0.4923418462276459, Test_Loss: 0.4950559735298157\n",
      "60:Epoch: 15, Train_Loss: 0.4575103521347046, Test_Loss: 0.49806350469589233\n",
      "61:Epoch: 15, Train_Loss: 0.47456568479537964, Test_Loss: 4.3035736083984375\n",
      "62:Epoch: 15, Train_Loss: 0.4646034836769104, Test_Loss: 2.022308826446533 *\n",
      "63:Epoch: 15, Train_Loss: 0.482664555311203, Test_Loss: 0.4580554962158203 *\n",
      "64:Epoch: 15, Train_Loss: 1.3281787633895874, Test_Loss: 0.46774667501449585\n",
      "65:Epoch: 15, Train_Loss: 4.451242923736572, Test_Loss: 0.5232346057891846\n",
      "66:Epoch: 15, Train_Loss: 1.1057440042495728, Test_Loss: 0.5241572260856628\n",
      "67:Epoch: 15, Train_Loss: 0.47158196568489075, Test_Loss: 0.476021945476532 *\n",
      "68:Epoch: 15, Train_Loss: 0.4611697793006897, Test_Loss: 0.5231031179428101\n",
      "69:Epoch: 15, Train_Loss: 0.5998400449752808, Test_Loss: 0.5309720039367676\n",
      "70:Epoch: 15, Train_Loss: 0.5263798236846924, Test_Loss: 0.45904219150543213 *\n",
      "71:Epoch: 15, Train_Loss: 0.4687383770942688, Test_Loss: 0.48572462797164917\n",
      "72:Epoch: 15, Train_Loss: 0.4547358751296997, Test_Loss: 0.4746669828891754 *\n",
      "73:Epoch: 15, Train_Loss: 0.5114819407463074, Test_Loss: 0.4702267646789551 *\n",
      "74:Epoch: 15, Train_Loss: 0.47113677859306335, Test_Loss: 0.45935848355293274 *\n",
      "75:Epoch: 15, Train_Loss: 0.47259172797203064, Test_Loss: 0.5777548551559448\n",
      "76:Epoch: 15, Train_Loss: 0.7328819036483765, Test_Loss: 0.5073890686035156 *\n",
      "77:Epoch: 15, Train_Loss: 1.656833291053772, Test_Loss: 0.5350191593170166\n",
      "78:Epoch: 15, Train_Loss: 1.4600436687469482, Test_Loss: 0.5031946897506714 *\n",
      "79:Epoch: 15, Train_Loss: 0.5076013207435608, Test_Loss: 0.5037704706192017\n",
      "80:Epoch: 15, Train_Loss: 0.5894696116447449, Test_Loss: 0.4962581396102905 *\n",
      "81:Epoch: 15, Train_Loss: 2.678243637084961, Test_Loss: 0.4782083034515381 *\n",
      "82:Epoch: 15, Train_Loss: 1.493263840675354, Test_Loss: 0.4753431975841522 *\n",
      "83:Epoch: 15, Train_Loss: 0.5089104771614075, Test_Loss: 0.48587676882743835\n",
      "84:Epoch: 15, Train_Loss: 0.4877219796180725, Test_Loss: 0.49378252029418945\n",
      "85:Epoch: 15, Train_Loss: 0.9335510730743408, Test_Loss: 0.48298436403274536 *\n",
      "86:Epoch: 15, Train_Loss: 1.4986097812652588, Test_Loss: 0.46600425243377686 *\n",
      "87:Epoch: 15, Train_Loss: 1.1024596691131592, Test_Loss: 0.46827277541160583\n",
      "88:Epoch: 15, Train_Loss: 0.4580667018890381, Test_Loss: 0.46055474877357483 *\n",
      "89:Epoch: 15, Train_Loss: 0.46780484914779663, Test_Loss: 0.4592936038970947 *\n",
      "90:Epoch: 15, Train_Loss: 0.7733820676803589, Test_Loss: 0.46532750129699707\n",
      "91:Epoch: 15, Train_Loss: 0.8753723502159119, Test_Loss: 0.46734997630119324\n",
      "92:Epoch: 15, Train_Loss: 0.4942643940448761, Test_Loss: 0.48508334159851074\n",
      "93:Epoch: 15, Train_Loss: 0.4931362271308899, Test_Loss: 0.587255597114563\n",
      "94:Epoch: 15, Train_Loss: 0.5347617864608765, Test_Loss: 0.7782821655273438\n",
      "95:Epoch: 15, Train_Loss: 0.4748653769493103, Test_Loss: 0.7643097043037415 *\n",
      "96:Epoch: 15, Train_Loss: 0.5759702324867249, Test_Loss: 0.5459122061729431 *\n",
      "97:Epoch: 15, Train_Loss: 0.5502268075942993, Test_Loss: 0.4581416845321655 *\n",
      "98:Epoch: 15, Train_Loss: 0.4977390468120575, Test_Loss: 0.4773387312889099\n",
      "99:Epoch: 15, Train_Loss: 0.5273258686065674, Test_Loss: 0.47751089930534363\n",
      "100:Epoch: 15, Train_Loss: 0.5980484485626221, Test_Loss: 0.6343104839324951\n",
      "Model saved at location ../Saver/model.ckpt at epoch 15\n",
      "101:Epoch: 15, Train_Loss: 0.5483699440956116, Test_Loss: 0.6802613735198975\n",
      "102:Epoch: 15, Train_Loss: 0.630325973033905, Test_Loss: 0.7448458671569824\n",
      "103:Epoch: 15, Train_Loss: 0.6652814149856567, Test_Loss: 0.5514365434646606 *\n",
      "104:Epoch: 15, Train_Loss: 0.4905998706817627, Test_Loss: 0.4751039147377014 *\n",
      "105:Epoch: 15, Train_Loss: 0.5750540494918823, Test_Loss: 0.4779800474643707\n",
      "106:Epoch: 15, Train_Loss: 0.5184414386749268, Test_Loss: 0.4750807285308838 *\n",
      "107:Epoch: 15, Train_Loss: 0.48855307698249817, Test_Loss: 0.5361223816871643\n",
      "108:Epoch: 15, Train_Loss: 0.4561591148376465, Test_Loss: 0.48448264598846436 *\n",
      "109:Epoch: 15, Train_Loss: 0.4511706233024597, Test_Loss: 0.5054270625114441\n",
      "110:Epoch: 15, Train_Loss: 0.4514205753803253, Test_Loss: 0.5183271169662476\n",
      "111:Epoch: 15, Train_Loss: 0.4598824679851532, Test_Loss: 0.7093758583068848\n",
      "112:Epoch: 15, Train_Loss: 0.46629899740219116, Test_Loss: 0.7117759585380554\n",
      "113:Epoch: 15, Train_Loss: 0.48355573415756226, Test_Loss: 0.7741304039955139\n",
      "114:Epoch: 15, Train_Loss: 0.4943530261516571, Test_Loss: 1.264230728149414\n",
      "115:Epoch: 15, Train_Loss: 0.5281152129173279, Test_Loss: 0.7121611833572388 *\n",
      "116:Epoch: 15, Train_Loss: 0.6012836694717407, Test_Loss: 0.6981168985366821 *\n",
      "117:Epoch: 15, Train_Loss: 0.6528691649436951, Test_Loss: 0.6953393220901489 *\n",
      "118:Epoch: 15, Train_Loss: 0.48578399419784546, Test_Loss: 0.6893410086631775 *\n",
      "119:Epoch: 15, Train_Loss: 0.4839123487472534, Test_Loss: 0.8610764741897583\n",
      "120:Epoch: 15, Train_Loss: 0.6874440908432007, Test_Loss: 3.297853708267212\n",
      "121:Epoch: 15, Train_Loss: 0.5695482492446899, Test_Loss: 2.8087875843048096 *\n",
      "122:Epoch: 15, Train_Loss: 0.49063777923583984, Test_Loss: 0.4882296919822693 *\n",
      "123:Epoch: 15, Train_Loss: 0.5139113068580627, Test_Loss: 0.46557533740997314 *\n",
      "124:Epoch: 15, Train_Loss: 0.8334635496139526, Test_Loss: 0.48436349630355835\n",
      "125:Epoch: 15, Train_Loss: 0.6760677099227905, Test_Loss: 0.45833033323287964 *\n",
      "126:Epoch: 15, Train_Loss: 0.6037341356277466, Test_Loss: 0.5154193043708801\n",
      "127:Epoch: 15, Train_Loss: 0.46755146980285645, Test_Loss: 0.5807260870933533\n",
      "128:Epoch: 15, Train_Loss: 0.5126763582229614, Test_Loss: 0.49503448605537415 *\n",
      "129:Epoch: 15, Train_Loss: 0.7185078263282776, Test_Loss: 0.4942859411239624 *\n",
      "130:Epoch: 15, Train_Loss: 1.219224452972412, Test_Loss: 0.503925085067749\n",
      "131:Epoch: 15, Train_Loss: 0.6222110986709595, Test_Loss: 0.5198543667793274\n",
      "132:Epoch: 15, Train_Loss: 0.48824208974838257, Test_Loss: 0.6098795533180237\n",
      "133:Epoch: 15, Train_Loss: 0.47187185287475586, Test_Loss: 0.49242478609085083 *\n",
      "134:Epoch: 15, Train_Loss: 0.45484259724617004, Test_Loss: 0.5854255557060242\n",
      "135:Epoch: 15, Train_Loss: 0.7514529228210449, Test_Loss: 0.5689080953598022 *\n",
      "136:Epoch: 15, Train_Loss: 0.5169488787651062, Test_Loss: 0.46970438957214355 *\n",
      "137:Epoch: 15, Train_Loss: 0.47120755910873413, Test_Loss: 0.4724608361721039\n",
      "138:Epoch: 15, Train_Loss: 0.7049844861030579, Test_Loss: 0.5254770517349243\n",
      "139:Epoch: 15, Train_Loss: 0.47957760095596313, Test_Loss: 0.5106657147407532 *\n",
      "140:Epoch: 15, Train_Loss: 0.47256186604499817, Test_Loss: 0.4664938747882843 *\n",
      "141:Epoch: 15, Train_Loss: 0.49890846014022827, Test_Loss: 0.49649932980537415\n",
      "142:Epoch: 15, Train_Loss: 0.6168546080589294, Test_Loss: 0.5029586553573608\n",
      "143:Epoch: 15, Train_Loss: 0.4910833239555359, Test_Loss: 0.5292555689811707\n",
      "144:Epoch: 15, Train_Loss: 0.6449030041694641, Test_Loss: 0.5398120284080505\n",
      "145:Epoch: 15, Train_Loss: 0.46981897950172424, Test_Loss: 0.5014574527740479 *\n",
      "146:Epoch: 15, Train_Loss: 0.6540172696113586, Test_Loss: 0.4802151918411255 *\n",
      "147:Epoch: 15, Train_Loss: 0.5153038501739502, Test_Loss: 0.48660773038864136\n",
      "148:Epoch: 15, Train_Loss: 0.4821585714817047, Test_Loss: 0.48364749550819397 *\n",
      "149:Epoch: 15, Train_Loss: 0.45192810893058777, Test_Loss: 0.47200143337249756 *\n",
      "150:Epoch: 15, Train_Loss: 0.5067749619483948, Test_Loss: 0.5590698719024658\n",
      "151:Epoch: 15, Train_Loss: 0.7108460664749146, Test_Loss: 0.5550989508628845 *\n",
      "152:Epoch: 15, Train_Loss: 0.6724669933319092, Test_Loss: 5.945204257965088\n",
      "153:Epoch: 15, Train_Loss: 0.7154004573822021, Test_Loss: 0.8072155714035034 *\n",
      "154:Epoch: 15, Train_Loss: 0.8683987855911255, Test_Loss: 0.5127077102661133 *\n",
      "155:Epoch: 15, Train_Loss: 0.7075982689857483, Test_Loss: 0.5177093744277954\n",
      "156:Epoch: 15, Train_Loss: 0.6602734923362732, Test_Loss: 0.4579385221004486 *\n",
      "157:Epoch: 15, Train_Loss: 0.5173118710517883, Test_Loss: 0.4548237919807434 *\n",
      "158:Epoch: 15, Train_Loss: 0.461533784866333, Test_Loss: 0.45957157015800476\n",
      "159:Epoch: 15, Train_Loss: 0.4635361135005951, Test_Loss: 0.5356192588806152\n",
      "160:Epoch: 15, Train_Loss: 0.45713570713996887, Test_Loss: 0.49207618832588196 *\n",
      "161:Epoch: 15, Train_Loss: 0.6251852512359619, Test_Loss: 0.4510256350040436 *\n",
      "162:Epoch: 15, Train_Loss: 0.820264995098114, Test_Loss: 0.4636380672454834\n",
      "163:Epoch: 15, Train_Loss: 0.7230732440948486, Test_Loss: 0.5139585733413696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164:Epoch: 15, Train_Loss: 1.8636586666107178, Test_Loss: 0.5082672238349915 *\n",
      "165:Epoch: 15, Train_Loss: 1.0305582284927368, Test_Loss: 0.4807961583137512 *\n",
      "166:Epoch: 15, Train_Loss: 0.7568008899688721, Test_Loss: 0.4463060200214386 *\n",
      "167:Epoch: 15, Train_Loss: 0.5291125774383545, Test_Loss: 0.5031868815422058\n",
      "168:Epoch: 15, Train_Loss: 0.4528558552265167, Test_Loss: 0.48337242007255554 *\n",
      "169:Epoch: 15, Train_Loss: 0.6390417814254761, Test_Loss: 0.47750213742256165 *\n",
      "170:Epoch: 15, Train_Loss: 1.160679817199707, Test_Loss: 0.5804503560066223\n",
      "171:Epoch: 15, Train_Loss: 1.0428521633148193, Test_Loss: 0.4723116159439087 *\n",
      "172:Epoch: 15, Train_Loss: 0.4889548718929291, Test_Loss: 0.49227994680404663\n",
      "173:Epoch: 15, Train_Loss: 0.5061991810798645, Test_Loss: 0.4739944040775299 *\n",
      "174:Epoch: 15, Train_Loss: 0.5171576142311096, Test_Loss: 0.4830770790576935\n",
      "175:Epoch: 15, Train_Loss: 0.765864372253418, Test_Loss: 0.4787399470806122 *\n",
      "176:Epoch: 15, Train_Loss: 0.6045874953269958, Test_Loss: 0.47505834698677063 *\n",
      "177:Epoch: 15, Train_Loss: 0.6077800989151001, Test_Loss: 0.49493247270584106\n",
      "178:Epoch: 15, Train_Loss: 0.5751363039016724, Test_Loss: 0.4689213037490845 *\n",
      "179:Epoch: 15, Train_Loss: 0.47959598898887634, Test_Loss: 0.4875788390636444\n",
      "180:Epoch: 15, Train_Loss: 0.4806032180786133, Test_Loss: 0.5128001570701599\n",
      "181:Epoch: 15, Train_Loss: 0.4851152300834656, Test_Loss: 0.546470582485199\n",
      "182:Epoch: 15, Train_Loss: 0.4661512076854706, Test_Loss: 0.4555964767932892 *\n",
      "183:Epoch: 15, Train_Loss: 0.49105304479599, Test_Loss: 0.5672906637191772\n",
      "184:Epoch: 15, Train_Loss: 0.5072249174118042, Test_Loss: 0.5706393718719482\n",
      "185:Epoch: 15, Train_Loss: 1.0466067790985107, Test_Loss: 0.4823189675807953 *\n",
      "186:Epoch: 15, Train_Loss: 15.604350090026855, Test_Loss: 0.5256401300430298\n",
      "187:Epoch: 15, Train_Loss: 0.6478351950645447, Test_Loss: 0.5030689239501953 *\n",
      "188:Epoch: 15, Train_Loss: 1.784698724746704, Test_Loss: 0.5528640151023865\n",
      "189:Epoch: 15, Train_Loss: 1.1283695697784424, Test_Loss: 0.46481406688690186 *\n",
      "190:Epoch: 15, Train_Loss: 0.47366464138031006, Test_Loss: 0.5737304091453552\n",
      "191:Epoch: 15, Train_Loss: 0.640675961971283, Test_Loss: 0.730636477470398\n",
      "192:Epoch: 15, Train_Loss: 4.992679119110107, Test_Loss: 0.6131581664085388 *\n",
      "193:Epoch: 15, Train_Loss: 2.6825625896453857, Test_Loss: 0.8267730474472046\n",
      "194:Epoch: 15, Train_Loss: 0.5340657830238342, Test_Loss: 0.8733319044113159\n",
      "195:Epoch: 15, Train_Loss: 1.3673168420791626, Test_Loss: 0.5807464122772217 *\n",
      "196:Epoch: 15, Train_Loss: 3.946101427078247, Test_Loss: 1.6370627880096436\n",
      "197:Epoch: 15, Train_Loss: 0.9966385364532471, Test_Loss: 1.6190297603607178 *\n",
      "198:Epoch: 15, Train_Loss: 0.48864948749542236, Test_Loss: 1.540258765220642 *\n",
      "199:Epoch: 15, Train_Loss: 0.49155107140541077, Test_Loss: 1.5808260440826416\n",
      "200:Epoch: 15, Train_Loss: 0.6119368672370911, Test_Loss: 2.386387348175049\n",
      "Model saved at location ../Saver/model.ckpt at epoch 15\n",
      "201:Epoch: 15, Train_Loss: 0.5798889994621277, Test_Loss: 1.6375809907913208 *\n",
      "202:Epoch: 15, Train_Loss: 0.4601300060749054, Test_Loss: 1.107776403427124 *\n",
      "203:Epoch: 15, Train_Loss: 0.4423423707485199, Test_Loss: 2.2781014442443848\n",
      "204:Epoch: 15, Train_Loss: 0.43860653042793274, Test_Loss: 0.8655360341072083 *\n",
      "205:Epoch: 15, Train_Loss: 0.4438292980194092, Test_Loss: 0.5534723997116089 *\n",
      "206:Epoch: 15, Train_Loss: 0.46822306513786316, Test_Loss: 0.48654574155807495 *\n",
      "207:Epoch: 15, Train_Loss: 0.48290401697158813, Test_Loss: 0.44394299387931824 *\n",
      "208:Epoch: 15, Train_Loss: 0.5669462084770203, Test_Loss: 0.44013911485671997 *\n",
      "209:Epoch: 15, Train_Loss: 0.47035452723503113, Test_Loss: 0.47046685218811035\n",
      "210:Epoch: 15, Train_Loss: 0.5480730533599854, Test_Loss: 0.6244434118270874\n",
      "211:Epoch: 15, Train_Loss: 0.4543812572956085, Test_Loss: 6.010071277618408\n",
      "212:Epoch: 15, Train_Loss: 0.47998765110969543, Test_Loss: 2.287748336791992 *\n",
      "213:Epoch: 15, Train_Loss: 0.4992792308330536, Test_Loss: 1.0020554065704346 *\n",
      "214:Epoch: 15, Train_Loss: 0.45181795954704285, Test_Loss: 0.912319540977478 *\n",
      "215:Epoch: 15, Train_Loss: 0.45570409297943115, Test_Loss: 0.9802221655845642\n",
      "216:Epoch: 15, Train_Loss: 0.44141021370887756, Test_Loss: 0.5399155616760254 *\n",
      "217:Epoch: 15, Train_Loss: 0.4394814372062683, Test_Loss: 1.2297611236572266\n",
      "218:Epoch: 15, Train_Loss: 0.4451712667942047, Test_Loss: 1.1864008903503418 *\n",
      "219:Epoch: 15, Train_Loss: 0.4433480501174927, Test_Loss: 0.6453263759613037 *\n",
      "220:Epoch: 15, Train_Loss: 0.43917953968048096, Test_Loss: 0.5928406119346619 *\n",
      "221:Epoch: 15, Train_Loss: 0.45225846767425537, Test_Loss: 0.6012061834335327\n",
      "222:Epoch: 15, Train_Loss: 0.4712006747722626, Test_Loss: 0.8001548647880554\n",
      "223:Epoch: 15, Train_Loss: 0.48251473903656006, Test_Loss: 0.8691375255584717\n",
      "224:Epoch: 15, Train_Loss: 0.620667040348053, Test_Loss: 0.7285774946212769 *\n",
      "225:Epoch: 15, Train_Loss: 0.4717945158481598, Test_Loss: 0.9686617851257324\n",
      "226:Epoch: 15, Train_Loss: 0.5952373743057251, Test_Loss: 0.6503761410713196 *\n",
      "227:Epoch: 15, Train_Loss: 6.666401386260986, Test_Loss: 0.4435573220252991 *\n",
      "228:Epoch: 15, Train_Loss: 2.6686766147613525, Test_Loss: 0.4754652976989746\n",
      "229:Epoch: 15, Train_Loss: 0.4641539752483368, Test_Loss: 0.5565937757492065\n",
      "230:Epoch: 15, Train_Loss: 0.49621376395225525, Test_Loss: 0.565298855304718\n",
      "231:Epoch: 15, Train_Loss: 0.5573530793190002, Test_Loss: 0.4988662600517273 *\n",
      "232:Epoch: 15, Train_Loss: 0.48275142908096313, Test_Loss: 0.5356019735336304\n",
      "233:Epoch: 15, Train_Loss: 0.4692993462085724, Test_Loss: 0.6002092361450195\n",
      "234:Epoch: 15, Train_Loss: 0.5174462795257568, Test_Loss: 0.5552085041999817 *\n",
      "235:Epoch: 15, Train_Loss: 0.5846419930458069, Test_Loss: 0.6049642562866211\n",
      "236:Epoch: 15, Train_Loss: 0.5827900767326355, Test_Loss: 0.5260974168777466 *\n",
      "237:Epoch: 15, Train_Loss: 0.5006926655769348, Test_Loss: 0.5019865036010742 *\n",
      "238:Epoch: 15, Train_Loss: 0.45099470019340515, Test_Loss: 0.4910103678703308 *\n",
      "239:Epoch: 15, Train_Loss: 0.4587540328502655, Test_Loss: 0.49042460322380066 *\n",
      "240:Epoch: 15, Train_Loss: 0.4607071280479431, Test_Loss: 0.49510499835014343\n",
      "241:Epoch: 15, Train_Loss: 0.6137129664421082, Test_Loss: 0.5467619299888611\n",
      "242:Epoch: 15, Train_Loss: 0.4450490176677704, Test_Loss: 0.6135117411613464\n",
      "243:Epoch: 15, Train_Loss: 0.4598563313484192, Test_Loss: 6.242669582366943\n",
      "244:Epoch: 15, Train_Loss: 0.46510112285614014, Test_Loss: 0.4888603091239929 *\n",
      "245:Epoch: 15, Train_Loss: 0.4467717409133911, Test_Loss: 0.44079819321632385 *\n",
      "246:Epoch: 15, Train_Loss: 0.4667285680770874, Test_Loss: 0.456921249628067\n",
      "247:Epoch: 15, Train_Loss: 0.46894556283950806, Test_Loss: 0.442696750164032 *\n",
      "248:Epoch: 15, Train_Loss: 0.4927857518196106, Test_Loss: 0.44732531905174255\n",
      "249:Epoch: 15, Train_Loss: 0.47239047288894653, Test_Loss: 0.4457930028438568 *\n",
      "250:Epoch: 15, Train_Loss: 0.4556262791156769, Test_Loss: 0.5401614904403687\n",
      "251:Epoch: 15, Train_Loss: 0.4695034325122833, Test_Loss: 0.47378167510032654 *\n",
      "252:Epoch: 15, Train_Loss: 5.162810802459717, Test_Loss: 0.436995267868042 *\n",
      "253:Epoch: 15, Train_Loss: 1.1210622787475586, Test_Loss: 0.463691383600235\n",
      "254:Epoch: 15, Train_Loss: 0.4355105459690094, Test_Loss: 0.46406957507133484\n",
      "255:Epoch: 15, Train_Loss: 0.448598176240921, Test_Loss: 0.4371113181114197 *\n",
      "256:Epoch: 15, Train_Loss: 0.4425395131111145, Test_Loss: 0.4727958142757416\n",
      "257:Epoch: 15, Train_Loss: 0.43496301770210266, Test_Loss: 0.4636274576187134 *\n",
      "258:Epoch: 15, Train_Loss: 0.4434741139411926, Test_Loss: 0.4867808520793915\n",
      "259:Epoch: 15, Train_Loss: 0.43701693415641785, Test_Loss: 0.49839526414871216\n",
      "260:Epoch: 15, Train_Loss: 0.4688969850540161, Test_Loss: 0.4719495177268982 *\n",
      "261:Epoch: 15, Train_Loss: 0.4432085156440735, Test_Loss: 0.4948083758354187\n",
      "262:Epoch: 15, Train_Loss: 0.4693494141101837, Test_Loss: 0.43908125162124634 *\n",
      "263:Epoch: 15, Train_Loss: 0.43338459730148315, Test_Loss: 0.4449704587459564\n",
      "264:Epoch: 15, Train_Loss: 0.4329766035079956, Test_Loss: 0.4427083730697632 *\n",
      "265:Epoch: 15, Train_Loss: 0.44165241718292236, Test_Loss: 0.4478064179420471\n",
      "266:Epoch: 15, Train_Loss: 0.44366738200187683, Test_Loss: 0.4384220838546753 *\n",
      "267:Epoch: 15, Train_Loss: 0.4401346743106842, Test_Loss: 0.4382691979408264 *\n",
      "268:Epoch: 15, Train_Loss: 0.44463586807250977, Test_Loss: 0.43871748447418213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269:Epoch: 15, Train_Loss: 0.46481043100357056, Test_Loss: 0.4363026022911072 *\n",
      "270:Epoch: 15, Train_Loss: 0.45273756980895996, Test_Loss: 0.4361816346645355 *\n",
      "271:Epoch: 15, Train_Loss: 0.433392196893692, Test_Loss: 0.4442755877971649\n",
      "272:Epoch: 15, Train_Loss: 0.43199917674064636, Test_Loss: 0.4409830868244171 *\n",
      "273:Epoch: 15, Train_Loss: 0.5159044861793518, Test_Loss: 0.44544169306755066\n",
      "274:Epoch: 15, Train_Loss: 0.4872002899646759, Test_Loss: 0.4553513526916504\n",
      "275:Epoch: 15, Train_Loss: 0.46592089533805847, Test_Loss: 0.6264424324035645\n",
      "276:Epoch: 15, Train_Loss: 0.461668998003006, Test_Loss: 0.696770429611206\n",
      "277:Epoch: 15, Train_Loss: 0.5121122598648071, Test_Loss: 0.5827651023864746 *\n",
      "278:Epoch: 15, Train_Loss: 0.4932512640953064, Test_Loss: 0.45968151092529297 *\n",
      "279:Epoch: 15, Train_Loss: 0.47101980447769165, Test_Loss: 0.44029614329338074 *\n",
      "280:Epoch: 15, Train_Loss: 0.5166274905204773, Test_Loss: 0.4512622654438019\n",
      "281:Epoch: 15, Train_Loss: 0.6795164346694946, Test_Loss: 0.5408039093017578\n",
      "282:Epoch: 15, Train_Loss: 0.47790488600730896, Test_Loss: 0.9732362031936646\n",
      "283:Epoch: 15, Train_Loss: 0.4502321481704712, Test_Loss: 1.0523431301116943\n",
      "284:Epoch: 15, Train_Loss: 0.4384598433971405, Test_Loss: 0.5199806094169617 *\n",
      "285:Epoch: 15, Train_Loss: 0.43091073632240295, Test_Loss: 0.4851592779159546 *\n",
      "286:Epoch: 15, Train_Loss: 0.4308958053588867, Test_Loss: 0.43682029843330383 *\n",
      "287:Epoch: 15, Train_Loss: 0.430177241563797, Test_Loss: 0.45854756236076355\n",
      "288:Epoch: 15, Train_Loss: 0.43366268277168274, Test_Loss: 0.4498389959335327 *\n",
      "289:Epoch: 15, Train_Loss: 4.7095255851745605, Test_Loss: 0.4561818242073059\n",
      "290:Epoch: 15, Train_Loss: 1.186939001083374, Test_Loss: 0.47180357575416565\n",
      "291:Epoch: 15, Train_Loss: 0.43098685145378113, Test_Loss: 0.45577847957611084 *\n",
      "292:Epoch: 15, Train_Loss: 0.43803566694259644, Test_Loss: 0.43497851490974426 *\n",
      "293:Epoch: 15, Train_Loss: 0.4323023855686188, Test_Loss: 0.5504816174507141\n",
      "294:Epoch: 15, Train_Loss: 0.43066421151161194, Test_Loss: 0.8217787742614746\n",
      "295:Epoch: 15, Train_Loss: 0.4295186698436737, Test_Loss: 0.5602609515190125 *\n",
      "296:Epoch: 15, Train_Loss: 0.4287024736404419, Test_Loss: 0.6091514825820923\n",
      "297:Epoch: 15, Train_Loss: 0.4283212423324585, Test_Loss: 0.4473668336868286 *\n",
      "298:Epoch: 15, Train_Loss: 0.42932888865470886, Test_Loss: 0.4472416937351227 *\n",
      "299:Epoch: 15, Train_Loss: 0.4549332857131958, Test_Loss: 0.44771650433540344\n",
      "300:Epoch: 15, Train_Loss: 0.4744985103607178, Test_Loss: 0.45814311504364014\n",
      "Model saved at location ../Saver/model.ckpt at epoch 15\n",
      "301:Epoch: 15, Train_Loss: 0.4960199296474457, Test_Loss: 0.48839423060417175\n",
      "302:Epoch: 15, Train_Loss: 0.4954730272293091, Test_Loss: 5.529600143432617\n",
      "303:Epoch: 15, Train_Loss: 0.43560731410980225, Test_Loss: 0.6816017627716064 *\n",
      "304:Epoch: 15, Train_Loss: 0.47978413105010986, Test_Loss: 0.4395166039466858 *\n",
      "305:Epoch: 15, Train_Loss: 0.6485776901245117, Test_Loss: 0.4325667917728424 *\n",
      "306:Epoch: 15, Train_Loss: 0.6481689810752869, Test_Loss: 0.43336427211761475\n",
      "307:Epoch: 15, Train_Loss: 0.6593215465545654, Test_Loss: 0.4395322799682617\n",
      "308:Epoch: 15, Train_Loss: 0.45053303241729736, Test_Loss: 0.4296947121620178 *\n",
      "309:Epoch: 15, Train_Loss: 0.42695924639701843, Test_Loss: 0.438865602016449\n",
      "310:Epoch: 15, Train_Loss: 0.4269729554653168, Test_Loss: 0.4288862347602844 *\n",
      "311:Epoch: 15, Train_Loss: 0.4336739182472229, Test_Loss: 0.42865607142448425 *\n",
      "312:Epoch: 15, Train_Loss: 0.43965524435043335, Test_Loss: 0.43468475341796875\n",
      "313:Epoch: 15, Train_Loss: 0.43941596150398254, Test_Loss: 0.44756805896759033\n",
      "314:Epoch: 15, Train_Loss: 0.4380239248275757, Test_Loss: 0.43184536695480347 *\n",
      "315:Epoch: 15, Train_Loss: 0.42657625675201416, Test_Loss: 0.44465988874435425\n",
      "316:Epoch: 15, Train_Loss: 0.42798498272895813, Test_Loss: 0.4405452013015747 *\n",
      "317:Epoch: 15, Train_Loss: 0.4388973116874695, Test_Loss: 0.4271901845932007 *\n",
      "318:Epoch: 15, Train_Loss: 0.5446140766143799, Test_Loss: 0.42741915583610535\n",
      "319:Epoch: 15, Train_Loss: 0.5977897644042969, Test_Loss: 0.4276975989341736\n",
      "320:Epoch: 15, Train_Loss: 0.5823585987091064, Test_Loss: 0.43364638090133667\n",
      "321:Epoch: 15, Train_Loss: 0.4915243089199066, Test_Loss: 0.42854222655296326 *\n",
      "322:Epoch: 15, Train_Loss: 0.5619038939476013, Test_Loss: 0.4315352737903595\n",
      "323:Epoch: 15, Train_Loss: 0.5927683711051941, Test_Loss: 0.43228209018707275\n",
      "324:Epoch: 15, Train_Loss: 0.4484504461288452, Test_Loss: 0.4379209280014038\n",
      "325:Epoch: 15, Train_Loss: 0.6213578581809998, Test_Loss: 0.4392247796058655\n",
      "326:Epoch: 15, Train_Loss: 0.5515939593315125, Test_Loss: 0.43675294518470764 *\n",
      "327:Epoch: 15, Train_Loss: 0.6829724311828613, Test_Loss: 0.4300616979598999 *\n",
      "328:Epoch: 15, Train_Loss: 0.4416731595993042, Test_Loss: 0.4314742088317871\n",
      "329:Epoch: 15, Train_Loss: 1.4937150478363037, Test_Loss: 0.4269445240497589 *\n",
      "330:Epoch: 15, Train_Loss: 2.4275102615356445, Test_Loss: 0.42847421765327454\n",
      "331:Epoch: 15, Train_Loss: 0.46553558111190796, Test_Loss: 0.4349692165851593\n",
      "332:Epoch: 15, Train_Loss: 0.48715928196907043, Test_Loss: 0.48813343048095703\n",
      "333:Epoch: 15, Train_Loss: 0.46735650300979614, Test_Loss: 1.4794543981552124\n",
      "334:Epoch: 15, Train_Loss: 0.4631617069244385, Test_Loss: 4.820846080780029\n",
      "335:Epoch: 15, Train_Loss: 0.4262189269065857, Test_Loss: 0.42998403310775757 *\n",
      "336:Epoch: 15, Train_Loss: 0.44053196907043457, Test_Loss: 0.425920695066452 *\n",
      "337:Epoch: 15, Train_Loss: 0.5542317628860474, Test_Loss: 0.4705630838871002\n",
      "338:Epoch: 15, Train_Loss: 0.5263034701347351, Test_Loss: 0.47004076838493347 *\n",
      "339:Epoch: 15, Train_Loss: 0.5200179219245911, Test_Loss: 0.4761212468147278\n",
      "340:Epoch: 15, Train_Loss: 0.48460400104522705, Test_Loss: 0.44092509150505066 *\n",
      "341:Epoch: 15, Train_Loss: 0.47888582944869995, Test_Loss: 0.530400276184082\n",
      "342:Epoch: 15, Train_Loss: 0.4490014314651489, Test_Loss: 0.44196030497550964 *\n",
      "343:Epoch: 15, Train_Loss: 0.4461321532726288, Test_Loss: 0.43146374821662903 *\n",
      "344:Epoch: 15, Train_Loss: 0.44479191303253174, Test_Loss: 0.45155924558639526\n",
      "345:Epoch: 15, Train_Loss: 0.45327743887901306, Test_Loss: 0.46166110038757324\n",
      "346:Epoch: 15, Train_Loss: 0.4329827129840851, Test_Loss: 0.42887139320373535 *\n",
      "347:Epoch: 15, Train_Loss: 0.42306119203567505, Test_Loss: 0.49570232629776\n",
      "348:Epoch: 15, Train_Loss: 0.4622375965118408, Test_Loss: 0.521278977394104\n",
      "349:Epoch: 15, Train_Loss: 0.4604721665382385, Test_Loss: 0.4770863354206085 *\n",
      "350:Epoch: 15, Train_Loss: 0.4411610960960388, Test_Loss: 0.4730817675590515 *\n",
      "351:Epoch: 15, Train_Loss: 0.423972487449646, Test_Loss: 0.4515340328216553 *\n",
      "352:Epoch: 15, Train_Loss: 0.42283204197883606, Test_Loss: 0.5032157897949219\n",
      "353:Epoch: 15, Train_Loss: 0.42217665910720825, Test_Loss: 0.44170787930488586 *\n",
      "354:Epoch: 15, Train_Loss: 0.42305850982666016, Test_Loss: 0.44064265489578247 *\n",
      "355:Epoch: 15, Train_Loss: 0.4234201908111572, Test_Loss: 0.4496254622936249\n",
      "356:Epoch: 15, Train_Loss: 0.42658042907714844, Test_Loss: 0.4635674059391022\n",
      "357:Epoch: 15, Train_Loss: 0.4326980710029602, Test_Loss: 0.4511682987213135 *\n",
      "358:Epoch: 15, Train_Loss: 0.4222390949726105, Test_Loss: 0.4475897550582886 *\n",
      "359:Epoch: 15, Train_Loss: 0.4213176965713501, Test_Loss: 0.44400376081466675 *\n",
      "360:Epoch: 15, Train_Loss: 0.42515242099761963, Test_Loss: 0.4359184801578522 *\n",
      "361:Epoch: 15, Train_Loss: 0.43567368388175964, Test_Loss: 0.44779282808303833\n",
      "362:Epoch: 15, Train_Loss: 0.43995168805122375, Test_Loss: 0.4248160421848297 *\n",
      "363:Epoch: 15, Train_Loss: 0.4416416585445404, Test_Loss: 0.4398486018180847\n",
      "364:Epoch: 15, Train_Loss: 0.45200422406196594, Test_Loss: 0.5066880583763123\n",
      "365:Epoch: 15, Train_Loss: 0.4298616647720337, Test_Loss: 0.4390135109424591 *\n",
      "366:Epoch: 15, Train_Loss: 0.43378961086273193, Test_Loss: 0.8709604740142822\n",
      "367:Epoch: 15, Train_Loss: 0.4223444163799286, Test_Loss: 0.937558114528656\n",
      "368:Epoch: 15, Train_Loss: 0.4289188086986542, Test_Loss: 0.6024787425994873 *\n",
      "369:Epoch: 15, Train_Loss: 0.44886717200279236, Test_Loss: 0.44461891055107117 *\n",
      "370:Epoch: 15, Train_Loss: 0.4313899576663971, Test_Loss: 0.44691476225852966\n",
      "371:Epoch: 15, Train_Loss: 0.42481598258018494, Test_Loss: 0.4323273301124573 *\n",
      "372:Epoch: 15, Train_Loss: 0.4255087077617645, Test_Loss: 0.5286313891410828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373:Epoch: 15, Train_Loss: 0.42848750948905945, Test_Loss: 1.0453089475631714\n",
      "374:Epoch: 15, Train_Loss: 0.4595889151096344, Test_Loss: 0.9973245859146118 *\n",
      "375:Epoch: 15, Train_Loss: 0.46330225467681885, Test_Loss: 0.47416210174560547 *\n",
      "376:Epoch: 15, Train_Loss: 0.46614739298820496, Test_Loss: 0.5074490308761597\n",
      "377:Epoch: 15, Train_Loss: 0.41932034492492676, Test_Loss: 0.4227924048900604 *\n",
      "378:Epoch: 15, Train_Loss: 0.46724003553390503, Test_Loss: 0.42364299297332764\n",
      "379:Epoch: 15, Train_Loss: 0.46533966064453125, Test_Loss: 0.427327424287796\n",
      "380:Epoch: 15, Train_Loss: 0.4216553270816803, Test_Loss: 0.4309299886226654\n",
      "381:Epoch: 15, Train_Loss: 0.4299534261226654, Test_Loss: 0.46975892782211304\n",
      "382:Epoch: 15, Train_Loss: 0.44481590390205383, Test_Loss: 0.43368637561798096 *\n",
      "383:Epoch: 15, Train_Loss: 0.5053883194923401, Test_Loss: 0.4322279393672943 *\n",
      "384:Epoch: 15, Train_Loss: 0.48791399598121643, Test_Loss: 0.5543131232261658\n",
      "385:Epoch: 15, Train_Loss: 0.46881622076034546, Test_Loss: 0.8288022875785828\n",
      "386:Epoch: 15, Train_Loss: 0.44175460934638977, Test_Loss: 0.7081818580627441 *\n",
      "387:Epoch: 15, Train_Loss: 0.42641377449035645, Test_Loss: 0.47633248567581177 *\n",
      "388:Epoch: 15, Train_Loss: 0.436389684677124, Test_Loss: 0.4472205638885498 *\n",
      "389:Epoch: 15, Train_Loss: 0.41905683279037476, Test_Loss: 0.4463242292404175 *\n",
      "390:Epoch: 15, Train_Loss: 0.4232199490070343, Test_Loss: 0.44697102904319763\n",
      "391:Epoch: 15, Train_Loss: 0.4379023611545563, Test_Loss: 0.45980411767959595\n",
      "392:Epoch: 15, Train_Loss: 0.4443581700325012, Test_Loss: 0.6765304803848267\n",
      "393:Epoch: 15, Train_Loss: 0.5262960195541382, Test_Loss: 5.492414474487305\n",
      "394:Epoch: 15, Train_Loss: 0.41917893290519714, Test_Loss: 0.49003398418426514 *\n",
      "395:Epoch: 15, Train_Loss: 0.48490965366363525, Test_Loss: 0.42708566784858704 *\n",
      "396:Epoch: 15, Train_Loss: 0.42188265919685364, Test_Loss: 0.4216393232345581 *\n",
      "397:Epoch: 15, Train_Loss: 0.4568972587585449, Test_Loss: 0.42511823773384094\n",
      "398:Epoch: 15, Train_Loss: 0.4361153841018677, Test_Loss: 0.4244806170463562 *\n",
      "399:Epoch: 15, Train_Loss: 0.711200475692749, Test_Loss: 0.4207671284675598 *\n",
      "400:Epoch: 15, Train_Loss: 0.4571496248245239, Test_Loss: 0.42532092332839966\n",
      "Model saved at location ../Saver/model.ckpt at epoch 15\n",
      "401:Epoch: 15, Train_Loss: 0.4553968608379364, Test_Loss: 0.4187612235546112 *\n",
      "402:Epoch: 15, Train_Loss: 0.4190406799316406, Test_Loss: 0.4182024598121643 *\n",
      "403:Epoch: 15, Train_Loss: 0.41755104064941406, Test_Loss: 0.423158198595047\n",
      "404:Epoch: 15, Train_Loss: 0.4169720411300659, Test_Loss: 0.43712812662124634\n",
      "405:Epoch: 15, Train_Loss: 0.41629576683044434, Test_Loss: 0.4333655536174774 *\n",
      "406:Epoch: 15, Train_Loss: 0.4244370758533478, Test_Loss: 0.42871344089508057 *\n",
      "407:Epoch: 15, Train_Loss: 0.423177570104599, Test_Loss: 0.4358178377151489\n",
      "408:Epoch: 15, Train_Loss: 0.4358588457107544, Test_Loss: 0.41754940152168274 *\n",
      "409:Epoch: 15, Train_Loss: 0.42347168922424316, Test_Loss: 0.4170921742916107 *\n",
      "410:Epoch: 15, Train_Loss: 0.42573583126068115, Test_Loss: 0.4173530638217926\n",
      "411:Epoch: 15, Train_Loss: 0.42894187569618225, Test_Loss: 0.4227451682090759\n",
      "412:Epoch: 15, Train_Loss: 0.4177949130535126, Test_Loss: 0.41612720489501953 *\n",
      "413:Epoch: 15, Train_Loss: 0.41493621468544006, Test_Loss: 0.4191283881664276\n",
      "414:Epoch: 15, Train_Loss: 0.4299303889274597, Test_Loss: 0.4162900447845459 *\n",
      "415:Epoch: 15, Train_Loss: 0.4396689534187317, Test_Loss: 0.41867172718048096\n",
      "416:Epoch: 15, Train_Loss: 0.45272669196128845, Test_Loss: 0.42061734199523926\n",
      "417:Epoch: 15, Train_Loss: 0.41652125120162964, Test_Loss: 0.4181036353111267 *\n",
      "418:Epoch: 15, Train_Loss: 0.4455798268318176, Test_Loss: 0.41557320952415466 *\n",
      "419:Epoch: 15, Train_Loss: 0.46623486280441284, Test_Loss: 0.41737398505210876\n",
      "420:Epoch: 15, Train_Loss: 0.4577830731868744, Test_Loss: 0.4172153174877167 *\n",
      "421:Epoch: 15, Train_Loss: 0.41517582535743713, Test_Loss: 0.4159905016422272 *\n",
      "422:Epoch: 15, Train_Loss: 0.4469754993915558, Test_Loss: 0.4239957332611084\n",
      "423:Epoch: 15, Train_Loss: 0.4170958995819092, Test_Loss: 0.4769085943698883\n",
      "424:Epoch: 15, Train_Loss: 0.43267008662223816, Test_Loss: 2.6454663276672363\n",
      "425:Epoch: 15, Train_Loss: 0.41577795147895813, Test_Loss: 3.5883941650390625\n",
      "426:Epoch: 15, Train_Loss: 0.43960824608802795, Test_Loss: 0.4177205562591553 *\n",
      "427:Epoch: 15, Train_Loss: 0.4889819622039795, Test_Loss: 0.41475358605384827 *\n",
      "428:Epoch: 15, Train_Loss: 3.301325798034668, Test_Loss: 0.47274038195610046\n",
      "429:Epoch: 15, Train_Loss: 2.9214508533477783, Test_Loss: 0.4600997567176819 *\n",
      "430:Epoch: 15, Train_Loss: 0.42885592579841614, Test_Loss: 0.46021217107772827\n",
      "431:Epoch: 15, Train_Loss: 0.4135966897010803, Test_Loss: 0.45542940497398376 *\n",
      "432:Epoch: 15, Train_Loss: 0.512588620185852, Test_Loss: 0.5152712464332581\n",
      "433:Epoch: 15, Train_Loss: 0.5267433524131775, Test_Loss: 0.41929709911346436 *\n",
      "434:Epoch: 15, Train_Loss: 0.4335494041442871, Test_Loss: 0.4313707947731018\n",
      "435:Epoch: 15, Train_Loss: 0.413033127784729, Test_Loss: 0.4345366060733795\n",
      "436:Epoch: 15, Train_Loss: 0.45518654584884644, Test_Loss: 0.4478297829627991\n",
      "437:Epoch: 15, Train_Loss: 0.4440080225467682, Test_Loss: 0.4214635193347931 *\n",
      "438:Epoch: 15, Train_Loss: 0.4256611466407776, Test_Loss: 0.4933691620826721\n",
      "439:Epoch: 15, Train_Loss: 0.520321786403656, Test_Loss: 0.4856114387512207 *\n",
      "440:Epoch: 15, Train_Loss: 1.4393877983093262, Test_Loss: 0.4758782982826233 *\n",
      "441:Epoch: 15, Train_Loss: 1.5594985485076904, Test_Loss: 0.45675092935562134 *\n",
      "442:Epoch: 15, Train_Loss: 0.46059688925743103, Test_Loss: 0.4409289062023163 *\n",
      "443:Epoch: 15, Train_Loss: 0.47948330640792847, Test_Loss: 0.49040359258651733\n",
      "444:Epoch: 15, Train_Loss: 2.0899319648742676, Test_Loss: 0.4238969385623932 *\n",
      "445:Epoch: 15, Train_Loss: 1.7643532752990723, Test_Loss: 0.4195830523967743 *\n",
      "446:Epoch: 15, Train_Loss: 0.4533860683441162, Test_Loss: 0.42693403363227844\n",
      "447:Epoch: 15, Train_Loss: 0.42857861518859863, Test_Loss: 0.4639100730419159\n",
      "448:Epoch: 15, Train_Loss: 0.6775128841400146, Test_Loss: 0.44068583846092224 *\n",
      "449:Epoch: 15, Train_Loss: 1.3452916145324707, Test_Loss: 0.41891658306121826 *\n",
      "450:Epoch: 15, Train_Loss: 1.2088876962661743, Test_Loss: 0.41689714789390564 *\n",
      "451:Epoch: 15, Train_Loss: 0.413216233253479, Test_Loss: 0.41503068804740906 *\n",
      "452:Epoch: 15, Train_Loss: 0.4313628077507019, Test_Loss: 0.41593730449676514\n",
      "453:Epoch: 15, Train_Loss: 0.5209481716156006, Test_Loss: 0.4332010746002197\n",
      "454:Epoch: 15, Train_Loss: 0.9812705516815186, Test_Loss: 0.41289207339286804 *\n",
      "1:Epoch: 16, Train_Loss: 0.47181400656700134, Test_Loss: 0.439359188079834 *\n",
      "2:Epoch: 16, Train_Loss: 0.47298136353492737, Test_Loss: 0.43986356258392334\n",
      "3:Epoch: 16, Train_Loss: 0.4598679840564728, Test_Loss: 0.7026515007019043\n",
      "4:Epoch: 16, Train_Loss: 0.45238202810287476, Test_Loss: 0.6962001323699951 *\n",
      "5:Epoch: 16, Train_Loss: 0.5455501675605774, Test_Loss: 0.5148565769195557 *\n",
      "6:Epoch: 16, Train_Loss: 0.5161654949188232, Test_Loss: 0.42242032289505005 *\n",
      "7:Epoch: 16, Train_Loss: 0.48194071650505066, Test_Loss: 0.4407300353050232\n",
      "8:Epoch: 16, Train_Loss: 0.4606947600841522, Test_Loss: 0.4278675317764282 *\n",
      "9:Epoch: 16, Train_Loss: 0.5253790616989136, Test_Loss: 0.5112853646278381\n",
      "10:Epoch: 16, Train_Loss: 0.4768716096878052, Test_Loss: 0.6318165063858032\n",
      "11:Epoch: 16, Train_Loss: 0.5651137232780457, Test_Loss: 0.7147811651229858\n",
      "12:Epoch: 16, Train_Loss: 0.5840033292770386, Test_Loss: 0.49990126490592957 *\n",
      "13:Epoch: 16, Train_Loss: 0.44197070598602295, Test_Loss: 0.4568824768066406 *\n",
      "14:Epoch: 16, Train_Loss: 0.5487184524536133, Test_Loss: 0.43798497319221497 *\n",
      "15:Epoch: 16, Train_Loss: 0.5172420144081116, Test_Loss: 0.4423702359199524\n",
      "16:Epoch: 16, Train_Loss: 0.4518205523490906, Test_Loss: 0.44932815432548523\n",
      "17:Epoch: 16, Train_Loss: 0.4188772141933441, Test_Loss: 0.4902020990848541\n",
      "18:Epoch: 16, Train_Loss: 0.412768691778183, Test_Loss: 0.4559950828552246 *\n",
      "19:Epoch: 16, Train_Loss: 0.4108513295650482, Test_Loss: 0.5340768098831177\n",
      "20:Epoch: 16, Train_Loss: 0.4114443063735962, Test_Loss: 0.518634021282196 *\n",
      "21:Epoch: 16, Train_Loss: 0.4180488884449005, Test_Loss: 0.6170406341552734\n",
      "22:Epoch: 16, Train_Loss: 0.43867361545562744, Test_Loss: 0.7559832334518433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:Epoch: 16, Train_Loss: 0.435951828956604, Test_Loss: 1.2630740404129028\n",
      "24:Epoch: 16, Train_Loss: 0.4757915139198303, Test_Loss: 0.7136693000793457 *\n",
      "25:Epoch: 16, Train_Loss: 0.529315173625946, Test_Loss: 0.6714149713516235 *\n",
      "26:Epoch: 16, Train_Loss: 0.7033013701438904, Test_Loss: 0.6670302152633667 *\n",
      "27:Epoch: 16, Train_Loss: 0.43177396059036255, Test_Loss: 0.6689542531967163\n",
      "28:Epoch: 16, Train_Loss: 0.44836950302124023, Test_Loss: 0.8049386739730835\n",
      "29:Epoch: 16, Train_Loss: 0.5767128467559814, Test_Loss: 1.250077486038208\n",
      "30:Epoch: 16, Train_Loss: 0.5207617282867432, Test_Loss: 4.770654201507568\n",
      "31:Epoch: 16, Train_Loss: 0.5492242574691772, Test_Loss: 0.4488590955734253 *\n",
      "32:Epoch: 16, Train_Loss: 0.46320033073425293, Test_Loss: 0.44004857540130615 *\n",
      "33:Epoch: 16, Train_Loss: 0.7235723733901978, Test_Loss: 0.44684290885925293\n",
      "34:Epoch: 16, Train_Loss: 0.6562427282333374, Test_Loss: 0.418773353099823 *\n",
      "35:Epoch: 16, Train_Loss: 0.5454078316688538, Test_Loss: 0.44570475816726685\n",
      "36:Epoch: 16, Train_Loss: 0.43282121419906616, Test_Loss: 0.5015338063240051\n",
      "37:Epoch: 16, Train_Loss: 0.450186550617218, Test_Loss: 0.483155220746994 *\n",
      "38:Epoch: 16, Train_Loss: 0.5569185614585876, Test_Loss: 0.4240289032459259 *\n",
      "39:Epoch: 16, Train_Loss: 1.0714980363845825, Test_Loss: 0.44603267312049866\n",
      "40:Epoch: 16, Train_Loss: 0.8108957409858704, Test_Loss: 0.47573769092559814\n",
      "41:Epoch: 16, Train_Loss: 0.43524935841560364, Test_Loss: 0.5531119704246521\n",
      "42:Epoch: 16, Train_Loss: 0.4513774514198303, Test_Loss: 0.4412820339202881 *\n",
      "43:Epoch: 16, Train_Loss: 0.41338419914245605, Test_Loss: 0.4987788796424866\n",
      "44:Epoch: 16, Train_Loss: 0.6359254121780396, Test_Loss: 0.5291582345962524\n",
      "45:Epoch: 16, Train_Loss: 0.6453597545623779, Test_Loss: 0.4354342818260193 *\n",
      "46:Epoch: 16, Train_Loss: 0.4131813645362854, Test_Loss: 0.41888943314552307 *\n",
      "47:Epoch: 16, Train_Loss: 0.640451967716217, Test_Loss: 0.5006555318832397\n",
      "48:Epoch: 16, Train_Loss: 0.4321189820766449, Test_Loss: 0.5145511031150818\n",
      "49:Epoch: 16, Train_Loss: 0.4276571273803711, Test_Loss: 0.42787235975265503 *\n",
      "50:Epoch: 16, Train_Loss: 0.46672624349594116, Test_Loss: 0.45978885889053345\n",
      "51:Epoch: 16, Train_Loss: 0.5560299158096313, Test_Loss: 0.4338814318180084 *\n",
      "52:Epoch: 16, Train_Loss: 0.47542014718055725, Test_Loss: 0.5032815933227539\n",
      "53:Epoch: 16, Train_Loss: 0.6308841109275818, Test_Loss: 0.5186207294464111\n",
      "54:Epoch: 16, Train_Loss: 0.428393691778183, Test_Loss: 0.47832441329956055 *\n",
      "55:Epoch: 16, Train_Loss: 0.5792632102966309, Test_Loss: 0.42400291562080383 *\n",
      "56:Epoch: 16, Train_Loss: 0.4523187279701233, Test_Loss: 0.46124979853630066\n",
      "57:Epoch: 16, Train_Loss: 0.4563482999801636, Test_Loss: 0.4517190456390381 *\n",
      "58:Epoch: 16, Train_Loss: 0.42147624492645264, Test_Loss: 0.42471736669540405 *\n",
      "59:Epoch: 16, Train_Loss: 0.46778738498687744, Test_Loss: 0.5060054063796997\n",
      "60:Epoch: 16, Train_Loss: 0.5092377066612244, Test_Loss: 0.5143234133720398\n",
      "61:Epoch: 16, Train_Loss: 0.6539620757102966, Test_Loss: 3.89094877243042\n",
      "62:Epoch: 16, Train_Loss: 0.6007676124572754, Test_Loss: 2.4102563858032227 *\n",
      "63:Epoch: 16, Train_Loss: 0.8404699563980103, Test_Loss: 0.4590170383453369 *\n",
      "64:Epoch: 16, Train_Loss: 0.7109659910202026, Test_Loss: 0.45292869210243225 *\n",
      "65:Epoch: 16, Train_Loss: 0.6259074211120605, Test_Loss: 0.4599665403366089\n",
      "66:Epoch: 16, Train_Loss: 0.5126447677612305, Test_Loss: 0.40809133648872375 *\n",
      "67:Epoch: 16, Train_Loss: 0.43069714307785034, Test_Loss: 0.421927273273468\n",
      "68:Epoch: 16, Train_Loss: 0.4273848831653595, Test_Loss: 0.476199746131897\n",
      "69:Epoch: 16, Train_Loss: 0.41377127170562744, Test_Loss: 0.4605712294578552 *\n",
      "70:Epoch: 16, Train_Loss: 0.5005201101303101, Test_Loss: 0.41678398847579956 *\n",
      "71:Epoch: 16, Train_Loss: 0.7962301969528198, Test_Loss: 0.43856802582740784\n",
      "72:Epoch: 16, Train_Loss: 0.7301342487335205, Test_Loss: 0.4551575481891632\n",
      "73:Epoch: 16, Train_Loss: 1.6759803295135498, Test_Loss: 0.5182161927223206\n",
      "74:Epoch: 16, Train_Loss: 1.2208681106567383, Test_Loss: 0.44299739599227905 *\n",
      "75:Epoch: 16, Train_Loss: 0.7373384237289429, Test_Loss: 0.43142789602279663 *\n",
      "76:Epoch: 16, Train_Loss: 0.5828987956047058, Test_Loss: 0.4553481638431549\n",
      "77:Epoch: 16, Train_Loss: 0.4130301773548126, Test_Loss: 0.45648515224456787\n",
      "78:Epoch: 16, Train_Loss: 0.5022989511489868, Test_Loss: 0.415388286113739 *\n",
      "79:Epoch: 16, Train_Loss: 0.9694087505340576, Test_Loss: 0.5119891166687012\n",
      "80:Epoch: 16, Train_Loss: 1.1778520345687866, Test_Loss: 0.5004613995552063 *\n",
      "81:Epoch: 16, Train_Loss: 0.43361780047416687, Test_Loss: 0.4356100559234619 *\n",
      "82:Epoch: 16, Train_Loss: 0.452729195356369, Test_Loss: 0.44161298871040344\n",
      "83:Epoch: 16, Train_Loss: 0.47556716203689575, Test_Loss: 0.4423729479312897\n",
      "84:Epoch: 16, Train_Loss: 0.6512171626091003, Test_Loss: 0.441716730594635 *\n",
      "85:Epoch: 16, Train_Loss: 0.539703369140625, Test_Loss: 0.4403294324874878 *\n",
      "86:Epoch: 16, Train_Loss: 0.6420698165893555, Test_Loss: 0.4449208080768585\n",
      "87:Epoch: 16, Train_Loss: 0.5733632445335388, Test_Loss: 0.4298675060272217 *\n",
      "88:Epoch: 16, Train_Loss: 0.5645416975021362, Test_Loss: 0.44882869720458984\n",
      "89:Epoch: 16, Train_Loss: 0.4358963966369629, Test_Loss: 0.441466361284256 *\n",
      "90:Epoch: 16, Train_Loss: 0.43004658818244934, Test_Loss: 0.47897955775260925\n",
      "91:Epoch: 16, Train_Loss: 0.41776493191719055, Test_Loss: 0.4258507490158081 *\n",
      "92:Epoch: 16, Train_Loss: 0.45934271812438965, Test_Loss: 0.44385379552841187\n",
      "93:Epoch: 16, Train_Loss: 0.4271731674671173, Test_Loss: 0.5002055168151855\n",
      "94:Epoch: 16, Train_Loss: 0.48337194323539734, Test_Loss: 0.4807713031768799 *\n",
      "95:Epoch: 16, Train_Loss: 15.54727554321289, Test_Loss: 0.5505587458610535\n",
      "96:Epoch: 16, Train_Loss: 0.4797518253326416, Test_Loss: 0.44662556052207947 *\n",
      "97:Epoch: 16, Train_Loss: 1.635507345199585, Test_Loss: 0.523317277431488\n",
      "98:Epoch: 16, Train_Loss: 1.3667713403701782, Test_Loss: 0.4732722043991089 *\n",
      "99:Epoch: 16, Train_Loss: 0.4193388521671295, Test_Loss: 0.4716784656047821 *\n",
      "100:Epoch: 16, Train_Loss: 0.5537662506103516, Test_Loss: 0.5994478464126587\n",
      "Model saved at location ../Saver/model.ckpt at epoch 16\n",
      "101:Epoch: 16, Train_Loss: 3.1142725944519043, Test_Loss: 0.5735350251197815 *\n",
      "102:Epoch: 16, Train_Loss: 4.043734073638916, Test_Loss: 0.8159835934638977\n",
      "103:Epoch: 16, Train_Loss: 0.4422052204608917, Test_Loss: 0.897598147392273\n",
      "104:Epoch: 16, Train_Loss: 0.707883358001709, Test_Loss: 0.8088960647583008 *\n",
      "105:Epoch: 16, Train_Loss: 3.8941752910614014, Test_Loss: 1.4040894508361816\n",
      "106:Epoch: 16, Train_Loss: 1.0834075212478638, Test_Loss: 1.8728080987930298\n",
      "107:Epoch: 16, Train_Loss: 0.5833768248558044, Test_Loss: 2.13219952583313\n",
      "108:Epoch: 16, Train_Loss: 0.4166499972343445, Test_Loss: 1.0409678220748901 *\n",
      "109:Epoch: 16, Train_Loss: 0.5567165613174438, Test_Loss: 2.5307083129882812\n",
      "110:Epoch: 16, Train_Loss: 0.5442798137664795, Test_Loss: 1.6108020544052124 *\n",
      "111:Epoch: 16, Train_Loss: 0.406183123588562, Test_Loss: 1.9346632957458496\n",
      "112:Epoch: 16, Train_Loss: 0.4105437099933624, Test_Loss: 3.7646615505218506\n",
      "113:Epoch: 16, Train_Loss: 0.39956483244895935, Test_Loss: 1.725085973739624 *\n",
      "114:Epoch: 16, Train_Loss: 0.4048648476600647, Test_Loss: 0.49655359983444214 *\n",
      "115:Epoch: 16, Train_Loss: 0.452224999666214, Test_Loss: 0.5684475898742676\n",
      "116:Epoch: 16, Train_Loss: 0.4356090724468231, Test_Loss: 0.4744918644428253 *\n",
      "117:Epoch: 16, Train_Loss: 0.5588407516479492, Test_Loss: 0.43247783184051514 *\n",
      "118:Epoch: 16, Train_Loss: 0.451854407787323, Test_Loss: 0.4047478437423706 *\n",
      "119:Epoch: 16, Train_Loss: 0.48353633284568787, Test_Loss: 0.5427998900413513\n",
      "120:Epoch: 16, Train_Loss: 0.4150930643081665, Test_Loss: 4.074558734893799\n",
      "121:Epoch: 16, Train_Loss: 0.4407493770122528, Test_Loss: 4.895183086395264\n",
      "122:Epoch: 16, Train_Loss: 0.4277597665786743, Test_Loss: 1.0593726634979248 *\n",
      "123:Epoch: 16, Train_Loss: 0.4203249216079712, Test_Loss: 1.0068964958190918 *\n",
      "124:Epoch: 16, Train_Loss: 0.40543505549430847, Test_Loss: 1.1704647541046143\n",
      "125:Epoch: 16, Train_Loss: 0.40160897374153137, Test_Loss: 0.5955530405044556 *\n",
      "126:Epoch: 16, Train_Loss: 0.40057989954948425, Test_Loss: 1.060555100440979\n",
      "127:Epoch: 16, Train_Loss: 0.4024839997291565, Test_Loss: 1.362152338027954\n",
      "128:Epoch: 16, Train_Loss: 0.40304306149482727, Test_Loss: 0.8036847114562988 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129:Epoch: 16, Train_Loss: 0.399919718503952, Test_Loss: 0.628810703754425 *\n",
      "130:Epoch: 16, Train_Loss: 0.4004148840904236, Test_Loss: 0.7766802906990051\n",
      "131:Epoch: 16, Train_Loss: 0.42346492409706116, Test_Loss: 0.6163742542266846 *\n",
      "132:Epoch: 16, Train_Loss: 0.4337366223335266, Test_Loss: 0.9561986923217773\n",
      "133:Epoch: 16, Train_Loss: 0.5117242336273193, Test_Loss: 0.7305692434310913 *\n",
      "134:Epoch: 16, Train_Loss: 0.5012730360031128, Test_Loss: 1.148457646369934\n",
      "135:Epoch: 16, Train_Loss: 0.6026016473770142, Test_Loss: 0.8953880667686462 *\n",
      "136:Epoch: 16, Train_Loss: 2.8728489875793457, Test_Loss: 0.4030081629753113 *\n",
      "137:Epoch: 16, Train_Loss: 5.747308254241943, Test_Loss: 0.42446866631507874\n",
      "138:Epoch: 16, Train_Loss: 0.43047070503234863, Test_Loss: 0.45576316118240356\n",
      "139:Epoch: 16, Train_Loss: 0.4912489652633667, Test_Loss: 0.6332679986953735\n",
      "140:Epoch: 16, Train_Loss: 0.5169345140457153, Test_Loss: 0.47058260440826416 *\n",
      "141:Epoch: 16, Train_Loss: 0.5206788778305054, Test_Loss: 0.5614477396011353\n",
      "142:Epoch: 16, Train_Loss: 0.4642799496650696, Test_Loss: 0.5160517692565918 *\n",
      "143:Epoch: 16, Train_Loss: 0.538306474685669, Test_Loss: 0.6371239423751831\n",
      "144:Epoch: 16, Train_Loss: 0.5160142779350281, Test_Loss: 0.6121002435684204 *\n",
      "145:Epoch: 16, Train_Loss: 0.5615510940551758, Test_Loss: 0.5056287050247192 *\n",
      "146:Epoch: 16, Train_Loss: 0.4345463216304779, Test_Loss: 0.42494896054267883 *\n",
      "147:Epoch: 16, Train_Loss: 0.41910386085510254, Test_Loss: 0.47577425837516785\n",
      "148:Epoch: 16, Train_Loss: 0.4166330397129059, Test_Loss: 0.4486486315727234 *\n",
      "149:Epoch: 16, Train_Loss: 0.42250752449035645, Test_Loss: 0.43273550271987915 *\n",
      "150:Epoch: 16, Train_Loss: 0.5098130106925964, Test_Loss: 0.5450016856193542\n",
      "151:Epoch: 16, Train_Loss: 0.4790143072605133, Test_Loss: 0.45015475153923035 *\n",
      "152:Epoch: 16, Train_Loss: 0.4119373857975006, Test_Loss: 5.155110836029053\n",
      "153:Epoch: 16, Train_Loss: 0.41471534967422485, Test_Loss: 1.3884916305541992 *\n",
      "154:Epoch: 16, Train_Loss: 0.40380191802978516, Test_Loss: 0.4279743432998657 *\n",
      "155:Epoch: 16, Train_Loss: 0.436043918132782, Test_Loss: 0.43187910318374634\n",
      "156:Epoch: 16, Train_Loss: 0.46946799755096436, Test_Loss: 0.4118748903274536 *\n",
      "157:Epoch: 16, Train_Loss: 0.4297642707824707, Test_Loss: 0.41249990463256836\n",
      "158:Epoch: 16, Train_Loss: 0.414359986782074, Test_Loss: 0.40648436546325684 *\n",
      "159:Epoch: 16, Train_Loss: 0.41911932826042175, Test_Loss: 0.47038280963897705\n",
      "160:Epoch: 16, Train_Loss: 0.410762220621109, Test_Loss: 0.4486635625362396 *\n",
      "161:Epoch: 16, Train_Loss: 2.854619026184082, Test_Loss: 0.4009353816509247 *\n",
      "162:Epoch: 16, Train_Loss: 3.636636257171631, Test_Loss: 0.41982316970825195\n",
      "163:Epoch: 16, Train_Loss: 0.3994176387786865, Test_Loss: 0.4267783463001251\n",
      "164:Epoch: 16, Train_Loss: 0.4033205509185791, Test_Loss: 0.43044421076774597\n",
      "165:Epoch: 16, Train_Loss: 0.40901389718055725, Test_Loss: 0.40833383798599243 *\n",
      "166:Epoch: 16, Train_Loss: 0.3993932902812958, Test_Loss: 0.4298311471939087\n",
      "167:Epoch: 16, Train_Loss: 0.4030032753944397, Test_Loss: 0.43462732434272766\n",
      "168:Epoch: 16, Train_Loss: 0.39797094464302063, Test_Loss: 0.469723641872406\n",
      "169:Epoch: 16, Train_Loss: 0.4142613708972931, Test_Loss: 0.42619091272354126 *\n",
      "170:Epoch: 16, Train_Loss: 0.41406500339508057, Test_Loss: 0.45240193605422974\n",
      "171:Epoch: 16, Train_Loss: 0.42653888463974, Test_Loss: 0.4223538339138031 *\n",
      "172:Epoch: 16, Train_Loss: 0.39858874678611755, Test_Loss: 0.40217384696006775 *\n",
      "173:Epoch: 16, Train_Loss: 0.39502331614494324, Test_Loss: 0.40505698323249817\n",
      "174:Epoch: 16, Train_Loss: 0.39945173263549805, Test_Loss: 0.40684056282043457\n",
      "175:Epoch: 16, Train_Loss: 0.4102238118648529, Test_Loss: 0.4045322835445404 *\n",
      "176:Epoch: 16, Train_Loss: 0.40234702825546265, Test_Loss: 0.4028680920600891 *\n",
      "177:Epoch: 16, Train_Loss: 0.39877742528915405, Test_Loss: 0.4019968509674072 *\n",
      "178:Epoch: 16, Train_Loss: 0.4213007092475891, Test_Loss: 0.397298127412796 *\n",
      "179:Epoch: 16, Train_Loss: 0.41533610224723816, Test_Loss: 0.397050678730011 *\n",
      "180:Epoch: 16, Train_Loss: 0.39589038491249084, Test_Loss: 0.39954400062561035\n",
      "181:Epoch: 16, Train_Loss: 0.39421388506889343, Test_Loss: 0.40924757719039917\n",
      "182:Epoch: 16, Train_Loss: 0.4395405054092407, Test_Loss: 0.4057788550853729 *\n",
      "183:Epoch: 16, Train_Loss: 0.4747579097747803, Test_Loss: 0.4159001410007477\n",
      "184:Epoch: 16, Train_Loss: 0.4214787781238556, Test_Loss: 0.5420743227005005\n",
      "185:Epoch: 16, Train_Loss: 0.43789348006248474, Test_Loss: 0.6921671628952026\n",
      "186:Epoch: 16, Train_Loss: 0.44130611419677734, Test_Loss: 0.631607174873352 *\n",
      "187:Epoch: 16, Train_Loss: 0.4716010093688965, Test_Loss: 0.457829087972641 *\n",
      "188:Epoch: 16, Train_Loss: 0.4208740293979645, Test_Loss: 0.3990468978881836 *\n",
      "189:Epoch: 16, Train_Loss: 0.45465391874313354, Test_Loss: 0.4114103317260742\n",
      "190:Epoch: 16, Train_Loss: 0.563814640045166, Test_Loss: 0.47316986322402954\n",
      "191:Epoch: 16, Train_Loss: 0.4944089651107788, Test_Loss: 0.7914280891418457\n",
      "192:Epoch: 16, Train_Loss: 0.4206085205078125, Test_Loss: 1.0112922191619873\n",
      "193:Epoch: 16, Train_Loss: 0.39570263028144836, Test_Loss: 0.676417350769043 *\n",
      "194:Epoch: 16, Train_Loss: 0.3948977589607239, Test_Loss: 0.464138925075531 *\n",
      "195:Epoch: 16, Train_Loss: 0.3971026837825775, Test_Loss: 0.4030381143093109 *\n",
      "196:Epoch: 16, Train_Loss: 0.39794743061065674, Test_Loss: 0.41927266120910645\n",
      "197:Epoch: 16, Train_Loss: 0.3942495584487915, Test_Loss: 0.41479915380477905 *\n",
      "198:Epoch: 16, Train_Loss: 3.209486484527588, Test_Loss: 0.41918593645095825\n",
      "199:Epoch: 16, Train_Loss: 2.4626121520996094, Test_Loss: 0.41634947061538696 *\n",
      "200:Epoch: 16, Train_Loss: 0.3943477272987366, Test_Loss: 0.4317057430744171\n",
      "Model saved at location ../Saver/model.ckpt at epoch 16\n",
      "201:Epoch: 16, Train_Loss: 0.3954015374183655, Test_Loss: 0.39821216464042664 *\n",
      "202:Epoch: 16, Train_Loss: 0.3948360085487366, Test_Loss: 0.5152814984321594\n",
      "203:Epoch: 16, Train_Loss: 0.39730140566825867, Test_Loss: 0.719352126121521\n",
      "204:Epoch: 16, Train_Loss: 0.3918544352054596, Test_Loss: 0.5281464457511902 *\n",
      "205:Epoch: 16, Train_Loss: 0.3921065628528595, Test_Loss: 0.6520166993141174\n",
      "206:Epoch: 16, Train_Loss: 0.39057162404060364, Test_Loss: 0.40081822872161865 *\n",
      "207:Epoch: 16, Train_Loss: 0.39097604155540466, Test_Loss: 0.3986009359359741 *\n",
      "208:Epoch: 16, Train_Loss: 0.40538597106933594, Test_Loss: 0.3987451493740082\n",
      "209:Epoch: 16, Train_Loss: 0.42339569330215454, Test_Loss: 0.39994126558303833\n",
      "210:Epoch: 16, Train_Loss: 0.4266628623008728, Test_Loss: 0.45021167397499084\n",
      "211:Epoch: 16, Train_Loss: 0.46984249353408813, Test_Loss: 3.958726406097412\n",
      "212:Epoch: 16, Train_Loss: 0.41397738456726074, Test_Loss: 2.1367344856262207 *\n",
      "213:Epoch: 16, Train_Loss: 0.40158358216285706, Test_Loss: 0.3999655246734619 *\n",
      "214:Epoch: 16, Train_Loss: 0.5962994694709778, Test_Loss: 0.3944334089756012 *\n",
      "215:Epoch: 16, Train_Loss: 0.5964993238449097, Test_Loss: 0.3927646279335022 *\n",
      "216:Epoch: 16, Train_Loss: 0.5941969752311707, Test_Loss: 0.4029826819896698\n",
      "217:Epoch: 16, Train_Loss: 0.4651869535446167, Test_Loss: 0.39461010694503784 *\n",
      "218:Epoch: 16, Train_Loss: 0.3899552524089813, Test_Loss: 0.4058615267276764\n",
      "219:Epoch: 16, Train_Loss: 0.38969045877456665, Test_Loss: 0.3934248685836792 *\n",
      "220:Epoch: 16, Train_Loss: 0.3935546875, Test_Loss: 0.3944932222366333\n",
      "221:Epoch: 16, Train_Loss: 0.39792802929878235, Test_Loss: 0.39827200770378113\n",
      "222:Epoch: 16, Train_Loss: 0.4016346335411072, Test_Loss: 0.4019485414028168\n",
      "223:Epoch: 16, Train_Loss: 0.398777574300766, Test_Loss: 0.40644288063049316\n",
      "224:Epoch: 16, Train_Loss: 0.38944900035858154, Test_Loss: 0.40275412797927856 *\n",
      "225:Epoch: 16, Train_Loss: 0.38944071531295776, Test_Loss: 0.39981529116630554 *\n",
      "226:Epoch: 16, Train_Loss: 0.39875224232673645, Test_Loss: 0.398766428232193 *\n",
      "227:Epoch: 16, Train_Loss: 0.46045243740081787, Test_Loss: 0.39138713479042053 *\n",
      "228:Epoch: 16, Train_Loss: 0.571587085723877, Test_Loss: 0.3916493356227875\n",
      "229:Epoch: 16, Train_Loss: 0.5290715098381042, Test_Loss: 0.39407065510749817\n",
      "230:Epoch: 16, Train_Loss: 0.48362982273101807, Test_Loss: 0.3950495421886444\n",
      "231:Epoch: 16, Train_Loss: 0.4968746602535248, Test_Loss: 0.3943656384944916 *\n",
      "232:Epoch: 16, Train_Loss: 0.5599974393844604, Test_Loss: 0.39355868101119995 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233:Epoch: 16, Train_Loss: 0.4321887791156769, Test_Loss: 0.3985728919506073\n",
      "234:Epoch: 16, Train_Loss: 0.5650100708007812, Test_Loss: 0.4038752615451813\n",
      "235:Epoch: 16, Train_Loss: 0.5025298595428467, Test_Loss: 0.4015616774559021 *\n",
      "236:Epoch: 16, Train_Loss: 0.6789875030517578, Test_Loss: 0.39911746978759766 *\n",
      "237:Epoch: 16, Train_Loss: 0.4010550081729889, Test_Loss: 0.39333540201187134 *\n",
      "238:Epoch: 16, Train_Loss: 0.6551352143287659, Test_Loss: 0.3928731679916382 *\n",
      "239:Epoch: 16, Train_Loss: 3.2025492191314697, Test_Loss: 0.3932107090950012\n",
      "240:Epoch: 16, Train_Loss: 0.4680264890193939, Test_Loss: 0.3941296637058258\n",
      "241:Epoch: 16, Train_Loss: 0.4565034806728363, Test_Loss: 0.4435889422893524\n",
      "242:Epoch: 16, Train_Loss: 0.4262324869632721, Test_Loss: 0.4253941476345062 *\n",
      "243:Epoch: 16, Train_Loss: 0.4152829945087433, Test_Loss: 5.875187873840332\n",
      "244:Epoch: 16, Train_Loss: 0.39171117544174194, Test_Loss: 0.5162871479988098 *\n",
      "245:Epoch: 16, Train_Loss: 0.40238305926322937, Test_Loss: 0.39739561080932617 *\n",
      "246:Epoch: 16, Train_Loss: 0.486415833234787, Test_Loss: 0.41682741045951843\n",
      "247:Epoch: 16, Train_Loss: 0.5230122804641724, Test_Loss: 0.44154849648475647\n",
      "248:Epoch: 16, Train_Loss: 0.4873977601528168, Test_Loss: 0.4385818839073181 *\n",
      "249:Epoch: 16, Train_Loss: 0.450184166431427, Test_Loss: 0.392538845539093 *\n",
      "250:Epoch: 16, Train_Loss: 0.4451119005680084, Test_Loss: 0.47384947538375854\n",
      "251:Epoch: 16, Train_Loss: 0.41088616847991943, Test_Loss: 0.43331700563430786 *\n",
      "252:Epoch: 16, Train_Loss: 0.4168534278869629, Test_Loss: 0.39111897349357605 *\n",
      "253:Epoch: 16, Train_Loss: 0.3968472480773926, Test_Loss: 0.40837404131889343\n",
      "254:Epoch: 16, Train_Loss: 0.42352059483528137, Test_Loss: 0.4222264289855957\n",
      "255:Epoch: 16, Train_Loss: 0.4007919728755951, Test_Loss: 0.3964237868785858 *\n",
      "256:Epoch: 16, Train_Loss: 0.38646095991134644, Test_Loss: 0.4211239814758301\n",
      "257:Epoch: 16, Train_Loss: 0.41764920949935913, Test_Loss: 0.5061627626419067\n",
      "258:Epoch: 16, Train_Loss: 0.4336087703704834, Test_Loss: 0.42315757274627686 *\n",
      "259:Epoch: 16, Train_Loss: 0.4145341217517853, Test_Loss: 0.44235464930534363\n",
      "260:Epoch: 16, Train_Loss: 0.38710832595825195, Test_Loss: 0.42092105746269226 *\n",
      "261:Epoch: 16, Train_Loss: 0.3859744071960449, Test_Loss: 0.4687598943710327\n",
      "262:Epoch: 16, Train_Loss: 0.38548150658607483, Test_Loss: 0.41208112239837646 *\n",
      "263:Epoch: 16, Train_Loss: 0.38530927896499634, Test_Loss: 0.4005112648010254 *\n",
      "264:Epoch: 16, Train_Loss: 0.3867665231227875, Test_Loss: 0.40514153242111206\n",
      "265:Epoch: 16, Train_Loss: 0.387655645608902, Test_Loss: 0.41481858491897583\n",
      "266:Epoch: 16, Train_Loss: 0.3930812478065491, Test_Loss: 0.407418817281723 *\n",
      "267:Epoch: 16, Train_Loss: 0.38762807846069336, Test_Loss: 0.40875354409217834\n",
      "268:Epoch: 16, Train_Loss: 0.38484421372413635, Test_Loss: 0.3966262936592102 *\n",
      "269:Epoch: 16, Train_Loss: 0.3858928978443146, Test_Loss: 0.40411266684532166\n",
      "270:Epoch: 16, Train_Loss: 0.392570823431015, Test_Loss: 0.4057823121547699\n",
      "271:Epoch: 16, Train_Loss: 0.40236398577690125, Test_Loss: 0.3928185701370239 *\n",
      "272:Epoch: 16, Train_Loss: 0.4112541675567627, Test_Loss: 0.3973489999771118\n",
      "273:Epoch: 16, Train_Loss: 0.41291844844818115, Test_Loss: 0.4470042288303375\n",
      "274:Epoch: 16, Train_Loss: 0.3919374942779541, Test_Loss: 0.41677001118659973 *\n",
      "275:Epoch: 16, Train_Loss: 0.3985324800014496, Test_Loss: 0.6901552677154541\n",
      "276:Epoch: 16, Train_Loss: 0.38570690155029297, Test_Loss: 0.8831971883773804\n",
      "277:Epoch: 16, Train_Loss: 0.3872164487838745, Test_Loss: 0.6381772756576538 *\n",
      "278:Epoch: 16, Train_Loss: 0.4038110375404358, Test_Loss: 0.4559234380722046 *\n",
      "279:Epoch: 16, Train_Loss: 0.40138471126556396, Test_Loss: 0.40938183665275574 *\n",
      "280:Epoch: 16, Train_Loss: 0.38561689853668213, Test_Loss: 0.39009571075439453 *\n",
      "281:Epoch: 16, Train_Loss: 0.3890601098537445, Test_Loss: 0.4671097695827484\n",
      "282:Epoch: 16, Train_Loss: 0.3909526765346527, Test_Loss: 0.8572953939437866\n",
      "283:Epoch: 16, Train_Loss: 0.41382336616516113, Test_Loss: 0.9726746082305908\n",
      "284:Epoch: 16, Train_Loss: 0.4198233187198639, Test_Loss: 0.5192443132400513 *\n",
      "285:Epoch: 16, Train_Loss: 0.43430769443511963, Test_Loss: 0.48937538266181946 *\n",
      "286:Epoch: 16, Train_Loss: 0.38825729489326477, Test_Loss: 0.3857998251914978 *\n",
      "287:Epoch: 16, Train_Loss: 0.39876416325569153, Test_Loss: 0.3900962173938751\n",
      "288:Epoch: 16, Train_Loss: 0.4526103138923645, Test_Loss: 0.3882538378238678 *\n",
      "289:Epoch: 16, Train_Loss: 0.38531002402305603, Test_Loss: 0.401140958070755\n",
      "290:Epoch: 16, Train_Loss: 0.3918806314468384, Test_Loss: 0.40808770060539246\n",
      "291:Epoch: 16, Train_Loss: 0.40501007437705994, Test_Loss: 0.41027796268463135\n",
      "292:Epoch: 16, Train_Loss: 0.44273266196250916, Test_Loss: 0.38743942975997925 *\n",
      "293:Epoch: 16, Train_Loss: 0.47106075286865234, Test_Loss: 0.5235741138458252\n",
      "294:Epoch: 16, Train_Loss: 0.44204476475715637, Test_Loss: 0.782757043838501\n",
      "295:Epoch: 16, Train_Loss: 0.4111562967300415, Test_Loss: 0.4969942569732666 *\n",
      "296:Epoch: 16, Train_Loss: 0.38690370321273804, Test_Loss: 0.6016600131988525\n",
      "297:Epoch: 16, Train_Loss: 0.4055596590042114, Test_Loss: 0.3958987593650818 *\n",
      "298:Epoch: 16, Train_Loss: 0.3836074769496918, Test_Loss: 0.39571237564086914 *\n",
      "299:Epoch: 16, Train_Loss: 0.38727280497550964, Test_Loss: 0.3958497941493988\n",
      "300:Epoch: 16, Train_Loss: 0.39569389820098877, Test_Loss: 0.40071555972099304\n",
      "Model saved at location ../Saver/model.ckpt at epoch 16\n",
      "301:Epoch: 16, Train_Loss: 0.4085964858531952, Test_Loss: 0.44417324662208557\n",
      "302:Epoch: 16, Train_Loss: 0.4616609215736389, Test_Loss: 5.1687397956848145\n",
      "303:Epoch: 16, Train_Loss: 0.4242863655090332, Test_Loss: 0.9898854494094849 *\n",
      "304:Epoch: 16, Train_Loss: 0.4372904896736145, Test_Loss: 0.3935416638851166 *\n",
      "305:Epoch: 16, Train_Loss: 0.3984781801700592, Test_Loss: 0.3847843408584595 *\n",
      "306:Epoch: 16, Train_Loss: 0.4152287244796753, Test_Loss: 0.38630905747413635\n",
      "307:Epoch: 16, Train_Loss: 0.3948316276073456, Test_Loss: 0.3935837745666504\n",
      "308:Epoch: 16, Train_Loss: 0.5840428471565247, Test_Loss: 0.3851626515388489 *\n",
      "309:Epoch: 16, Train_Loss: 0.5220411419868469, Test_Loss: 0.39465081691741943\n",
      "310:Epoch: 16, Train_Loss: 0.39083927869796753, Test_Loss: 0.3838220536708832 *\n",
      "311:Epoch: 16, Train_Loss: 0.41422298550605774, Test_Loss: 0.38386476039886475\n",
      "312:Epoch: 16, Train_Loss: 0.3823525011539459, Test_Loss: 0.38905128836631775\n",
      "313:Epoch: 16, Train_Loss: 0.381087988615036, Test_Loss: 0.400236576795578\n",
      "314:Epoch: 16, Train_Loss: 0.3811659812927246, Test_Loss: 0.3865799605846405 *\n",
      "315:Epoch: 16, Train_Loss: 0.38423100113868713, Test_Loss: 0.39840465784072876\n",
      "316:Epoch: 16, Train_Loss: 0.38737595081329346, Test_Loss: 0.3961540758609772 *\n",
      "317:Epoch: 16, Train_Loss: 0.40163323283195496, Test_Loss: 0.3899208903312683 *\n",
      "318:Epoch: 16, Train_Loss: 0.38811489939689636, Test_Loss: 0.382752001285553 *\n",
      "319:Epoch: 16, Train_Loss: 0.3905049264431, Test_Loss: 0.3808070719242096 *\n",
      "320:Epoch: 16, Train_Loss: 0.3949410021305084, Test_Loss: 0.38494348526000977\n",
      "321:Epoch: 16, Train_Loss: 0.38110679388046265, Test_Loss: 0.3824009895324707 *\n",
      "322:Epoch: 16, Train_Loss: 0.38132110238075256, Test_Loss: 0.38123953342437744 *\n",
      "323:Epoch: 16, Train_Loss: 0.38095158338546753, Test_Loss: 0.381212055683136 *\n",
      "324:Epoch: 16, Train_Loss: 0.4141407012939453, Test_Loss: 0.38241511583328247\n",
      "325:Epoch: 16, Train_Loss: 0.4100930392742157, Test_Loss: 0.38308611512184143\n",
      "326:Epoch: 16, Train_Loss: 0.39148542284965515, Test_Loss: 0.38433417677879333\n",
      "327:Epoch: 16, Train_Loss: 0.3993107080459595, Test_Loss: 0.3807426393032074 *\n",
      "328:Epoch: 16, Train_Loss: 0.4335727393627167, Test_Loss: 0.381071001291275\n",
      "329:Epoch: 16, Train_Loss: 0.4248391091823578, Test_Loss: 0.3817814886569977\n",
      "330:Epoch: 16, Train_Loss: 0.3853302001953125, Test_Loss: 0.38075703382492065 *\n",
      "331:Epoch: 16, Train_Loss: 0.4041564464569092, Test_Loss: 0.38450461626052856\n",
      "332:Epoch: 16, Train_Loss: 0.3915213942527771, Test_Loss: 0.44031673669815063\n",
      "333:Epoch: 16, Train_Loss: 0.3923751711845398, Test_Loss: 0.834642767906189\n",
      "334:Epoch: 16, Train_Loss: 0.38416042923927307, Test_Loss: 5.386599063873291\n",
      "335:Epoch: 16, Train_Loss: 0.40661099553108215, Test_Loss: 0.3849755823612213 *\n",
      "336:Epoch: 16, Train_Loss: 0.43751218914985657, Test_Loss: 0.38844501972198486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337:Epoch: 16, Train_Loss: 2.646101474761963, Test_Loss: 0.423330694437027\n",
      "338:Epoch: 16, Train_Loss: 3.392686605453491, Test_Loss: 0.43576306104660034\n",
      "339:Epoch: 16, Train_Loss: 0.3935823142528534, Test_Loss: 0.43138450384140015 *\n",
      "340:Epoch: 16, Train_Loss: 0.3794298470020294, Test_Loss: 0.38444843888282776 *\n",
      "341:Epoch: 16, Train_Loss: 0.4371028244495392, Test_Loss: 0.47686338424682617\n",
      "342:Epoch: 16, Train_Loss: 0.508951723575592, Test_Loss: 0.4055323898792267 *\n",
      "343:Epoch: 16, Train_Loss: 0.4042898714542389, Test_Loss: 0.3847351372241974 *\n",
      "344:Epoch: 16, Train_Loss: 0.3818657696247101, Test_Loss: 0.4026213586330414\n",
      "345:Epoch: 16, Train_Loss: 0.39293742179870605, Test_Loss: 0.42244014143943787\n",
      "346:Epoch: 16, Train_Loss: 0.4287070035934448, Test_Loss: 0.38722261786460876 *\n",
      "347:Epoch: 16, Train_Loss: 0.38849419355392456, Test_Loss: 0.4408726990222931\n",
      "348:Epoch: 16, Train_Loss: 0.38849055767059326, Test_Loss: 0.4603368043899536\n",
      "349:Epoch: 16, Train_Loss: 1.1879686117172241, Test_Loss: 0.427910178899765 *\n",
      "350:Epoch: 16, Train_Loss: 1.360548734664917, Test_Loss: 0.40905460715293884 *\n",
      "351:Epoch: 16, Train_Loss: 0.6354537606239319, Test_Loss: 0.411503404378891\n",
      "352:Epoch: 16, Train_Loss: 0.45351970195770264, Test_Loss: 0.4898742437362671\n",
      "353:Epoch: 16, Train_Loss: 1.4693474769592285, Test_Loss: 0.3908947706222534 *\n",
      "354:Epoch: 16, Train_Loss: 2.0271143913269043, Test_Loss: 0.38053491711616516 *\n",
      "355:Epoch: 16, Train_Loss: 0.5116119980812073, Test_Loss: 0.3909834921360016\n",
      "356:Epoch: 16, Train_Loss: 0.3953996002674103, Test_Loss: 0.4151362180709839\n",
      "357:Epoch: 16, Train_Loss: 0.4759654700756073, Test_Loss: 0.4068008065223694 *\n",
      "358:Epoch: 16, Train_Loss: 1.1929779052734375, Test_Loss: 0.39314720034599304 *\n",
      "359:Epoch: 16, Train_Loss: 1.1220855712890625, Test_Loss: 0.3805755376815796 *\n",
      "360:Epoch: 16, Train_Loss: 0.38527825474739075, Test_Loss: 0.38358479738235474\n",
      "361:Epoch: 16, Train_Loss: 0.39947471022605896, Test_Loss: 0.38139891624450684 *\n",
      "362:Epoch: 16, Train_Loss: 0.38912010192871094, Test_Loss: 0.3971208333969116\n",
      "363:Epoch: 16, Train_Loss: 0.7289711236953735, Test_Loss: 0.3840043246746063 *\n",
      "364:Epoch: 16, Train_Loss: 0.41375014185905457, Test_Loss: 0.38626572489738464\n",
      "365:Epoch: 16, Train_Loss: 0.44505155086517334, Test_Loss: 0.4195723533630371\n",
      "366:Epoch: 16, Train_Loss: 0.3882395327091217, Test_Loss: 0.5850669741630554\n",
      "367:Epoch: 16, Train_Loss: 0.4011136591434479, Test_Loss: 0.5594221949577332 *\n",
      "368:Epoch: 16, Train_Loss: 0.4824482500553131, Test_Loss: 0.5349292755126953 *\n",
      "369:Epoch: 16, Train_Loss: 0.5036710500717163, Test_Loss: 0.4014957547187805 *\n",
      "370:Epoch: 16, Train_Loss: 0.46228086948394775, Test_Loss: 0.40273332595825195\n",
      "371:Epoch: 16, Train_Loss: 0.4055466055870056, Test_Loss: 0.38365304470062256 *\n",
      "372:Epoch: 16, Train_Loss: 0.4623429775238037, Test_Loss: 0.42725494503974915\n",
      "373:Epoch: 16, Train_Loss: 0.40721967816352844, Test_Loss: 0.5182399153709412\n",
      "374:Epoch: 16, Train_Loss: 0.446563184261322, Test_Loss: 0.5488651990890503\n",
      "375:Epoch: 16, Train_Loss: 0.4760029911994934, Test_Loss: 0.4523007273674011 *\n",
      "376:Epoch: 16, Train_Loss: 0.41973504424095154, Test_Loss: 0.44276949763298035 *\n",
      "377:Epoch: 16, Train_Loss: 0.5041207671165466, Test_Loss: 0.4025515913963318 *\n",
      "378:Epoch: 16, Train_Loss: 0.5059178471565247, Test_Loss: 0.41583946347236633\n",
      "379:Epoch: 16, Train_Loss: 0.4282262623310089, Test_Loss: 0.42359960079193115\n",
      "380:Epoch: 16, Train_Loss: 0.3885165750980377, Test_Loss: 0.545236349105835\n",
      "381:Epoch: 16, Train_Loss: 0.3794066309928894, Test_Loss: 0.4060917794704437 *\n",
      "382:Epoch: 16, Train_Loss: 0.3782784938812256, Test_Loss: 0.5120263695716858\n",
      "383:Epoch: 16, Train_Loss: 0.375416100025177, Test_Loss: 0.42923879623413086 *\n",
      "384:Epoch: 16, Train_Loss: 0.38331860303878784, Test_Loss: 0.7638002634048462\n",
      "385:Epoch: 16, Train_Loss: 0.40324538946151733, Test_Loss: 0.7455075979232788 *\n",
      "386:Epoch: 16, Train_Loss: 0.39303654432296753, Test_Loss: 1.102675437927246\n",
      "387:Epoch: 16, Train_Loss: 0.42826446890830994, Test_Loss: 0.9778972268104553 *\n",
      "388:Epoch: 16, Train_Loss: 0.5224694013595581, Test_Loss: 0.7063421607017517 *\n",
      "389:Epoch: 16, Train_Loss: 0.5984289050102234, Test_Loss: 0.7029646039009094 *\n",
      "390:Epoch: 16, Train_Loss: 0.38020700216293335, Test_Loss: 0.7030404806137085\n",
      "391:Epoch: 16, Train_Loss: 0.43865588307380676, Test_Loss: 0.7919138669967651\n",
      "392:Epoch: 16, Train_Loss: 0.45675599575042725, Test_Loss: 0.7366889715194702 *\n",
      "393:Epoch: 16, Train_Loss: 0.5318315029144287, Test_Loss: 5.464535713195801\n",
      "394:Epoch: 16, Train_Loss: 0.5633149743080139, Test_Loss: 0.5556743144989014 *\n",
      "395:Epoch: 16, Train_Loss: 0.4492921233177185, Test_Loss: 0.4152391850948334 *\n",
      "396:Epoch: 16, Train_Loss: 0.5757390260696411, Test_Loss: 0.40162166953086853 *\n",
      "397:Epoch: 16, Train_Loss: 0.5637592077255249, Test_Loss: 0.3962721526622772 *\n",
      "398:Epoch: 16, Train_Loss: 0.4877113103866577, Test_Loss: 0.3898318111896515 *\n",
      "399:Epoch: 16, Train_Loss: 0.4050772488117218, Test_Loss: 0.4494471251964569\n",
      "400:Epoch: 16, Train_Loss: 0.38598453998565674, Test_Loss: 0.47594937682151794\n",
      "Model saved at location ../Saver/model.ckpt at epoch 16\n",
      "401:Epoch: 16, Train_Loss: 0.4774649441242218, Test_Loss: 0.3902663588523865 *\n",
      "402:Epoch: 16, Train_Loss: 0.818299412727356, Test_Loss: 0.4144851565361023\n",
      "403:Epoch: 16, Train_Loss: 0.7963322401046753, Test_Loss: 0.42612284421920776\n",
      "404:Epoch: 16, Train_Loss: 0.3920084834098816, Test_Loss: 0.512696385383606\n",
      "405:Epoch: 16, Train_Loss: 0.4298519194126129, Test_Loss: 0.424210786819458 *\n",
      "406:Epoch: 16, Train_Loss: 0.39036041498184204, Test_Loss: 0.45273876190185547\n",
      "407:Epoch: 16, Train_Loss: 0.4748532772064209, Test_Loss: 0.49519625306129456\n",
      "408:Epoch: 16, Train_Loss: 0.6819330453872681, Test_Loss: 0.3928316831588745 *\n",
      "409:Epoch: 16, Train_Loss: 0.3766048848628998, Test_Loss: 0.4101621210575104\n",
      "410:Epoch: 16, Train_Loss: 0.525187611579895, Test_Loss: 0.48092377185821533\n",
      "411:Epoch: 16, Train_Loss: 0.40802547335624695, Test_Loss: 0.4869614243507385\n",
      "412:Epoch: 16, Train_Loss: 0.4046255350112915, Test_Loss: 0.3988898992538452 *\n",
      "413:Epoch: 16, Train_Loss: 0.43465688824653625, Test_Loss: 0.42831534147262573\n",
      "414:Epoch: 16, Train_Loss: 0.49235957860946655, Test_Loss: 0.40048953890800476 *\n",
      "415:Epoch: 16, Train_Loss: 0.4654097557067871, Test_Loss: 0.46915510296821594\n",
      "416:Epoch: 16, Train_Loss: 0.559798538684845, Test_Loss: 0.4773911237716675\n",
      "417:Epoch: 16, Train_Loss: 0.42695459723472595, Test_Loss: 0.4692847728729248 *\n",
      "418:Epoch: 16, Train_Loss: 0.4260939359664917, Test_Loss: 0.4060239791870117 *\n",
      "419:Epoch: 16, Train_Loss: 0.44331425428390503, Test_Loss: 0.4245797395706177\n",
      "420:Epoch: 16, Train_Loss: 0.42028677463531494, Test_Loss: 0.4370279908180237\n",
      "421:Epoch: 16, Train_Loss: 0.41558945178985596, Test_Loss: 0.3979838788509369 *\n",
      "422:Epoch: 16, Train_Loss: 0.4041577875614166, Test_Loss: 0.4182450771331787\n",
      "423:Epoch: 16, Train_Loss: 0.448699414730072, Test_Loss: 0.4915885925292969\n",
      "424:Epoch: 16, Train_Loss: 0.5861297845840454, Test_Loss: 2.081622362136841\n",
      "425:Epoch: 16, Train_Loss: 0.6108360290527344, Test_Loss: 4.245325088500977\n",
      "426:Epoch: 16, Train_Loss: 0.7703154683113098, Test_Loss: 0.40468358993530273 *\n",
      "427:Epoch: 16, Train_Loss: 0.6569505929946899, Test_Loss: 0.4295504093170166\n",
      "428:Epoch: 16, Train_Loss: 0.5755388140678406, Test_Loss: 0.4389439821243286\n",
      "429:Epoch: 16, Train_Loss: 0.5093162059783936, Test_Loss: 0.37390488386154175 *\n",
      "430:Epoch: 16, Train_Loss: 0.432024747133255, Test_Loss: 0.38379424810409546\n",
      "431:Epoch: 16, Train_Loss: 0.3935263156890869, Test_Loss: 0.4217418432235718\n",
      "432:Epoch: 16, Train_Loss: 0.38343533873558044, Test_Loss: 0.4664890170097351\n",
      "433:Epoch: 16, Train_Loss: 0.4395602345466614, Test_Loss: 0.38256213068962097 *\n",
      "434:Epoch: 16, Train_Loss: 0.6856187582015991, Test_Loss: 0.3915202021598816\n",
      "435:Epoch: 16, Train_Loss: 0.8351427316665649, Test_Loss: 0.40318459272384644\n",
      "436:Epoch: 16, Train_Loss: 1.234775424003601, Test_Loss: 0.4723847210407257\n",
      "437:Epoch: 16, Train_Loss: 1.4431992769241333, Test_Loss: 0.4057563543319702 *\n",
      "438:Epoch: 16, Train_Loss: 0.6532508730888367, Test_Loss: 0.44172555208206177\n",
      "439:Epoch: 16, Train_Loss: 0.6484155654907227, Test_Loss: 0.3935641050338745 *\n",
      "440:Epoch: 16, Train_Loss: 0.3786410689353943, Test_Loss: 0.45294129848480225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441:Epoch: 16, Train_Loss: 0.40884432196617126, Test_Loss: 0.3812735974788666 *\n",
      "442:Epoch: 16, Train_Loss: 0.6963467597961426, Test_Loss: 0.4312523603439331\n",
      "443:Epoch: 16, Train_Loss: 1.3006305694580078, Test_Loss: 0.4816610813140869\n",
      "444:Epoch: 16, Train_Loss: 0.4184657633304596, Test_Loss: 0.40299150347709656 *\n",
      "445:Epoch: 16, Train_Loss: 0.41282427310943604, Test_Loss: 0.4589521586894989\n",
      "446:Epoch: 16, Train_Loss: 0.41742444038391113, Test_Loss: 0.4369168281555176 *\n",
      "447:Epoch: 16, Train_Loss: 0.5127032399177551, Test_Loss: 0.4169517755508423 *\n",
      "448:Epoch: 16, Train_Loss: 0.6347154378890991, Test_Loss: 0.4178178310394287\n",
      "449:Epoch: 16, Train_Loss: 0.6381740570068359, Test_Loss: 0.4396679401397705\n",
      "450:Epoch: 16, Train_Loss: 0.5785834789276123, Test_Loss: 0.44134625792503357\n",
      "451:Epoch: 16, Train_Loss: 0.6618025302886963, Test_Loss: 0.4629089832305908\n",
      "452:Epoch: 16, Train_Loss: 0.3948327600955963, Test_Loss: 0.3989730179309845 *\n",
      "453:Epoch: 16, Train_Loss: 0.40667474269866943, Test_Loss: 0.45158877968788147\n",
      "454:Epoch: 16, Train_Loss: 0.39172497391700745, Test_Loss: 0.4060957133769989 *\n",
      "1:Epoch: 17, Train_Loss: 0.4408256709575653, Test_Loss: 0.3811405897140503 *\n",
      "2:Epoch: 17, Train_Loss: 0.37769684195518494, Test_Loss: 0.4756416082382202\n",
      "3:Epoch: 17, Train_Loss: 0.4295709431171417, Test_Loss: 0.47927701473236084\n",
      "4:Epoch: 17, Train_Loss: 15.208487510681152, Test_Loss: 0.45913165807724 *\n",
      "5:Epoch: 17, Train_Loss: 0.5130353569984436, Test_Loss: 0.4198172092437744 *\n",
      "6:Epoch: 17, Train_Loss: 1.7107985019683838, Test_Loss: 0.4921298623085022\n",
      "7:Epoch: 17, Train_Loss: 1.3996349573135376, Test_Loss: 0.5721137523651123\n",
      "8:Epoch: 17, Train_Loss: 0.3813585638999939, Test_Loss: 0.4472067356109619 *\n",
      "9:Epoch: 17, Train_Loss: 0.4640158414840698, Test_Loss: 0.46479320526123047\n",
      "10:Epoch: 17, Train_Loss: 2.05210018157959, Test_Loss: 0.6149912476539612\n",
      "11:Epoch: 17, Train_Loss: 3.9474265575408936, Test_Loss: 0.8432049751281738\n",
      "12:Epoch: 17, Train_Loss: 0.5688527822494507, Test_Loss: 1.021432876586914\n",
      "13:Epoch: 17, Train_Loss: 0.64469313621521, Test_Loss: 2.000452756881714\n",
      "14:Epoch: 17, Train_Loss: 3.0045180320739746, Test_Loss: 1.5692436695098877 *\n",
      "15:Epoch: 17, Train_Loss: 1.4948140382766724, Test_Loss: 1.8954360485076904\n",
      "16:Epoch: 17, Train_Loss: 0.6073938012123108, Test_Loss: 2.7258477210998535\n",
      "17:Epoch: 17, Train_Loss: 0.3991915285587311, Test_Loss: 1.0297068357467651 *\n",
      "18:Epoch: 17, Train_Loss: 0.3812969923019409, Test_Loss: 1.725730299949646\n",
      "19:Epoch: 17, Train_Loss: 0.5045722126960754, Test_Loss: 0.7771264314651489 *\n",
      "20:Epoch: 17, Train_Loss: 0.37747517228126526, Test_Loss: 1.8764432668685913\n",
      "21:Epoch: 17, Train_Loss: 0.41869503259658813, Test_Loss: 2.0552401542663574\n",
      "22:Epoch: 17, Train_Loss: 0.3698872923851013, Test_Loss: 2.5367543697357178\n",
      "23:Epoch: 17, Train_Loss: 0.36858436465263367, Test_Loss: 0.7232754230499268 *\n",
      "24:Epoch: 17, Train_Loss: 0.37011703848838806, Test_Loss: 0.4919731020927429 *\n",
      "25:Epoch: 17, Train_Loss: 0.389445036649704, Test_Loss: 0.4538552463054657 *\n",
      "26:Epoch: 17, Train_Loss: 0.4862097203731537, Test_Loss: 0.4239893853664398 *\n",
      "27:Epoch: 17, Train_Loss: 0.4046490490436554, Test_Loss: 0.4040428400039673 *\n",
      "28:Epoch: 17, Train_Loss: 0.3949926197528839, Test_Loss: 0.4087844789028168\n",
      "29:Epoch: 17, Train_Loss: 0.377669095993042, Test_Loss: 1.2503151893615723\n",
      "30:Epoch: 17, Train_Loss: 0.49087005853652954, Test_Loss: 8.364383697509766\n",
      "31:Epoch: 17, Train_Loss: 0.3869326114654541, Test_Loss: 1.1091277599334717 *\n",
      "32:Epoch: 17, Train_Loss: 0.45649129152297974, Test_Loss: 1.7855571508407593\n",
      "33:Epoch: 17, Train_Loss: 0.3668338656425476, Test_Loss: 1.3641586303710938 *\n",
      "34:Epoch: 17, Train_Loss: 0.3684103190898895, Test_Loss: 0.8196550607681274 *\n",
      "35:Epoch: 17, Train_Loss: 0.36764097213745117, Test_Loss: 0.8821247816085815\n",
      "36:Epoch: 17, Train_Loss: 0.366828590631485, Test_Loss: 1.8600479364395142\n",
      "37:Epoch: 17, Train_Loss: 0.36932122707366943, Test_Loss: 1.2199101448059082 *\n",
      "38:Epoch: 17, Train_Loss: 0.3686886429786682, Test_Loss: 0.3893141448497772 *\n",
      "39:Epoch: 17, Train_Loss: 0.3684859275817871, Test_Loss: 0.5625349879264832\n",
      "40:Epoch: 17, Train_Loss: 0.3764135539531708, Test_Loss: 0.5142536163330078 *\n",
      "41:Epoch: 17, Train_Loss: 0.4046611487865448, Test_Loss: 1.0508924722671509\n",
      "42:Epoch: 17, Train_Loss: 0.41984254121780396, Test_Loss: 0.7259364128112793 *\n",
      "43:Epoch: 17, Train_Loss: 0.44657596945762634, Test_Loss: 1.2600029706954956\n",
      "44:Epoch: 17, Train_Loss: 0.3989091217517853, Test_Loss: 1.0280787944793701 *\n",
      "45:Epoch: 17, Train_Loss: 0.842700719833374, Test_Loss: 0.36827611923217773 *\n",
      "46:Epoch: 17, Train_Loss: 7.168201923370361, Test_Loss: 0.3738693594932556\n",
      "47:Epoch: 17, Train_Loss: 0.40696829557418823, Test_Loss: 0.37035131454467773 *\n",
      "48:Epoch: 17, Train_Loss: 0.4216235876083374, Test_Loss: 0.7894797325134277\n",
      "49:Epoch: 17, Train_Loss: 0.4892120957374573, Test_Loss: 0.3912862539291382 *\n",
      "50:Epoch: 17, Train_Loss: 0.5400059223175049, Test_Loss: 0.6048428416252136\n",
      "51:Epoch: 17, Train_Loss: 0.5041687488555908, Test_Loss: 0.4192657470703125 *\n",
      "52:Epoch: 17, Train_Loss: 0.5209951400756836, Test_Loss: 0.7719735503196716\n",
      "53:Epoch: 17, Train_Loss: 0.5718991756439209, Test_Loss: 0.631987452507019 *\n",
      "54:Epoch: 17, Train_Loss: 0.5160818099975586, Test_Loss: 0.491965651512146 *\n",
      "55:Epoch: 17, Train_Loss: 0.4295244514942169, Test_Loss: 0.39297908544540405 *\n",
      "56:Epoch: 17, Train_Loss: 0.40886321663856506, Test_Loss: 0.4119237959384918\n",
      "57:Epoch: 17, Train_Loss: 0.37043216824531555, Test_Loss: 0.4020272493362427 *\n",
      "58:Epoch: 17, Train_Loss: 0.4241980314254761, Test_Loss: 0.37196141481399536 *\n",
      "59:Epoch: 17, Train_Loss: 0.40970268845558167, Test_Loss: 0.4015706479549408\n",
      "60:Epoch: 17, Train_Loss: 0.5907922387123108, Test_Loss: 0.44615647196769714\n",
      "61:Epoch: 17, Train_Loss: 0.41817399859428406, Test_Loss: 3.1734933853149414\n",
      "62:Epoch: 17, Train_Loss: 0.4088861346244812, Test_Loss: 3.195791721343994\n",
      "63:Epoch: 17, Train_Loss: 0.371781587600708, Test_Loss: 0.3777582049369812 *\n",
      "64:Epoch: 17, Train_Loss: 0.4075655937194824, Test_Loss: 0.3803117275238037\n",
      "65:Epoch: 17, Train_Loss: 0.4459649324417114, Test_Loss: 0.408554345369339\n",
      "66:Epoch: 17, Train_Loss: 0.3928990066051483, Test_Loss: 0.38562411069869995 *\n",
      "67:Epoch: 17, Train_Loss: 0.3677924573421478, Test_Loss: 0.3999879062175751\n",
      "68:Epoch: 17, Train_Loss: 0.3665308952331543, Test_Loss: 0.4144490957260132\n",
      "69:Epoch: 17, Train_Loss: 0.3654951751232147, Test_Loss: 0.4482882022857666\n",
      "70:Epoch: 17, Train_Loss: 1.425344705581665, Test_Loss: 0.3688538670539856 *\n",
      "71:Epoch: 17, Train_Loss: 5.2625579833984375, Test_Loss: 0.3867145776748657\n",
      "72:Epoch: 17, Train_Loss: 0.3690314292907715, Test_Loss: 0.38487136363983154 *\n",
      "73:Epoch: 17, Train_Loss: 0.3653644025325775, Test_Loss: 0.40517738461494446\n",
      "74:Epoch: 17, Train_Loss: 0.36971375346183777, Test_Loss: 0.3738754391670227 *\n",
      "75:Epoch: 17, Train_Loss: 0.36873769760131836, Test_Loss: 0.4318753778934479\n",
      "76:Epoch: 17, Train_Loss: 0.3656024634838104, Test_Loss: 0.4170446991920471 *\n",
      "77:Epoch: 17, Train_Loss: 0.3644823431968689, Test_Loss: 0.44776451587677\n",
      "78:Epoch: 17, Train_Loss: 0.37368762493133545, Test_Loss: 0.41547891497612 *\n",
      "79:Epoch: 17, Train_Loss: 0.41475167870521545, Test_Loss: 0.38581809401512146 *\n",
      "80:Epoch: 17, Train_Loss: 0.38221102952957153, Test_Loss: 0.3880403935909271\n",
      "81:Epoch: 17, Train_Loss: 0.3655819594860077, Test_Loss: 0.36675071716308594 *\n",
      "82:Epoch: 17, Train_Loss: 0.3643189072608948, Test_Loss: 0.3669624924659729\n",
      "83:Epoch: 17, Train_Loss: 0.36356186866760254, Test_Loss: 0.36546894907951355 *\n",
      "84:Epoch: 17, Train_Loss: 0.3785047233104706, Test_Loss: 0.37318211793899536\n",
      "85:Epoch: 17, Train_Loss: 0.36651620268821716, Test_Loss: 0.36919599771499634 *\n",
      "86:Epoch: 17, Train_Loss: 0.3653358519077301, Test_Loss: 0.3650340735912323 *\n",
      "87:Epoch: 17, Train_Loss: 0.3927188515663147, Test_Loss: 0.3694462776184082\n",
      "88:Epoch: 17, Train_Loss: 0.4062585234642029, Test_Loss: 0.36629438400268555 *\n",
      "89:Epoch: 17, Train_Loss: 0.37177932262420654, Test_Loss: 0.369954377412796\n",
      "90:Epoch: 17, Train_Loss: 0.36196428537368774, Test_Loss: 0.371359258890152\n",
      "91:Epoch: 17, Train_Loss: 0.37429702281951904, Test_Loss: 0.374328076839447\n",
      "92:Epoch: 17, Train_Loss: 0.45715317130088806, Test_Loss: 0.40412089228630066\n",
      "93:Epoch: 17, Train_Loss: 0.43006622791290283, Test_Loss: 0.39737841486930847 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94:Epoch: 17, Train_Loss: 0.4405810832977295, Test_Loss: 0.752528190612793\n",
      "95:Epoch: 17, Train_Loss: 0.3981480002403259, Test_Loss: 0.7379767894744873 *\n",
      "96:Epoch: 17, Train_Loss: 0.422005832195282, Test_Loss: 0.468997061252594 *\n",
      "97:Epoch: 17, Train_Loss: 0.38843461871147156, Test_Loss: 0.3767968714237213 *\n",
      "98:Epoch: 17, Train_Loss: 0.4196968376636505, Test_Loss: 0.3796283006668091\n",
      "99:Epoch: 17, Train_Loss: 0.3800615668296814, Test_Loss: 0.40501952171325684\n",
      "100:Epoch: 17, Train_Loss: 0.49679046869277954, Test_Loss: 0.6160662174224854\n",
      "Model saved at location ../Saver/model.ckpt at epoch 17\n",
      "101:Epoch: 17, Train_Loss: 0.38426917791366577, Test_Loss: 0.9465342164039612\n",
      "102:Epoch: 17, Train_Loss: 0.36498504877090454, Test_Loss: 0.8176097273826599 *\n",
      "103:Epoch: 17, Train_Loss: 0.36491796374320984, Test_Loss: 0.44213375449180603 *\n",
      "104:Epoch: 17, Train_Loss: 0.36185383796691895, Test_Loss: 0.3914150893688202 *\n",
      "105:Epoch: 17, Train_Loss: 0.36183515191078186, Test_Loss: 0.3724364638328552 *\n",
      "106:Epoch: 17, Train_Loss: 0.3612020015716553, Test_Loss: 0.3681858479976654 *\n",
      "107:Epoch: 17, Train_Loss: 1.3418596982955933, Test_Loss: 0.3724405765533447\n",
      "108:Epoch: 17, Train_Loss: 3.9084932804107666, Test_Loss: 0.37459495663642883\n",
      "109:Epoch: 17, Train_Loss: 0.3675704598426819, Test_Loss: 0.4124017059803009\n",
      "110:Epoch: 17, Train_Loss: 0.36546623706817627, Test_Loss: 0.385976105928421 *\n",
      "111:Epoch: 17, Train_Loss: 0.36482828855514526, Test_Loss: 0.4328470826148987\n",
      "112:Epoch: 17, Train_Loss: 0.3670594394207001, Test_Loss: 0.4815528094768524\n",
      "113:Epoch: 17, Train_Loss: 0.36358383297920227, Test_Loss: 0.7396506071090698\n",
      "114:Epoch: 17, Train_Loss: 0.36657893657684326, Test_Loss: 0.7308404445648193 *\n",
      "115:Epoch: 17, Train_Loss: 0.3601549565792084, Test_Loss: 0.4243682026863098 *\n",
      "116:Epoch: 17, Train_Loss: 0.36052191257476807, Test_Loss: 0.4057984948158264 *\n",
      "117:Epoch: 17, Train_Loss: 0.3639645576477051, Test_Loss: 0.40603089332580566\n",
      "118:Epoch: 17, Train_Loss: 0.3922147750854492, Test_Loss: 0.40677329897880554\n",
      "119:Epoch: 17, Train_Loss: 0.3782290816307068, Test_Loss: 0.4938836395740509\n",
      "120:Epoch: 17, Train_Loss: 0.42813992500305176, Test_Loss: 1.7416496276855469\n",
      "121:Epoch: 17, Train_Loss: 0.38626495003700256, Test_Loss: 4.033602714538574\n",
      "122:Epoch: 17, Train_Loss: 0.3654960095882416, Test_Loss: 0.3749127984046936 *\n",
      "123:Epoch: 17, Train_Loss: 0.5393496751785278, Test_Loss: 0.36580216884613037 *\n",
      "124:Epoch: 17, Train_Loss: 0.5784130692481995, Test_Loss: 0.3649674952030182 *\n",
      "125:Epoch: 17, Train_Loss: 0.5698570013046265, Test_Loss: 0.3696783781051636\n",
      "126:Epoch: 17, Train_Loss: 0.4800366461277008, Test_Loss: 0.36385056376457214 *\n",
      "127:Epoch: 17, Train_Loss: 0.3607960641384125, Test_Loss: 0.3670504093170166\n",
      "128:Epoch: 17, Train_Loss: 0.35900089144706726, Test_Loss: 0.3667455017566681 *\n",
      "129:Epoch: 17, Train_Loss: 0.3618287444114685, Test_Loss: 0.3624224364757538 *\n",
      "130:Epoch: 17, Train_Loss: 0.3700498342514038, Test_Loss: 0.36667540669441223\n",
      "131:Epoch: 17, Train_Loss: 0.3733154833316803, Test_Loss: 0.3649328649044037 *\n",
      "132:Epoch: 17, Train_Loss: 0.3760093152523041, Test_Loss: 0.38357266783714294\n",
      "133:Epoch: 17, Train_Loss: 0.3610406219959259, Test_Loss: 0.3775286078453064 *\n",
      "134:Epoch: 17, Train_Loss: 0.35850656032562256, Test_Loss: 0.3676110506057739 *\n",
      "135:Epoch: 17, Train_Loss: 0.37027883529663086, Test_Loss: 0.3735256493091583\n",
      "136:Epoch: 17, Train_Loss: 0.4037253260612488, Test_Loss: 0.36140868067741394 *\n",
      "137:Epoch: 17, Train_Loss: 0.5475148558616638, Test_Loss: 0.35966435074806213 *\n",
      "138:Epoch: 17, Train_Loss: 0.5121990442276001, Test_Loss: 0.36138808727264404\n",
      "139:Epoch: 17, Train_Loss: 0.5024850368499756, Test_Loss: 0.37096545100212097\n",
      "140:Epoch: 17, Train_Loss: 0.4247351884841919, Test_Loss: 0.3606674373149872 *\n",
      "141:Epoch: 17, Train_Loss: 0.4957098960876465, Test_Loss: 0.3668937683105469\n",
      "142:Epoch: 17, Train_Loss: 0.4466134309768677, Test_Loss: 0.36205175518989563 *\n",
      "143:Epoch: 17, Train_Loss: 0.4791407287120819, Test_Loss: 0.3726460933685303\n",
      "144:Epoch: 17, Train_Loss: 0.4742748439311981, Test_Loss: 0.37110379338264465 *\n",
      "145:Epoch: 17, Train_Loss: 0.6603837013244629, Test_Loss: 0.36706113815307617 *\n",
      "146:Epoch: 17, Train_Loss: 0.36805278062820435, Test_Loss: 0.3604475259780884 *\n",
      "147:Epoch: 17, Train_Loss: 0.3677445948123932, Test_Loss: 0.3634132742881775\n",
      "148:Epoch: 17, Train_Loss: 3.1279795169830322, Test_Loss: 0.36286860704421997 *\n",
      "149:Epoch: 17, Train_Loss: 0.644303560256958, Test_Loss: 0.36023595929145813 *\n",
      "150:Epoch: 17, Train_Loss: 0.41680747270584106, Test_Loss: 0.4088660478591919\n",
      "151:Epoch: 17, Train_Loss: 0.39932459592819214, Test_Loss: 0.4045712351799011 *\n",
      "152:Epoch: 17, Train_Loss: 0.3772047758102417, Test_Loss: 4.160740375518799\n",
      "153:Epoch: 17, Train_Loss: 0.3751050531864166, Test_Loss: 2.0413858890533447 *\n",
      "154:Epoch: 17, Train_Loss: 0.36997878551483154, Test_Loss: 0.3646880090236664 *\n",
      "155:Epoch: 17, Train_Loss: 0.4281822443008423, Test_Loss: 0.3727700412273407\n",
      "156:Epoch: 17, Train_Loss: 0.5060461163520813, Test_Loss: 0.41270437836647034\n",
      "157:Epoch: 17, Train_Loss: 0.45520177483558655, Test_Loss: 0.4051085412502289 *\n",
      "158:Epoch: 17, Train_Loss: 0.4298449754714966, Test_Loss: 0.3770967423915863 *\n",
      "159:Epoch: 17, Train_Loss: 0.39943671226501465, Test_Loss: 0.42333894968032837\n",
      "160:Epoch: 17, Train_Loss: 0.3826051950454712, Test_Loss: 0.42956602573394775\n",
      "161:Epoch: 17, Train_Loss: 0.39025840163230896, Test_Loss: 0.36177515983581543 *\n",
      "162:Epoch: 17, Train_Loss: 0.3631030023097992, Test_Loss: 0.3812592029571533\n",
      "163:Epoch: 17, Train_Loss: 0.3989121913909912, Test_Loss: 0.3861866295337677\n",
      "164:Epoch: 17, Train_Loss: 0.3762854337692261, Test_Loss: 0.3998490273952484\n",
      "165:Epoch: 17, Train_Loss: 0.3565806746482849, Test_Loss: 0.3702370524406433 *\n",
      "166:Epoch: 17, Train_Loss: 0.3721744418144226, Test_Loss: 0.46324509382247925\n",
      "167:Epoch: 17, Train_Loss: 0.40212714672088623, Test_Loss: 0.40737417340278625 *\n",
      "168:Epoch: 17, Train_Loss: 0.38653603196144104, Test_Loss: 0.423073947429657\n",
      "169:Epoch: 17, Train_Loss: 0.3586379289627075, Test_Loss: 0.37659019231796265 *\n",
      "170:Epoch: 17, Train_Loss: 0.35592418909072876, Test_Loss: 0.4264170825481415\n",
      "171:Epoch: 17, Train_Loss: 0.3563241958618164, Test_Loss: 0.40556830167770386 *\n",
      "172:Epoch: 17, Train_Loss: 0.35548660159111023, Test_Loss: 0.37335264682769775 *\n",
      "173:Epoch: 17, Train_Loss: 0.3570042550563812, Test_Loss: 0.3720783293247223 *\n",
      "174:Epoch: 17, Train_Loss: 0.3590492308139801, Test_Loss: 0.38846102356910706\n",
      "175:Epoch: 17, Train_Loss: 0.3592504858970642, Test_Loss: 0.403179407119751\n",
      "176:Epoch: 17, Train_Loss: 0.36235958337783813, Test_Loss: 0.38360992074012756 *\n",
      "177:Epoch: 17, Train_Loss: 0.3554409146308899, Test_Loss: 0.36912551522254944 *\n",
      "178:Epoch: 17, Train_Loss: 0.35591113567352295, Test_Loss: 0.37698352336883545\n",
      "179:Epoch: 17, Train_Loss: 0.3619967997074127, Test_Loss: 0.36998602747917175 *\n",
      "180:Epoch: 17, Train_Loss: 0.3730696737766266, Test_Loss: 0.3696043789386749 *\n",
      "181:Epoch: 17, Train_Loss: 0.37720727920532227, Test_Loss: 0.36372077465057373 *\n",
      "182:Epoch: 17, Train_Loss: 0.3713330328464508, Test_Loss: 0.39230436086654663\n",
      "183:Epoch: 17, Train_Loss: 0.3765409290790558, Test_Loss: 0.41437098383903503\n",
      "184:Epoch: 17, Train_Loss: 0.3809831738471985, Test_Loss: 0.5123060941696167\n",
      "185:Epoch: 17, Train_Loss: 0.3615778684616089, Test_Loss: 0.8462873697280884\n",
      "186:Epoch: 17, Train_Loss: 0.36003756523132324, Test_Loss: 0.7405309677124023 *\n",
      "187:Epoch: 17, Train_Loss: 0.3847832977771759, Test_Loss: 0.4695507884025574 *\n",
      "188:Epoch: 17, Train_Loss: 0.38331785798072815, Test_Loss: 0.3855668306350708 *\n",
      "189:Epoch: 17, Train_Loss: 0.35590019822120667, Test_Loss: 0.3705912232398987 *\n",
      "190:Epoch: 17, Train_Loss: 0.36361169815063477, Test_Loss: 0.40776681900024414\n",
      "191:Epoch: 17, Train_Loss: 0.3605445623397827, Test_Loss: 0.6802260875701904\n",
      "192:Epoch: 17, Train_Loss: 0.37738990783691406, Test_Loss: 0.8633524179458618\n",
      "193:Epoch: 17, Train_Loss: 0.40079057216644287, Test_Loss: 0.7078855037689209 *\n",
      "194:Epoch: 17, Train_Loss: 0.40413254499435425, Test_Loss: 0.4680710732936859 *\n",
      "195:Epoch: 17, Train_Loss: 0.3683822453022003, Test_Loss: 0.36617615818977356 *\n",
      "196:Epoch: 17, Train_Loss: 0.35568666458129883, Test_Loss: 0.3600720465183258 *\n",
      "197:Epoch: 17, Train_Loss: 0.4210502505302429, Test_Loss: 0.3550788164138794 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198:Epoch: 17, Train_Loss: 0.35800427198410034, Test_Loss: 0.3692759573459625\n",
      "199:Epoch: 17, Train_Loss: 0.3628447651863098, Test_Loss: 0.3768935799598694\n",
      "200:Epoch: 17, Train_Loss: 0.374605655670166, Test_Loss: 0.39394673705101013\n",
      "Model saved at location ../Saver/model.ckpt at epoch 17\n",
      "201:Epoch: 17, Train_Loss: 0.3661421239376068, Test_Loss: 0.3602922558784485 *\n",
      "202:Epoch: 17, Train_Loss: 0.4821391999721527, Test_Loss: 0.4415002465248108\n",
      "203:Epoch: 17, Train_Loss: 0.41978031396865845, Test_Loss: 0.5949611663818359\n",
      "204:Epoch: 17, Train_Loss: 0.3878743648529053, Test_Loss: 0.601341187953949\n",
      "205:Epoch: 17, Train_Loss: 0.36301204562187195, Test_Loss: 0.6148083806037903\n",
      "206:Epoch: 17, Train_Loss: 0.37511247396469116, Test_Loss: 0.3666253387928009 *\n",
      "207:Epoch: 17, Train_Loss: 0.35841771960258484, Test_Loss: 0.36307191848754883 *\n",
      "208:Epoch: 17, Train_Loss: 0.35712456703186035, Test_Loss: 0.3629016876220703 *\n",
      "209:Epoch: 17, Train_Loss: 0.36602362990379333, Test_Loss: 0.36306214332580566\n",
      "210:Epoch: 17, Train_Loss: 0.37253955006599426, Test_Loss: 0.4229174852371216\n",
      "211:Epoch: 17, Train_Loss: 0.39829087257385254, Test_Loss: 3.0907843112945557\n",
      "212:Epoch: 17, Train_Loss: 0.4343779683113098, Test_Loss: 2.9236178398132324 *\n",
      "213:Epoch: 17, Train_Loss: 0.37669676542282104, Test_Loss: 0.36409544944763184 *\n",
      "214:Epoch: 17, Train_Loss: 0.40231093764305115, Test_Loss: 0.3566818833351135 *\n",
      "215:Epoch: 17, Train_Loss: 0.3805789053440094, Test_Loss: 0.3556601107120514 *\n",
      "216:Epoch: 17, Train_Loss: 0.36967140436172485, Test_Loss: 0.36472687125205994\n",
      "217:Epoch: 17, Train_Loss: 0.4546455144882202, Test_Loss: 0.3548245429992676 *\n",
      "218:Epoch: 17, Train_Loss: 0.5834581851959229, Test_Loss: 0.3599507808685303\n",
      "219:Epoch: 17, Train_Loss: 0.3583042323589325, Test_Loss: 0.35436493158340454 *\n",
      "220:Epoch: 17, Train_Loss: 0.39026761054992676, Test_Loss: 0.3550536036491394\n",
      "221:Epoch: 17, Train_Loss: 0.35304969549179077, Test_Loss: 0.35690608620643616\n",
      "222:Epoch: 17, Train_Loss: 0.35170602798461914, Test_Loss: 0.3554553985595703 *\n",
      "223:Epoch: 17, Train_Loss: 0.35230737924575806, Test_Loss: 0.3660689890384674\n",
      "224:Epoch: 17, Train_Loss: 0.35183513164520264, Test_Loss: 0.38060861825942993\n",
      "225:Epoch: 17, Train_Loss: 0.3635029196739197, Test_Loss: 0.37004005908966064 *\n",
      "226:Epoch: 17, Train_Loss: 0.3735584616661072, Test_Loss: 0.35693982243537903 *\n",
      "227:Epoch: 17, Train_Loss: 0.3638307750225067, Test_Loss: 0.3525729477405548 *\n",
      "228:Epoch: 17, Train_Loss: 0.3625710904598236, Test_Loss: 0.35174286365509033 *\n",
      "229:Epoch: 17, Train_Loss: 0.3663742244243622, Test_Loss: 0.3535596430301666\n",
      "230:Epoch: 17, Train_Loss: 0.35311195254325867, Test_Loss: 0.3534391522407532 *\n",
      "231:Epoch: 17, Train_Loss: 0.352544903755188, Test_Loss: 0.3524110019207001 *\n",
      "232:Epoch: 17, Train_Loss: 0.35075074434280396, Test_Loss: 0.3530495762825012\n",
      "233:Epoch: 17, Train_Loss: 0.37859198451042175, Test_Loss: 0.35239240527153015 *\n",
      "234:Epoch: 17, Train_Loss: 0.3787900507450104, Test_Loss: 0.3556117117404938\n",
      "235:Epoch: 17, Train_Loss: 0.3736126720905304, Test_Loss: 0.3542083501815796 *\n",
      "236:Epoch: 17, Train_Loss: 0.35838553309440613, Test_Loss: 0.3528817594051361 *\n",
      "237:Epoch: 17, Train_Loss: 0.40487632155418396, Test_Loss: 0.35137468576431274 *\n",
      "238:Epoch: 17, Train_Loss: 0.3912106156349182, Test_Loss: 0.3513484597206116 *\n",
      "239:Epoch: 17, Train_Loss: 0.37235599756240845, Test_Loss: 0.35258811712265015\n",
      "240:Epoch: 17, Train_Loss: 0.3651329278945923, Test_Loss: 0.3512672483921051 *\n",
      "241:Epoch: 17, Train_Loss: 0.3758721649646759, Test_Loss: 0.3922990560531616\n",
      "242:Epoch: 17, Train_Loss: 0.35589733719825745, Test_Loss: 0.3859356641769409 *\n",
      "243:Epoch: 17, Train_Loss: 0.36277928948402405, Test_Loss: 5.348674297332764\n",
      "244:Epoch: 17, Train_Loss: 0.37562012672424316, Test_Loss: 0.8669114708900452 *\n",
      "245:Epoch: 17, Train_Loss: 0.3930331766605377, Test_Loss: 0.3536021411418915 *\n",
      "246:Epoch: 17, Train_Loss: 2.371459722518921, Test_Loss: 0.3739020526409149\n",
      "247:Epoch: 17, Train_Loss: 3.648435354232788, Test_Loss: 0.40859195590019226\n",
      "248:Epoch: 17, Train_Loss: 0.3563375473022461, Test_Loss: 0.4080630838871002 *\n",
      "249:Epoch: 17, Train_Loss: 0.35987067222595215, Test_Loss: 0.3580894470214844 *\n",
      "250:Epoch: 17, Train_Loss: 0.38051414489746094, Test_Loss: 0.43673500418663025\n",
      "251:Epoch: 17, Train_Loss: 0.4703172743320465, Test_Loss: 0.41139131784439087 *\n",
      "252:Epoch: 17, Train_Loss: 0.3709036707878113, Test_Loss: 0.35194000601768494 *\n",
      "253:Epoch: 17, Train_Loss: 0.35616469383239746, Test_Loss: 0.37529218196868896\n",
      "254:Epoch: 17, Train_Loss: 0.353407084941864, Test_Loss: 0.37442129850387573 *\n",
      "255:Epoch: 17, Train_Loss: 0.4042026400566101, Test_Loss: 0.3643171787261963 *\n",
      "256:Epoch: 17, Train_Loss: 0.3591478168964386, Test_Loss: 0.3635872006416321 *\n",
      "257:Epoch: 17, Train_Loss: 0.3660157322883606, Test_Loss: 0.44474607706069946\n",
      "258:Epoch: 17, Train_Loss: 0.912704348564148, Test_Loss: 0.38751572370529175 *\n",
      "259:Epoch: 17, Train_Loss: 1.281828761100769, Test_Loss: 0.4093487560749054\n",
      "260:Epoch: 17, Train_Loss: 0.8398791551589966, Test_Loss: 0.37724968791007996 *\n",
      "261:Epoch: 17, Train_Loss: 0.4042741060256958, Test_Loss: 0.45140308141708374\n",
      "262:Epoch: 17, Train_Loss: 0.8859221935272217, Test_Loss: 0.3894581198692322 *\n",
      "263:Epoch: 17, Train_Loss: 2.2098865509033203, Test_Loss: 0.3529031574726105 *\n",
      "264:Epoch: 17, Train_Loss: 0.7932975888252258, Test_Loss: 0.3875822126865387\n",
      "265:Epoch: 17, Train_Loss: 0.38412854075431824, Test_Loss: 0.4130275249481201\n",
      "266:Epoch: 17, Train_Loss: 0.4103313684463501, Test_Loss: 0.46552518010139465\n",
      "267:Epoch: 17, Train_Loss: 0.8623272776603699, Test_Loss: 0.4138951301574707 *\n",
      "268:Epoch: 17, Train_Loss: 0.9541869759559631, Test_Loss: 0.361338347196579 *\n",
      "269:Epoch: 17, Train_Loss: 0.5554850697517395, Test_Loss: 0.35786575078964233 *\n",
      "270:Epoch: 17, Train_Loss: 0.35219767689704895, Test_Loss: 0.3682529628276825\n",
      "271:Epoch: 17, Train_Loss: 0.36196473240852356, Test_Loss: 0.37320753931999207\n",
      "272:Epoch: 17, Train_Loss: 0.7374527454376221, Test_Loss: 0.39800021052360535\n",
      "273:Epoch: 17, Train_Loss: 0.5095155239105225, Test_Loss: 0.35594868659973145 *\n",
      "274:Epoch: 17, Train_Loss: 0.3865818381309509, Test_Loss: 0.39311105012893677\n",
      "275:Epoch: 17, Train_Loss: 0.37557199597358704, Test_Loss: 0.5572742819786072\n",
      "276:Epoch: 17, Train_Loss: 0.4089454412460327, Test_Loss: 0.5364905595779419 *\n",
      "277:Epoch: 17, Train_Loss: 0.41254937648773193, Test_Loss: 0.5597900152206421\n",
      "278:Epoch: 17, Train_Loss: 0.49996644258499146, Test_Loss: 0.40501001477241516 *\n",
      "279:Epoch: 17, Train_Loss: 0.427215576171875, Test_Loss: 0.3592804968357086 *\n",
      "280:Epoch: 17, Train_Loss: 0.36572587490081787, Test_Loss: 0.37345561385154724\n",
      "281:Epoch: 17, Train_Loss: 0.43875226378440857, Test_Loss: 0.40960943698883057\n",
      "282:Epoch: 17, Train_Loss: 0.41520699858665466, Test_Loss: 0.5331112742424011\n",
      "283:Epoch: 17, Train_Loss: 0.46757543087005615, Test_Loss: 0.43567198514938354 *\n",
      "284:Epoch: 17, Train_Loss: 0.46514302492141724, Test_Loss: 0.5563421845436096\n",
      "285:Epoch: 17, Train_Loss: 0.4670679569244385, Test_Loss: 0.4367392361164093 *\n",
      "286:Epoch: 17, Train_Loss: 0.47928744554519653, Test_Loss: 0.4131089746952057 *\n",
      "287:Epoch: 17, Train_Loss: 0.47428175806999207, Test_Loss: 0.45903459191322327\n",
      "288:Epoch: 17, Train_Loss: 0.39534300565719604, Test_Loss: 0.4389108419418335 *\n",
      "289:Epoch: 17, Train_Loss: 0.3596619665622711, Test_Loss: 0.5232225060462952\n",
      "290:Epoch: 17, Train_Loss: 0.34848952293395996, Test_Loss: 0.4264385998249054 *\n",
      "291:Epoch: 17, Train_Loss: 0.34869638085365295, Test_Loss: 0.47851333022117615\n",
      "292:Epoch: 17, Train_Loss: 0.34860289096832275, Test_Loss: 0.4565340280532837 *\n",
      "293:Epoch: 17, Train_Loss: 0.3547414243221283, Test_Loss: 0.7454993724822998\n",
      "294:Epoch: 17, Train_Loss: 0.3842001259326935, Test_Loss: 0.7117618322372437 *\n",
      "295:Epoch: 17, Train_Loss: 0.37872204184532166, Test_Loss: 0.6750277280807495 *\n",
      "296:Epoch: 17, Train_Loss: 0.3936828374862671, Test_Loss: 1.3813211917877197\n",
      "297:Epoch: 17, Train_Loss: 0.4911441504955292, Test_Loss: 0.6306601762771606 *\n",
      "298:Epoch: 17, Train_Loss: 0.5090870261192322, Test_Loss: 0.6178747415542603 *\n",
      "299:Epoch: 17, Train_Loss: 0.36629873514175415, Test_Loss: 0.614858865737915 *\n",
      "300:Epoch: 17, Train_Loss: 0.392461359500885, Test_Loss: 0.6184411644935608\n",
      "Model saved at location ../Saver/model.ckpt at epoch 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301:Epoch: 17, Train_Loss: 0.38945356011390686, Test_Loss: 0.9384067058563232\n",
      "302:Epoch: 17, Train_Loss: 0.5355930924415588, Test_Loss: 4.436709880828857\n",
      "303:Epoch: 17, Train_Loss: 0.4955940246582031, Test_Loss: 1.697067141532898 *\n",
      "304:Epoch: 17, Train_Loss: 0.3833141028881073, Test_Loss: 0.4025549292564392 *\n",
      "305:Epoch: 17, Train_Loss: 0.43809863924980164, Test_Loss: 0.38600873947143555 *\n",
      "306:Epoch: 17, Train_Loss: 0.46390464901924133, Test_Loss: 0.39167505502700806\n",
      "307:Epoch: 17, Train_Loss: 0.42343589663505554, Test_Loss: 0.3534378111362457 *\n",
      "308:Epoch: 17, Train_Loss: 0.41579216718673706, Test_Loss: 0.430122971534729\n",
      "309:Epoch: 17, Train_Loss: 0.3651827871799469, Test_Loss: 0.4724872410297394\n",
      "310:Epoch: 17, Train_Loss: 0.42648810148239136, Test_Loss: 0.38170650601387024 *\n",
      "311:Epoch: 17, Train_Loss: 0.6478263139724731, Test_Loss: 0.3798213601112366 *\n",
      "312:Epoch: 17, Train_Loss: 0.7935324907302856, Test_Loss: 0.3769422173500061 *\n",
      "313:Epoch: 17, Train_Loss: 0.38614651560783386, Test_Loss: 0.410650372505188\n",
      "314:Epoch: 17, Train_Loss: 0.40920719504356384, Test_Loss: 0.44840896129608154\n",
      "315:Epoch: 17, Train_Loss: 0.3794881999492645, Test_Loss: 0.37680160999298096 *\n",
      "316:Epoch: 17, Train_Loss: 0.361054390668869, Test_Loss: 0.4279090166091919\n",
      "317:Epoch: 17, Train_Loss: 0.5994577407836914, Test_Loss: 0.3974205255508423 *\n",
      "318:Epoch: 17, Train_Loss: 0.35993650555610657, Test_Loss: 0.3829887807369232 *\n",
      "319:Epoch: 17, Train_Loss: 0.4392235279083252, Test_Loss: 0.40964627265930176\n",
      "320:Epoch: 17, Train_Loss: 0.4360368847846985, Test_Loss: 0.48555776476860046\n",
      "321:Epoch: 17, Train_Loss: 0.40099430084228516, Test_Loss: 0.3995068371295929 *\n",
      "322:Epoch: 17, Train_Loss: 0.39634430408477783, Test_Loss: 0.36851924657821655 *\n",
      "323:Epoch: 17, Train_Loss: 0.4663859009742737, Test_Loss: 0.39020636677742004\n",
      "324:Epoch: 17, Train_Loss: 0.45010024309158325, Test_Loss: 0.4169357717037201\n",
      "325:Epoch: 17, Train_Loss: 0.4097367227077484, Test_Loss: 0.41237664222717285 *\n",
      "326:Epoch: 17, Train_Loss: 0.4321466386318207, Test_Loss: 0.47150474786758423\n",
      "327:Epoch: 17, Train_Loss: 0.36138996481895447, Test_Loss: 0.3976009488105774 *\n",
      "328:Epoch: 17, Train_Loss: 0.45306649804115295, Test_Loss: 0.37704482674598694 *\n",
      "329:Epoch: 17, Train_Loss: 0.39263567328453064, Test_Loss: 0.4005378484725952\n",
      "330:Epoch: 17, Train_Loss: 0.3951699137687683, Test_Loss: 0.3917404115200043 *\n",
      "331:Epoch: 17, Train_Loss: 0.36873653531074524, Test_Loss: 0.36585307121276855 *\n",
      "332:Epoch: 17, Train_Loss: 0.39705702662467957, Test_Loss: 0.44794708490371704\n",
      "333:Epoch: 17, Train_Loss: 0.6089712381362915, Test_Loss: 0.466813862323761\n",
      "334:Epoch: 17, Train_Loss: 0.6258022785186768, Test_Loss: 6.225584983825684\n",
      "335:Epoch: 17, Train_Loss: 0.7765923738479614, Test_Loss: 0.3932664096355438 *\n",
      "336:Epoch: 17, Train_Loss: 0.6949920654296875, Test_Loss: 0.4198169708251953\n",
      "337:Epoch: 17, Train_Loss: 0.5319210290908813, Test_Loss: 0.41770994663238525 *\n",
      "338:Epoch: 17, Train_Loss: 0.5693536400794983, Test_Loss: 0.34987613558769226 *\n",
      "339:Epoch: 17, Train_Loss: 0.4125179946422577, Test_Loss: 0.35056841373443604\n",
      "340:Epoch: 17, Train_Loss: 0.35780763626098633, Test_Loss: 0.361659973859787\n",
      "341:Epoch: 17, Train_Loss: 0.3549818992614746, Test_Loss: 0.4314858317375183\n",
      "342:Epoch: 17, Train_Loss: 0.409125417470932, Test_Loss: 0.3791831135749817 *\n",
      "343:Epoch: 17, Train_Loss: 0.594430685043335, Test_Loss: 0.3512049615383148 *\n",
      "344:Epoch: 17, Train_Loss: 0.7788198590278625, Test_Loss: 0.36177530884742737\n",
      "345:Epoch: 17, Train_Loss: 0.8883717060089111, Test_Loss: 0.407937228679657\n",
      "346:Epoch: 17, Train_Loss: 1.5401535034179688, Test_Loss: 0.38140231370925903 *\n",
      "347:Epoch: 17, Train_Loss: 0.6382852792739868, Test_Loss: 0.4047415852546692\n",
      "348:Epoch: 17, Train_Loss: 0.6206586360931396, Test_Loss: 0.35113734006881714 *\n",
      "349:Epoch: 17, Train_Loss: 0.35260486602783203, Test_Loss: 0.43831735849380493\n",
      "350:Epoch: 17, Train_Loss: 0.35564231872558594, Test_Loss: 0.3750719130039215 *\n",
      "351:Epoch: 17, Train_Loss: 0.6116215586662292, Test_Loss: 0.4029264748096466\n",
      "352:Epoch: 17, Train_Loss: 1.2370272874832153, Test_Loss: 0.4478134512901306\n",
      "353:Epoch: 17, Train_Loss: 0.47453978657722473, Test_Loss: 0.37497061491012573 *\n",
      "354:Epoch: 17, Train_Loss: 0.3933563232421875, Test_Loss: 0.4820902347564697\n",
      "355:Epoch: 17, Train_Loss: 0.4111782908439636, Test_Loss: 0.43677499890327454 *\n",
      "356:Epoch: 17, Train_Loss: 0.4138610363006592, Test_Loss: 0.4274769723415375 *\n",
      "357:Epoch: 17, Train_Loss: 0.6324847936630249, Test_Loss: 0.40168899297714233 *\n",
      "358:Epoch: 17, Train_Loss: 0.5352574586868286, Test_Loss: 0.4499194324016571\n",
      "359:Epoch: 17, Train_Loss: 0.5002464056015015, Test_Loss: 0.5197988152503967\n",
      "360:Epoch: 17, Train_Loss: 0.6319475173950195, Test_Loss: 0.48174935579299927 *\n",
      "361:Epoch: 17, Train_Loss: 0.4036747217178345, Test_Loss: 0.40935400128364563 *\n",
      "362:Epoch: 17, Train_Loss: 0.3756580352783203, Test_Loss: 0.43612760305404663\n",
      "363:Epoch: 17, Train_Loss: 0.39168480038642883, Test_Loss: 0.43935585021972656\n",
      "364:Epoch: 17, Train_Loss: 0.41896289587020874, Test_Loss: 0.3618650734424591 *\n",
      "365:Epoch: 17, Train_Loss: 0.3759639263153076, Test_Loss: 0.45518526434898376\n",
      "366:Epoch: 17, Train_Loss: 0.4016491770744324, Test_Loss: 0.4538479447364807 *\n",
      "367:Epoch: 17, Train_Loss: 12.88188362121582, Test_Loss: 0.4060662090778351 *\n",
      "368:Epoch: 17, Train_Loss: 3.0462443828582764, Test_Loss: 0.43669942021369934\n",
      "369:Epoch: 17, Train_Loss: 1.3564810752868652, Test_Loss: 0.3780703842639923 *\n",
      "370:Epoch: 17, Train_Loss: 1.5586931705474854, Test_Loss: 0.36356279253959656 *\n",
      "371:Epoch: 17, Train_Loss: 0.43089786171913147, Test_Loss: 0.34945130348205566 *\n",
      "372:Epoch: 17, Train_Loss: 0.3824016749858856, Test_Loss: 0.41620463132858276\n",
      "373:Epoch: 17, Train_Loss: 1.1220464706420898, Test_Loss: 0.5810719728469849\n",
      "374:Epoch: 17, Train_Loss: 6.027933597564697, Test_Loss: 0.6205511689186096\n",
      "375:Epoch: 17, Train_Loss: 1.1644878387451172, Test_Loss: 0.4579334259033203 *\n",
      "376:Epoch: 17, Train_Loss: 0.3831006586551666, Test_Loss: 0.6648261547088623\n",
      "377:Epoch: 17, Train_Loss: 4.171105861663818, Test_Loss: 0.48567622900009155 *\n",
      "378:Epoch: 17, Train_Loss: 1.8800994157791138, Test_Loss: 0.6044766902923584\n",
      "379:Epoch: 17, Train_Loss: 0.42928969860076904, Test_Loss: 0.6996837854385376\n",
      "380:Epoch: 17, Train_Loss: 0.35045674443244934, Test_Loss: 0.8619254231452942\n",
      "381:Epoch: 17, Train_Loss: 0.4899222254753113, Test_Loss: 0.5797929167747498 *\n",
      "382:Epoch: 17, Train_Loss: 0.44608592987060547, Test_Loss: 0.9712505340576172\n",
      "383:Epoch: 17, Train_Loss: 0.407350093126297, Test_Loss: 1.4738143682479858\n",
      "384:Epoch: 17, Train_Loss: 0.41202306747436523, Test_Loss: 1.3844035863876343 *\n",
      "385:Epoch: 17, Train_Loss: 0.3533347249031067, Test_Loss: 2.8962409496307373\n",
      "386:Epoch: 17, Train_Loss: 0.35431626439094543, Test_Loss: 0.7002365589141846 *\n",
      "387:Epoch: 17, Train_Loss: 0.358633428812027, Test_Loss: 0.4313800036907196 *\n",
      "388:Epoch: 17, Train_Loss: 0.38119277358055115, Test_Loss: 0.3831760585308075 *\n",
      "389:Epoch: 17, Train_Loss: 0.3905419111251831, Test_Loss: 0.3572545349597931 *\n",
      "390:Epoch: 17, Train_Loss: 0.5077718496322632, Test_Loss: 0.3442477881908417 *\n",
      "391:Epoch: 17, Train_Loss: 0.37790948152542114, Test_Loss: 0.34211573004722595 *\n",
      "392:Epoch: 17, Train_Loss: 0.38854554295539856, Test_Loss: 0.40044230222702026\n",
      "393:Epoch: 17, Train_Loss: 0.37929069995880127, Test_Loss: 8.625507354736328\n",
      "394:Epoch: 17, Train_Loss: 0.3439081907272339, Test_Loss: 1.8200814723968506 *\n",
      "395:Epoch: 17, Train_Loss: 0.42805469036102295, Test_Loss: 1.7917060852050781 *\n",
      "396:Epoch: 17, Train_Loss: 0.340228408575058, Test_Loss: 1.4124115705490112 *\n",
      "397:Epoch: 17, Train_Loss: 0.3412567675113678, Test_Loss: 1.2929596900939941 *\n",
      "398:Epoch: 17, Train_Loss: 0.33975109457969666, Test_Loss: 0.6495958566665649 *\n",
      "399:Epoch: 17, Train_Loss: 0.34047818183898926, Test_Loss: 2.2347476482391357\n",
      "400:Epoch: 17, Train_Loss: 0.34034261107444763, Test_Loss: 1.7554489374160767 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 17\n",
      "401:Epoch: 17, Train_Loss: 0.3412182033061981, Test_Loss: 0.5667931437492371 *\n",
      "402:Epoch: 17, Train_Loss: 0.3408253490924835, Test_Loss: 0.7609657049179077\n",
      "403:Epoch: 17, Train_Loss: 0.34265726804733276, Test_Loss: 0.6230626106262207 *\n",
      "404:Epoch: 17, Train_Loss: 0.36231082677841187, Test_Loss: 1.148836374282837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405:Epoch: 17, Train_Loss: 0.37550613284111023, Test_Loss: 1.1933616399765015\n",
      "406:Epoch: 17, Train_Loss: 0.5932034850120544, Test_Loss: 1.453588604927063\n",
      "407:Epoch: 17, Train_Loss: 0.41601642966270447, Test_Loss: 2.0542707443237305\n",
      "408:Epoch: 17, Train_Loss: 0.6898092031478882, Test_Loss: 0.4381835460662842 *\n",
      "409:Epoch: 17, Train_Loss: 6.850621700286865, Test_Loss: 0.3717561364173889 *\n",
      "410:Epoch: 17, Train_Loss: 0.48461097478866577, Test_Loss: 0.4214363694190979\n",
      "411:Epoch: 17, Train_Loss: 0.4239673316478729, Test_Loss: 0.8158735036849976\n",
      "412:Epoch: 17, Train_Loss: 0.527687668800354, Test_Loss: 0.795815110206604 *\n",
      "413:Epoch: 17, Train_Loss: 0.5946425199508667, Test_Loss: 0.573908805847168 *\n",
      "414:Epoch: 17, Train_Loss: 0.45547932386398315, Test_Loss: 0.589280366897583\n",
      "415:Epoch: 17, Train_Loss: 0.5053621530532837, Test_Loss: 0.683478593826294\n",
      "416:Epoch: 17, Train_Loss: 0.4441191256046295, Test_Loss: 0.5428332090377808 *\n",
      "417:Epoch: 17, Train_Loss: 0.4730587303638458, Test_Loss: 0.5295371413230896 *\n",
      "418:Epoch: 17, Train_Loss: 0.4534682035446167, Test_Loss: 0.42512401938438416 *\n",
      "419:Epoch: 17, Train_Loss: 0.4160129427909851, Test_Loss: 0.4007291793823242 *\n",
      "420:Epoch: 17, Train_Loss: 0.33997228741645813, Test_Loss: 0.395133912563324 *\n",
      "421:Epoch: 17, Train_Loss: 0.4072900414466858, Test_Loss: 0.3747000992298126 *\n",
      "422:Epoch: 17, Train_Loss: 0.3994694948196411, Test_Loss: 0.38044217228889465\n",
      "423:Epoch: 17, Train_Loss: 0.6066568493843079, Test_Loss: 0.43398886919021606\n",
      "424:Epoch: 17, Train_Loss: 0.39632758498191833, Test_Loss: 1.2886171340942383\n",
      "425:Epoch: 17, Train_Loss: 0.3800351321697235, Test_Loss: 4.921996116638184\n",
      "426:Epoch: 17, Train_Loss: 0.3587147891521454, Test_Loss: 0.37219923734664917 *\n",
      "427:Epoch: 17, Train_Loss: 0.36717262864112854, Test_Loss: 0.35986098647117615 *\n",
      "428:Epoch: 17, Train_Loss: 0.41880208253860474, Test_Loss: 0.3813234865665436\n",
      "429:Epoch: 17, Train_Loss: 0.376309871673584, Test_Loss: 0.375165730714798 *\n",
      "430:Epoch: 17, Train_Loss: 0.35224708914756775, Test_Loss: 0.38706713914871216\n",
      "431:Epoch: 17, Train_Loss: 0.3420485258102417, Test_Loss: 0.35037288069725037 *\n",
      "432:Epoch: 17, Train_Loss: 0.3419533669948578, Test_Loss: 0.4237667918205261\n",
      "433:Epoch: 17, Train_Loss: 0.4782443344593048, Test_Loss: 0.35864508152008057 *\n",
      "434:Epoch: 17, Train_Loss: 6.209956169128418, Test_Loss: 0.34729278087615967 *\n",
      "435:Epoch: 17, Train_Loss: 0.35293471813201904, Test_Loss: 0.36593544483184814\n",
      "436:Epoch: 17, Train_Loss: 0.3393464982509613, Test_Loss: 0.38016125559806824\n",
      "437:Epoch: 17, Train_Loss: 0.3443315327167511, Test_Loss: 0.34536251425743103 *\n",
      "438:Epoch: 17, Train_Loss: 0.3431417644023895, Test_Loss: 0.3920511305332184\n",
      "439:Epoch: 17, Train_Loss: 0.34054821729660034, Test_Loss: 0.41605469584465027\n",
      "440:Epoch: 17, Train_Loss: 0.3385975658893585, Test_Loss: 0.4024117887020111 *\n",
      "441:Epoch: 17, Train_Loss: 0.3408724367618561, Test_Loss: 0.3918604552745819 *\n",
      "442:Epoch: 17, Train_Loss: 0.3909622132778168, Test_Loss: 0.36897772550582886 *\n",
      "443:Epoch: 17, Train_Loss: 0.36137378215789795, Test_Loss: 0.38243669271469116\n",
      "444:Epoch: 17, Train_Loss: 0.34276267886161804, Test_Loss: 0.34052836894989014 *\n",
      "445:Epoch: 17, Train_Loss: 0.33886584639549255, Test_Loss: 0.3393977880477905 *\n",
      "446:Epoch: 17, Train_Loss: 0.33928829431533813, Test_Loss: 0.3414878249168396\n",
      "447:Epoch: 17, Train_Loss: 0.35161328315734863, Test_Loss: 0.3433317542076111\n",
      "448:Epoch: 17, Train_Loss: 0.3463212549686432, Test_Loss: 0.3419068157672882 *\n",
      "449:Epoch: 17, Train_Loss: 0.346030592918396, Test_Loss: 0.34214332699775696\n",
      "450:Epoch: 17, Train_Loss: 0.3575274348258972, Test_Loss: 0.34176552295684814 *\n",
      "451:Epoch: 17, Train_Loss: 0.37490493059158325, Test_Loss: 0.3398132920265198 *\n",
      "452:Epoch: 17, Train_Loss: 0.355867862701416, Test_Loss: 0.3491227328777313\n",
      "453:Epoch: 17, Train_Loss: 0.3370996117591858, Test_Loss: 0.34506410360336304 *\n",
      "454:Epoch: 17, Train_Loss: 0.3368040919303894, Test_Loss: 0.34465861320495605 *\n",
      "1:Epoch: 18, Train_Loss: 0.4295695424079895, Test_Loss: 0.38088834285736084 *\n",
      "2:Epoch: 18, Train_Loss: 0.4088953137397766, Test_Loss: 0.3496027886867523 *\n",
      "3:Epoch: 18, Train_Loss: 0.4091281294822693, Test_Loss: 0.6473482847213745\n",
      "4:Epoch: 18, Train_Loss: 0.3777124285697937, Test_Loss: 0.7492290139198303\n",
      "5:Epoch: 18, Train_Loss: 0.3733353912830353, Test_Loss: 0.5019741058349609 *\n",
      "6:Epoch: 18, Train_Loss: 0.37740960717201233, Test_Loss: 0.3633517622947693 *\n",
      "7:Epoch: 18, Train_Loss: 0.3937900960445404, Test_Loss: 0.36002081632614136 *\n",
      "8:Epoch: 18, Train_Loss: 0.358720064163208, Test_Loss: 0.3543562889099121 *\n",
      "9:Epoch: 18, Train_Loss: 0.48046746850013733, Test_Loss: 0.426891028881073\n",
      "10:Epoch: 18, Train_Loss: 0.3581300675868988, Test_Loss: 0.7381964325904846\n",
      "11:Epoch: 18, Train_Loss: 0.3521728217601776, Test_Loss: 0.8171046376228333\n",
      "12:Epoch: 18, Train_Loss: 0.34340518712997437, Test_Loss: 0.3902527391910553 *\n",
      "13:Epoch: 18, Train_Loss: 0.34494081139564514, Test_Loss: 0.3935603201389313\n",
      "14:Epoch: 18, Train_Loss: 0.344959557056427, Test_Loss: 0.3405452072620392 *\n",
      "15:Epoch: 18, Train_Loss: 0.33892256021499634, Test_Loss: 0.34919148683547974\n",
      "16:Epoch: 18, Train_Loss: 0.42745450139045715, Test_Loss: 0.34712257981300354 *\n",
      "17:Epoch: 18, Train_Loss: 4.687615394592285, Test_Loss: 0.35557857155799866\n",
      "18:Epoch: 18, Train_Loss: 0.42499712109565735, Test_Loss: 0.39587387442588806\n",
      "19:Epoch: 18, Train_Loss: 0.34290677309036255, Test_Loss: 0.36309000849723816 *\n",
      "20:Epoch: 18, Train_Loss: 0.34137940406799316, Test_Loss: 0.35436394810676575 *\n",
      "21:Epoch: 18, Train_Loss: 0.3410347104072571, Test_Loss: 0.48024338483810425\n",
      "22:Epoch: 18, Train_Loss: 0.3418029248714447, Test_Loss: 0.7225974798202515\n",
      "23:Epoch: 18, Train_Loss: 0.3386804461479187, Test_Loss: 0.6859596967697144 *\n",
      "24:Epoch: 18, Train_Loss: 0.3348415493965149, Test_Loss: 0.4842795729637146 *\n",
      "25:Epoch: 18, Train_Loss: 0.3355601727962494, Test_Loss: 0.4101147949695587 *\n",
      "26:Epoch: 18, Train_Loss: 0.33511024713516235, Test_Loss: 0.409450501203537 *\n",
      "27:Epoch: 18, Train_Loss: 0.3530517816543579, Test_Loss: 0.4131544232368469\n",
      "28:Epoch: 18, Train_Loss: 0.3411535322666168, Test_Loss: 0.45534735918045044\n",
      "29:Epoch: 18, Train_Loss: 0.3908812701702118, Test_Loss: 0.5702466368675232\n",
      "30:Epoch: 18, Train_Loss: 0.3560756742954254, Test_Loss: 5.200855255126953\n",
      "31:Epoch: 18, Train_Loss: 0.3428024351596832, Test_Loss: 0.41857069730758667 *\n",
      "32:Epoch: 18, Train_Loss: 0.45133131742477417, Test_Loss: 0.34397387504577637 *\n",
      "33:Epoch: 18, Train_Loss: 0.5270686149597168, Test_Loss: 0.3377203643321991 *\n",
      "34:Epoch: 18, Train_Loss: 0.5267975330352783, Test_Loss: 0.34203842282295227\n",
      "35:Epoch: 18, Train_Loss: 0.496853768825531, Test_Loss: 0.3409944474697113 *\n",
      "36:Epoch: 18, Train_Loss: 0.33853477239608765, Test_Loss: 0.3369748890399933 *\n",
      "37:Epoch: 18, Train_Loss: 0.33416908979415894, Test_Loss: 0.3445131480693817\n",
      "38:Epoch: 18, Train_Loss: 0.33485889434814453, Test_Loss: 0.3365984857082367 *\n",
      "39:Epoch: 18, Train_Loss: 0.3407200276851654, Test_Loss: 0.33665889501571655\n",
      "40:Epoch: 18, Train_Loss: 0.34314587712287903, Test_Loss: 0.3405108153820038\n",
      "41:Epoch: 18, Train_Loss: 0.34174951910972595, Test_Loss: 0.3549729287624359\n",
      "42:Epoch: 18, Train_Loss: 0.3387013077735901, Test_Loss: 0.34389588236808777 *\n",
      "43:Epoch: 18, Train_Loss: 0.33475303649902344, Test_Loss: 0.34229156374931335 *\n",
      "44:Epoch: 18, Train_Loss: 0.3393627107143402, Test_Loss: 0.3492067754268646\n",
      "45:Epoch: 18, Train_Loss: 0.35144251585006714, Test_Loss: 0.33496952056884766 *\n",
      "46:Epoch: 18, Train_Loss: 0.4840630888938904, Test_Loss: 0.33481886982917786 *\n",
      "47:Epoch: 18, Train_Loss: 0.48710596561431885, Test_Loss: 0.33525776863098145\n",
      "48:Epoch: 18, Train_Loss: 0.48850196599960327, Test_Loss: 0.3459336459636688\n",
      "49:Epoch: 18, Train_Loss: 0.37435784935951233, Test_Loss: 0.3351472020149231 *\n",
      "50:Epoch: 18, Train_Loss: 0.47344914078712463, Test_Loss: 0.3429467976093292\n",
      "51:Epoch: 18, Train_Loss: 0.47270867228507996, Test_Loss: 0.3392784893512726 *\n",
      "52:Epoch: 18, Train_Loss: 0.4182489514350891, Test_Loss: 0.3468761146068573\n",
      "53:Epoch: 18, Train_Loss: 0.4787646532058716, Test_Loss: 0.3470774292945862\n",
      "54:Epoch: 18, Train_Loss: 0.6043387651443481, Test_Loss: 0.3439921438694 *\n",
      "55:Epoch: 18, Train_Loss: 0.40735316276550293, Test_Loss: 0.3382669985294342 *\n",
      "56:Epoch: 18, Train_Loss: 0.3474450707435608, Test_Loss: 0.34020885825157166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57:Epoch: 18, Train_Loss: 2.562448263168335, Test_Loss: 0.3365967273712158 *\n",
      "58:Epoch: 18, Train_Loss: 1.1138309240341187, Test_Loss: 0.33625528216362 *\n",
      "59:Epoch: 18, Train_Loss: 0.3765118420124054, Test_Loss: 0.34619513154029846\n",
      "60:Epoch: 18, Train_Loss: 0.3635306656360626, Test_Loss: 0.40057891607284546\n",
      "61:Epoch: 18, Train_Loss: 0.35696253180503845, Test_Loss: 2.4683382511138916\n",
      "62:Epoch: 18, Train_Loss: 0.35950222611427307, Test_Loss: 3.6490061283111572\n",
      "63:Epoch: 18, Train_Loss: 0.3385190963745117, Test_Loss: 0.34105515480041504 *\n",
      "64:Epoch: 18, Train_Loss: 0.3806781768798828, Test_Loss: 0.33882877230644226 *\n",
      "65:Epoch: 18, Train_Loss: 0.49721086025238037, Test_Loss: 0.38010451197624207\n",
      "66:Epoch: 18, Train_Loss: 0.43293431401252747, Test_Loss: 0.36787161231040955 *\n",
      "67:Epoch: 18, Train_Loss: 0.4169270694255829, Test_Loss: 0.37610453367233276\n",
      "68:Epoch: 18, Train_Loss: 0.37994256615638733, Test_Loss: 0.36831262707710266 *\n",
      "69:Epoch: 18, Train_Loss: 0.36600953340530396, Test_Loss: 0.42422544956207275\n",
      "70:Epoch: 18, Train_Loss: 0.36187782883644104, Test_Loss: 0.33920812606811523 *\n",
      "71:Epoch: 18, Train_Loss: 0.3467651903629303, Test_Loss: 0.35211700201034546\n",
      "72:Epoch: 18, Train_Loss: 0.37795084714889526, Test_Loss: 0.35491541028022766\n",
      "73:Epoch: 18, Train_Loss: 0.35412129759788513, Test_Loss: 0.3976268172264099\n",
      "74:Epoch: 18, Train_Loss: 0.3357175886631012, Test_Loss: 0.3498988747596741 *\n",
      "75:Epoch: 18, Train_Loss: 0.33872705698013306, Test_Loss: 0.41304102540016174\n",
      "76:Epoch: 18, Train_Loss: 0.37734878063201904, Test_Loss: 0.40946030616760254 *\n",
      "77:Epoch: 18, Train_Loss: 0.3625141978263855, Test_Loss: 0.38981279730796814 *\n",
      "78:Epoch: 18, Train_Loss: 0.34058481454849243, Test_Loss: 0.3494749069213867 *\n",
      "79:Epoch: 18, Train_Loss: 0.3312446177005768, Test_Loss: 0.3666441738605499\n",
      "80:Epoch: 18, Train_Loss: 0.3315693140029907, Test_Loss: 0.40671077370643616\n",
      "81:Epoch: 18, Train_Loss: 0.33090704679489136, Test_Loss: 0.3430488109588623 *\n",
      "82:Epoch: 18, Train_Loss: 0.33142879605293274, Test_Loss: 0.3401382267475128 *\n",
      "83:Epoch: 18, Train_Loss: 0.33262017369270325, Test_Loss: 0.34631988406181335\n",
      "84:Epoch: 18, Train_Loss: 0.3323339521884918, Test_Loss: 0.37535107135772705\n",
      "85:Epoch: 18, Train_Loss: 0.3400176465511322, Test_Loss: 0.35683730244636536 *\n",
      "86:Epoch: 18, Train_Loss: 0.33129602670669556, Test_Loss: 0.3470388948917389 *\n",
      "87:Epoch: 18, Train_Loss: 0.3307872414588928, Test_Loss: 0.34651631116867065 *\n",
      "88:Epoch: 18, Train_Loss: 0.337196946144104, Test_Loss: 0.3398735225200653 *\n",
      "89:Epoch: 18, Train_Loss: 0.34427958726882935, Test_Loss: 0.3478778302669525\n",
      "90:Epoch: 18, Train_Loss: 0.34930339455604553, Test_Loss: 0.33480221033096313 *\n",
      "91:Epoch: 18, Train_Loss: 0.3445134162902832, Test_Loss: 0.3507269024848938\n",
      "92:Epoch: 18, Train_Loss: 0.35445693135261536, Test_Loss: 0.39184844493865967\n",
      "93:Epoch: 18, Train_Loss: 0.3445571959018707, Test_Loss: 0.35102078318595886 *\n",
      "94:Epoch: 18, Train_Loss: 0.34572890400886536, Test_Loss: 0.7905329465866089\n",
      "95:Epoch: 18, Train_Loss: 0.33727672696113586, Test_Loss: 0.8107720613479614\n",
      "96:Epoch: 18, Train_Loss: 0.353824257850647, Test_Loss: 0.48767316341400146 *\n",
      "97:Epoch: 18, Train_Loss: 0.36089587211608887, Test_Loss: 0.36263298988342285 *\n",
      "98:Epoch: 18, Train_Loss: 0.33601242303848267, Test_Loss: 0.3551241457462311 *\n",
      "99:Epoch: 18, Train_Loss: 0.33840468525886536, Test_Loss: 0.3498809337615967 *\n",
      "100:Epoch: 18, Train_Loss: 0.33485621213912964, Test_Loss: 0.4876982569694519\n",
      "Model saved at location ../Saver/model.ckpt at epoch 18\n",
      "101:Epoch: 18, Train_Loss: 0.34361153841018677, Test_Loss: 0.7658548951148987\n",
      "102:Epoch: 18, Train_Loss: 0.3827497363090515, Test_Loss: 0.7551796436309814 *\n",
      "103:Epoch: 18, Train_Loss: 0.3809954822063446, Test_Loss: 0.40152299404144287 *\n",
      "104:Epoch: 18, Train_Loss: 0.35367104411125183, Test_Loss: 0.38519376516342163 *\n",
      "105:Epoch: 18, Train_Loss: 0.3294580280780792, Test_Loss: 0.3333967626094818 *\n",
      "106:Epoch: 18, Train_Loss: 0.3911377787590027, Test_Loss: 0.33273279666900635 *\n",
      "107:Epoch: 18, Train_Loss: 0.3444991111755371, Test_Loss: 0.3382832109928131\n",
      "108:Epoch: 18, Train_Loss: 0.33401748538017273, Test_Loss: 0.34426644444465637\n",
      "109:Epoch: 18, Train_Loss: 0.34159836173057556, Test_Loss: 0.3903955817222595\n",
      "110:Epoch: 18, Train_Loss: 0.34926021099090576, Test_Loss: 0.3458220958709717 *\n",
      "111:Epoch: 18, Train_Loss: 0.4355136752128601, Test_Loss: 0.3665604889392853\n",
      "112:Epoch: 18, Train_Loss: 0.3930460810661316, Test_Loss: 0.4638042449951172\n",
      "113:Epoch: 18, Train_Loss: 0.37450289726257324, Test_Loss: 0.6947469711303711\n",
      "114:Epoch: 18, Train_Loss: 0.3448638916015625, Test_Loss: 0.6405119299888611 *\n",
      "115:Epoch: 18, Train_Loss: 0.3374118506908417, Test_Loss: 0.3674573600292206 *\n",
      "116:Epoch: 18, Train_Loss: 0.34645235538482666, Test_Loss: 0.33794504404067993 *\n",
      "117:Epoch: 18, Train_Loss: 0.3311193585395813, Test_Loss: 0.3374720811843872 *\n",
      "118:Epoch: 18, Train_Loss: 0.3369596004486084, Test_Loss: 0.33726197481155396 *\n",
      "119:Epoch: 18, Train_Loss: 0.3491876423358917, Test_Loss: 0.3892432153224945\n",
      "120:Epoch: 18, Train_Loss: 0.35729044675827026, Test_Loss: 1.1784306764602661\n",
      "121:Epoch: 18, Train_Loss: 0.43188825249671936, Test_Loss: 4.863245964050293\n",
      "122:Epoch: 18, Train_Loss: 0.33381038904190063, Test_Loss: 0.35263314843177795 *\n",
      "123:Epoch: 18, Train_Loss: 0.3955150246620178, Test_Loss: 0.3340340256690979 *\n",
      "124:Epoch: 18, Train_Loss: 0.34578824043273926, Test_Loss: 0.33131375908851624 *\n",
      "125:Epoch: 18, Train_Loss: 0.3610125482082367, Test_Loss: 0.3368869125843048\n",
      "126:Epoch: 18, Train_Loss: 0.415330171585083, Test_Loss: 0.3327201008796692 *\n",
      "127:Epoch: 18, Train_Loss: 0.5609966516494751, Test_Loss: 0.33181795477867126 *\n",
      "128:Epoch: 18, Train_Loss: 0.3431345224380493, Test_Loss: 0.33374112844467163\n",
      "129:Epoch: 18, Train_Loss: 0.36576321721076965, Test_Loss: 0.3297557234764099 *\n",
      "130:Epoch: 18, Train_Loss: 0.32842880487442017, Test_Loss: 0.33048850297927856\n",
      "131:Epoch: 18, Train_Loss: 0.32820385694503784, Test_Loss: 0.3314373791217804\n",
      "132:Epoch: 18, Train_Loss: 0.3285472095012665, Test_Loss: 0.34058499336242676\n",
      "133:Epoch: 18, Train_Loss: 0.32792335748672485, Test_Loss: 0.3466196656227112\n",
      "134:Epoch: 18, Train_Loss: 0.3404908776283264, Test_Loss: 0.3398703336715698 *\n",
      "135:Epoch: 18, Train_Loss: 0.3433135747909546, Test_Loss: 0.3401769995689392\n",
      "136:Epoch: 18, Train_Loss: 0.3425959646701813, Test_Loss: 0.3279867172241211 *\n",
      "137:Epoch: 18, Train_Loss: 0.331490159034729, Test_Loss: 0.32831284403800964\n",
      "138:Epoch: 18, Train_Loss: 0.33702170848846436, Test_Loss: 0.32857030630111694\n",
      "139:Epoch: 18, Train_Loss: 0.3361773192882538, Test_Loss: 0.3342745900154114\n",
      "140:Epoch: 18, Train_Loss: 0.3286309242248535, Test_Loss: 0.32802513241767883 *\n",
      "141:Epoch: 18, Train_Loss: 0.3268123269081116, Test_Loss: 0.33124369382858276\n",
      "142:Epoch: 18, Train_Loss: 0.34872379899024963, Test_Loss: 0.3285522162914276 *\n",
      "143:Epoch: 18, Train_Loss: 0.3503221571445465, Test_Loss: 0.3323667347431183\n",
      "144:Epoch: 18, Train_Loss: 0.35894522070884705, Test_Loss: 0.3317818343639374 *\n",
      "145:Epoch: 18, Train_Loss: 0.328009694814682, Test_Loss: 0.330532431602478 *\n",
      "146:Epoch: 18, Train_Loss: 0.3711768090724945, Test_Loss: 0.32825398445129395 *\n",
      "147:Epoch: 18, Train_Loss: 0.37006089091300964, Test_Loss: 0.3287701904773712\n",
      "148:Epoch: 18, Train_Loss: 0.3596171438694, Test_Loss: 0.32800623774528503 *\n",
      "149:Epoch: 18, Train_Loss: 0.33286452293395996, Test_Loss: 0.3276083767414093 *\n",
      "150:Epoch: 18, Train_Loss: 0.3584226071834564, Test_Loss: 0.3501496911048889\n",
      "151:Epoch: 18, Train_Loss: 0.32722294330596924, Test_Loss: 0.37974682450294495\n",
      "152:Epoch: 18, Train_Loss: 0.3452364206314087, Test_Loss: 3.463202953338623\n",
      "153:Epoch: 18, Train_Loss: 0.3368350565433502, Test_Loss: 2.7006473541259766 *\n",
      "154:Epoch: 18, Train_Loss: 0.35061025619506836, Test_Loss: 0.32938385009765625 *\n",
      "155:Epoch: 18, Train_Loss: 1.4051812887191772, Test_Loss: 0.3300272822380066\n",
      "156:Epoch: 18, Train_Loss: 4.391146659851074, Test_Loss: 0.38332152366638184\n",
      "157:Epoch: 18, Train_Loss: 0.7076031565666199, Test_Loss: 0.37711820006370544 *\n",
      "158:Epoch: 18, Train_Loss: 0.34277239441871643, Test_Loss: 0.35887956619262695 *\n",
      "159:Epoch: 18, Train_Loss: 0.3329264223575592, Test_Loss: 0.3843381106853485\n",
      "160:Epoch: 18, Train_Loss: 0.4166637361049652, Test_Loss: 0.4156409204006195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:Epoch: 18, Train_Loss: 0.378384530544281, Test_Loss: 0.32912376523017883 *\n",
      "162:Epoch: 18, Train_Loss: 0.33833152055740356, Test_Loss: 0.3516659438610077\n",
      "163:Epoch: 18, Train_Loss: 0.32653921842575073, Test_Loss: 0.3456485867500305 *\n",
      "164:Epoch: 18, Train_Loss: 0.37453046441078186, Test_Loss: 0.35277533531188965\n",
      "165:Epoch: 18, Train_Loss: 0.3415793776512146, Test_Loss: 0.33257558941841125 *\n",
      "166:Epoch: 18, Train_Loss: 0.34129196405410767, Test_Loss: 0.4208536148071289\n",
      "167:Epoch: 18, Train_Loss: 0.628508448600769, Test_Loss: 0.3872467279434204 *\n",
      "168:Epoch: 18, Train_Loss: 1.301311731338501, Test_Loss: 0.3989577889442444\n",
      "169:Epoch: 18, Train_Loss: 1.1183456182479858, Test_Loss: 0.35095271468162537 *\n",
      "170:Epoch: 18, Train_Loss: 0.36065179109573364, Test_Loss: 0.37081968784332275\n",
      "171:Epoch: 18, Train_Loss: 0.49561238288879395, Test_Loss: 0.3971695303916931\n",
      "172:Epoch: 18, Train_Loss: 2.235241174697876, Test_Loss: 0.33084309101104736 *\n",
      "173:Epoch: 18, Train_Loss: 1.175049901008606, Test_Loss: 0.3464164733886719\n",
      "174:Epoch: 18, Train_Loss: 0.36008480191230774, Test_Loss: 0.3799020051956177\n",
      "175:Epoch: 18, Train_Loss: 0.3501838147640228, Test_Loss: 0.43433600664138794\n",
      "176:Epoch: 18, Train_Loss: 0.7493992447853088, Test_Loss: 0.3982366621494293 *\n",
      "177:Epoch: 18, Train_Loss: 1.0393704175949097, Test_Loss: 0.3289746344089508 *\n",
      "178:Epoch: 18, Train_Loss: 0.8361399173736572, Test_Loss: 0.329721063375473\n",
      "179:Epoch: 18, Train_Loss: 0.3285830020904541, Test_Loss: 0.3304988145828247\n",
      "180:Epoch: 18, Train_Loss: 0.34062814712524414, Test_Loss: 0.3335956633090973\n",
      "181:Epoch: 18, Train_Loss: 0.6304830312728882, Test_Loss: 0.35346725583076477\n",
      "182:Epoch: 18, Train_Loss: 0.6350640654563904, Test_Loss: 0.3280322849750519 *\n",
      "183:Epoch: 18, Train_Loss: 0.33993640542030334, Test_Loss: 0.35146355628967285\n",
      "184:Epoch: 18, Train_Loss: 0.354828417301178, Test_Loss: 0.41926202178001404\n",
      "185:Epoch: 18, Train_Loss: 0.39176246523857117, Test_Loss: 0.6108611822128296\n",
      "186:Epoch: 18, Train_Loss: 0.41435956954956055, Test_Loss: 0.6811385154724121\n",
      "187:Epoch: 18, Train_Loss: 0.491627037525177, Test_Loss: 0.4406982660293579 *\n",
      "188:Epoch: 18, Train_Loss: 0.36563605070114136, Test_Loss: 0.34797242283821106 *\n",
      "189:Epoch: 18, Train_Loss: 0.3578243851661682, Test_Loss: 0.3583664298057556\n",
      "190:Epoch: 18, Train_Loss: 0.416396826505661, Test_Loss: 0.3423209488391876 *\n",
      "191:Epoch: 18, Train_Loss: 0.4745715260505676, Test_Loss: 0.4592752754688263\n",
      "192:Epoch: 18, Train_Loss: 0.4746018946170807, Test_Loss: 0.45112138986587524 *\n",
      "193:Epoch: 18, Train_Loss: 0.5710881948471069, Test_Loss: 0.6346800923347473\n",
      "194:Epoch: 18, Train_Loss: 0.5474597811698914, Test_Loss: 0.4445668160915375 *\n",
      "195:Epoch: 18, Train_Loss: 0.36218586564064026, Test_Loss: 0.37185966968536377 *\n",
      "196:Epoch: 18, Train_Loss: 0.4010704457759857, Test_Loss: 0.36588752269744873 *\n",
      "197:Epoch: 18, Train_Loss: 0.37361928820610046, Test_Loss: 0.3671645224094391\n",
      "198:Epoch: 18, Train_Loss: 0.3593006730079651, Test_Loss: 0.41641947627067566\n",
      "199:Epoch: 18, Train_Loss: 0.3281135559082031, Test_Loss: 0.4248456060886383\n",
      "200:Epoch: 18, Train_Loss: 0.3252377212047577, Test_Loss: 0.38713526725769043 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 18\n",
      "201:Epoch: 18, Train_Loss: 0.3262196481227875, Test_Loss: 0.5016565322875977\n",
      "202:Epoch: 18, Train_Loss: 0.3352428078651428, Test_Loss: 0.6230885982513428\n",
      "203:Epoch: 18, Train_Loss: 0.34150180220603943, Test_Loss: 0.5318994522094727 *\n",
      "204:Epoch: 18, Train_Loss: 0.33862701058387756, Test_Loss: 0.7988985776901245\n",
      "205:Epoch: 18, Train_Loss: 0.35429805517196655, Test_Loss: 1.3948354721069336\n",
      "206:Epoch: 18, Train_Loss: 0.3770309388637543, Test_Loss: 0.6687837839126587 *\n",
      "207:Epoch: 18, Train_Loss: 0.46878674626350403, Test_Loss: 0.5863715410232544 *\n",
      "208:Epoch: 18, Train_Loss: 0.4902658462524414, Test_Loss: 0.5766216516494751 *\n",
      "209:Epoch: 18, Train_Loss: 0.37272605299949646, Test_Loss: 0.5578298568725586 *\n",
      "210:Epoch: 18, Train_Loss: 0.35803458094596863, Test_Loss: 0.8515965938568115\n",
      "211:Epoch: 18, Train_Loss: 0.4204564094543457, Test_Loss: 2.098158121109009\n",
      "212:Epoch: 18, Train_Loss: 0.3948390781879425, Test_Loss: 3.6237704753875732\n",
      "213:Epoch: 18, Train_Loss: 0.36040520668029785, Test_Loss: 0.36890527606010437 *\n",
      "214:Epoch: 18, Train_Loss: 0.3773525655269623, Test_Loss: 0.3475637435913086 *\n",
      "215:Epoch: 18, Train_Loss: 0.5669334530830383, Test_Loss: 0.3616819381713867\n",
      "216:Epoch: 18, Train_Loss: 0.44594642519950867, Test_Loss: 0.3320845067501068 *\n",
      "217:Epoch: 18, Train_Loss: 0.4451892375946045, Test_Loss: 0.3640390932559967\n",
      "218:Epoch: 18, Train_Loss: 0.34055575728416443, Test_Loss: 0.3981174826622009\n",
      "219:Epoch: 18, Train_Loss: 0.3683997392654419, Test_Loss: 0.3778219223022461 *\n",
      "220:Epoch: 18, Train_Loss: 0.6127599477767944, Test_Loss: 0.3409373462200165 *\n",
      "221:Epoch: 18, Train_Loss: 0.8711367845535278, Test_Loss: 0.352305144071579\n",
      "222:Epoch: 18, Train_Loss: 0.46187835931777954, Test_Loss: 0.37268829345703125\n",
      "223:Epoch: 18, Train_Loss: 0.4006933867931366, Test_Loss: 0.4344499409198761\n",
      "224:Epoch: 18, Train_Loss: 0.3833470344543457, Test_Loss: 0.3444966971874237 *\n",
      "225:Epoch: 18, Train_Loss: 0.3301214575767517, Test_Loss: 0.3622715473175049\n",
      "226:Epoch: 18, Train_Loss: 0.5598085522651672, Test_Loss: 0.38164976239204407\n",
      "227:Epoch: 18, Train_Loss: 0.386138379573822, Test_Loss: 0.3425922691822052 *\n",
      "228:Epoch: 18, Train_Loss: 0.3621773421764374, Test_Loss: 0.34348970651626587\n",
      "229:Epoch: 18, Train_Loss: 0.5720599889755249, Test_Loss: 0.39925023913383484\n",
      "230:Epoch: 18, Train_Loss: 0.35159918665885925, Test_Loss: 0.45186129212379456\n",
      "231:Epoch: 18, Train_Loss: 0.3312363624572754, Test_Loss: 0.3493453860282898 *\n",
      "232:Epoch: 18, Train_Loss: 0.351022332906723, Test_Loss: 0.3730944097042084\n",
      "233:Epoch: 18, Train_Loss: 0.43574583530426025, Test_Loss: 0.3680447041988373 *\n",
      "234:Epoch: 18, Train_Loss: 0.36735114455223083, Test_Loss: 0.44711506366729736\n",
      "235:Epoch: 18, Train_Loss: 0.4725956320762634, Test_Loss: 0.44684481620788574 *\n",
      "236:Epoch: 18, Train_Loss: 0.33646732568740845, Test_Loss: 0.4066440165042877 *\n",
      "237:Epoch: 18, Train_Loss: 0.5695958733558655, Test_Loss: 0.34438467025756836 *\n",
      "238:Epoch: 18, Train_Loss: 0.38193708658218384, Test_Loss: 0.3753891587257385\n",
      "239:Epoch: 18, Train_Loss: 0.35397782921791077, Test_Loss: 0.37673714756965637\n",
      "240:Epoch: 18, Train_Loss: 0.337753564119339, Test_Loss: 0.3484693467617035 *\n",
      "241:Epoch: 18, Train_Loss: 0.3738580644130707, Test_Loss: 0.4426272213459015\n",
      "242:Epoch: 18, Train_Loss: 0.5928727984428406, Test_Loss: 0.4248000979423523 *\n",
      "243:Epoch: 18, Train_Loss: 0.544731855392456, Test_Loss: 5.232082843780518\n",
      "244:Epoch: 18, Train_Loss: 0.6363034248352051, Test_Loss: 1.4513311386108398 *\n",
      "245:Epoch: 18, Train_Loss: 0.8133963942527771, Test_Loss: 0.39045441150665283 *\n",
      "246:Epoch: 18, Train_Loss: 0.5644536018371582, Test_Loss: 0.40734246373176575\n",
      "247:Epoch: 18, Train_Loss: 0.4970839023590088, Test_Loss: 0.35247549414634705 *\n",
      "248:Epoch: 18, Train_Loss: 0.3944973349571228, Test_Loss: 0.3264021575450897 *\n",
      "249:Epoch: 18, Train_Loss: 0.3337075710296631, Test_Loss: 0.33898481726646423\n",
      "250:Epoch: 18, Train_Loss: 0.33406227827072144, Test_Loss: 0.388141930103302\n",
      "251:Epoch: 18, Train_Loss: 0.34537795186042786, Test_Loss: 0.361601859331131 *\n",
      "252:Epoch: 18, Train_Loss: 0.5199776887893677, Test_Loss: 0.3344215750694275 *\n",
      "253:Epoch: 18, Train_Loss: 0.6867740154266357, Test_Loss: 0.36175376176834106\n",
      "254:Epoch: 18, Train_Loss: 0.6178516745567322, Test_Loss: 0.3925199806690216\n",
      "255:Epoch: 18, Train_Loss: 1.623824119567871, Test_Loss: 0.4454597234725952\n",
      "256:Epoch: 18, Train_Loss: 0.849963903427124, Test_Loss: 0.354859858751297 *\n",
      "257:Epoch: 18, Train_Loss: 0.6253838539123535, Test_Loss: 0.3280505836009979 *\n",
      "258:Epoch: 18, Train_Loss: 0.3631203770637512, Test_Loss: 0.3795710802078247\n",
      "259:Epoch: 18, Train_Loss: 0.3258254826068878, Test_Loss: 0.3639891743659973 *\n",
      "260:Epoch: 18, Train_Loss: 0.5355232954025269, Test_Loss: 0.3667019307613373\n",
      "261:Epoch: 18, Train_Loss: 1.0445829629898071, Test_Loss: 0.47243615984916687\n",
      "262:Epoch: 18, Train_Loss: 0.8608837723731995, Test_Loss: 0.39992186427116394 *\n",
      "263:Epoch: 18, Train_Loss: 0.3692108690738678, Test_Loss: 0.4121231138706207\n",
      "264:Epoch: 18, Train_Loss: 0.3903268873691559, Test_Loss: 0.39820924401283264 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265:Epoch: 18, Train_Loss: 0.3893733620643616, Test_Loss: 0.4134426712989807\n",
      "266:Epoch: 18, Train_Loss: 0.6272575855255127, Test_Loss: 0.37462300062179565 *\n",
      "267:Epoch: 18, Train_Loss: 0.5129042267799377, Test_Loss: 0.40051016211509705\n",
      "268:Epoch: 18, Train_Loss: 0.43306106328964233, Test_Loss: 0.4487863779067993\n",
      "269:Epoch: 18, Train_Loss: 0.470916211605072, Test_Loss: 0.3934790790081024 *\n",
      "270:Epoch: 18, Train_Loss: 0.373651385307312, Test_Loss: 0.392471045255661 *\n",
      "271:Epoch: 18, Train_Loss: 0.3628535866737366, Test_Loss: 0.3597204387187958 *\n",
      "272:Epoch: 18, Train_Loss: 0.3818003237247467, Test_Loss: 0.40741318464279175\n",
      "273:Epoch: 18, Train_Loss: 0.35173311829566956, Test_Loss: 0.3476579189300537 *\n",
      "274:Epoch: 18, Train_Loss: 0.396654337644577, Test_Loss: 0.36710140109062195\n",
      "275:Epoch: 18, Train_Loss: 0.37830865383148193, Test_Loss: 0.4515554904937744\n",
      "276:Epoch: 18, Train_Loss: 1.511399745941162, Test_Loss: 0.45636510848999023\n",
      "277:Epoch: 18, Train_Loss: 14.705277442932129, Test_Loss: 0.503430187702179\n",
      "278:Epoch: 18, Train_Loss: 0.7324893474578857, Test_Loss: 0.3801754117012024 *\n",
      "279:Epoch: 18, Train_Loss: 2.0130562782287598, Test_Loss: 0.32732054591178894 *\n",
      "280:Epoch: 18, Train_Loss: 0.9910248517990112, Test_Loss: 0.3311766982078552\n",
      "281:Epoch: 18, Train_Loss: 0.35821980237960815, Test_Loss: 0.37524640560150146\n",
      "282:Epoch: 18, Train_Loss: 0.41845226287841797, Test_Loss: 0.5676138997077942\n",
      "283:Epoch: 18, Train_Loss: 5.911252975463867, Test_Loss: 0.49938246607780457 *\n",
      "284:Epoch: 18, Train_Loss: 2.9060888290405273, Test_Loss: 0.6199100017547607\n",
      "285:Epoch: 18, Train_Loss: 0.36700278520584106, Test_Loss: 0.40123212337493896 *\n",
      "286:Epoch: 18, Train_Loss: 1.6927788257598877, Test_Loss: 0.35943904519081116 *\n",
      "287:Epoch: 18, Train_Loss: 4.915671348571777, Test_Loss: 0.43653416633605957\n",
      "288:Epoch: 18, Train_Loss: 0.41947808861732483, Test_Loss: 0.4804140329360962\n",
      "289:Epoch: 18, Train_Loss: 0.39406731724739075, Test_Loss: 0.4765625298023224 *\n",
      "290:Epoch: 18, Train_Loss: 0.5936050415039062, Test_Loss: 0.3712969422340393 *\n",
      "291:Epoch: 18, Train_Loss: 0.49154889583587646, Test_Loss: 0.42854487895965576\n",
      "292:Epoch: 18, Train_Loss: 0.32910823822021484, Test_Loss: 0.3391549289226532 *\n",
      "293:Epoch: 18, Train_Loss: 0.33177313208580017, Test_Loss: 0.4144466519355774\n",
      "294:Epoch: 18, Train_Loss: 0.31816500425338745, Test_Loss: 0.7245749235153198\n",
      "295:Epoch: 18, Train_Loss: 0.31835252046585083, Test_Loss: 0.42768603563308716 *\n",
      "296:Epoch: 18, Train_Loss: 0.3209819197654724, Test_Loss: 0.5698944330215454\n",
      "297:Epoch: 18, Train_Loss: 0.32475247979164124, Test_Loss: 0.3280518651008606 *\n",
      "298:Epoch: 18, Train_Loss: 0.34394511580467224, Test_Loss: 0.32858285307884216\n",
      "299:Epoch: 18, Train_Loss: 0.37048885226249695, Test_Loss: 0.3273836672306061 *\n",
      "300:Epoch: 18, Train_Loss: 0.3558226525783539, Test_Loss: 0.3285790681838989\n",
      "Model saved at location ../Saver/model.ckpt at epoch 18\n",
      "301:Epoch: 18, Train_Loss: 0.37426072359085083, Test_Loss: 0.35432907938957214\n",
      "302:Epoch: 18, Train_Loss: 0.32820796966552734, Test_Loss: 4.47022008895874\n",
      "303:Epoch: 18, Train_Loss: 0.3222719430923462, Test_Loss: 2.7616372108459473 *\n",
      "304:Epoch: 18, Train_Loss: 0.3305019438266754, Test_Loss: 0.41845476627349854 *\n",
      "305:Epoch: 18, Train_Loss: 0.31789663434028625, Test_Loss: 0.33629709482192993 *\n",
      "306:Epoch: 18, Train_Loss: 0.31796446442604065, Test_Loss: 0.3566342890262604\n",
      "307:Epoch: 18, Train_Loss: 0.3173941373825073, Test_Loss: 0.32182610034942627 *\n",
      "308:Epoch: 18, Train_Loss: 0.31822097301483154, Test_Loss: 0.34480956196784973\n",
      "309:Epoch: 18, Train_Loss: 0.3192864656448364, Test_Loss: 0.392408549785614\n",
      "310:Epoch: 18, Train_Loss: 0.31709298491477966, Test_Loss: 0.3292626738548279 *\n",
      "311:Epoch: 18, Train_Loss: 0.31699368357658386, Test_Loss: 0.32948410511016846\n",
      "312:Epoch: 18, Train_Loss: 0.3170868158340454, Test_Loss: 0.32649320363998413 *\n",
      "313:Epoch: 18, Train_Loss: 0.3217150568962097, Test_Loss: 0.3243921995162964 *\n",
      "314:Epoch: 18, Train_Loss: 0.3470170795917511, Test_Loss: 0.34575197100639343\n",
      "315:Epoch: 18, Train_Loss: 0.35622212290763855, Test_Loss: 0.3339041471481323 *\n",
      "316:Epoch: 18, Train_Loss: 0.34453535079956055, Test_Loss: 0.32586896419525146 *\n",
      "317:Epoch: 18, Train_Loss: 0.41677314043045044, Test_Loss: 0.33294031023979187\n",
      "318:Epoch: 18, Train_Loss: 6.6494646072387695, Test_Loss: 0.3213130831718445 *\n",
      "319:Epoch: 18, Train_Loss: 2.158834457397461, Test_Loss: 0.3321467638015747\n",
      "320:Epoch: 18, Train_Loss: 0.3306608200073242, Test_Loss: 0.4138498306274414\n",
      "321:Epoch: 18, Train_Loss: 0.3420991599559784, Test_Loss: 0.5236551761627197\n",
      "322:Epoch: 18, Train_Loss: 0.41238290071487427, Test_Loss: 0.4867275357246399 *\n",
      "323:Epoch: 18, Train_Loss: 0.35310810804367065, Test_Loss: 0.47530800104141235 *\n",
      "324:Epoch: 18, Train_Loss: 0.3956921100616455, Test_Loss: 0.5645554065704346\n",
      "325:Epoch: 18, Train_Loss: 0.3685927391052246, Test_Loss: 0.547461211681366 *\n",
      "326:Epoch: 18, Train_Loss: 0.47067874670028687, Test_Loss: 0.5495254397392273\n",
      "327:Epoch: 18, Train_Loss: 0.5479856133460999, Test_Loss: 0.4550217390060425 *\n",
      "328:Epoch: 18, Train_Loss: 0.48744553327560425, Test_Loss: 0.36655643582344055 *\n",
      "329:Epoch: 18, Train_Loss: 0.35169553756713867, Test_Loss: 0.4150526523590088\n",
      "330:Epoch: 18, Train_Loss: 0.39736056327819824, Test_Loss: 0.3815750777721405 *\n",
      "331:Epoch: 18, Train_Loss: 0.4857255220413208, Test_Loss: 0.3690175414085388 *\n",
      "332:Epoch: 18, Train_Loss: 0.5418441891670227, Test_Loss: 0.5156837105751038\n",
      "333:Epoch: 18, Train_Loss: 0.4815254807472229, Test_Loss: 0.3986872136592865 *\n",
      "334:Epoch: 18, Train_Loss: 0.42343083024024963, Test_Loss: 5.921168327331543\n",
      "335:Epoch: 18, Train_Loss: 0.37090903520584106, Test_Loss: 0.537891685962677 *\n",
      "336:Epoch: 18, Train_Loss: 0.32647669315338135, Test_Loss: 0.31972000002861023 *\n",
      "337:Epoch: 18, Train_Loss: 0.3746452331542969, Test_Loss: 0.3432047367095947\n",
      "338:Epoch: 18, Train_Loss: 0.3533148169517517, Test_Loss: 0.35901719331741333\n",
      "339:Epoch: 18, Train_Loss: 0.32363161444664, Test_Loss: 0.35684776306152344 *\n",
      "340:Epoch: 18, Train_Loss: 0.3170764148235321, Test_Loss: 0.32252389192581177 *\n",
      "341:Epoch: 18, Train_Loss: 0.31687304377555847, Test_Loss: 0.41205212473869324\n",
      "342:Epoch: 18, Train_Loss: 0.3537680506706238, Test_Loss: 0.3677521347999573 *\n",
      "343:Epoch: 18, Train_Loss: 5.857155799865723, Test_Loss: 0.31827008724212646 *\n",
      "344:Epoch: 18, Train_Loss: 0.6752643585205078, Test_Loss: 0.3534676432609558\n",
      "345:Epoch: 18, Train_Loss: 0.32321277260780334, Test_Loss: 0.33531442284584045 *\n",
      "346:Epoch: 18, Train_Loss: 0.3247138559818268, Test_Loss: 0.3214760720729828 *\n",
      "347:Epoch: 18, Train_Loss: 0.32607501745224, Test_Loss: 0.33659839630126953\n",
      "348:Epoch: 18, Train_Loss: 0.32288530468940735, Test_Loss: 0.38449788093566895\n",
      "349:Epoch: 18, Train_Loss: 0.31802791357040405, Test_Loss: 0.3524532914161682 *\n",
      "350:Epoch: 18, Train_Loss: 0.3184969127178192, Test_Loss: 0.420696496963501\n",
      "351:Epoch: 18, Train_Loss: 0.3634152412414551, Test_Loss: 0.3710138499736786 *\n",
      "352:Epoch: 18, Train_Loss: 0.3598794639110565, Test_Loss: 0.3575930595397949 *\n",
      "353:Epoch: 18, Train_Loss: 0.3206612765789032, Test_Loss: 0.32691490650177 *\n",
      "354:Epoch: 18, Train_Loss: 0.31824108958244324, Test_Loss: 0.31833621859550476 *\n",
      "355:Epoch: 18, Train_Loss: 0.31718990206718445, Test_Loss: 0.3221369683742523\n",
      "356:Epoch: 18, Train_Loss: 0.3298740088939667, Test_Loss: 0.323026567697525\n",
      "357:Epoch: 18, Train_Loss: 0.3204878568649292, Test_Loss: 0.32429414987564087\n",
      "358:Epoch: 18, Train_Loss: 0.3221080005168915, Test_Loss: 0.3254227936267853\n",
      "359:Epoch: 18, Train_Loss: 0.33140066266059875, Test_Loss: 0.31715354323387146 *\n",
      "360:Epoch: 18, Train_Loss: 0.36040928959846497, Test_Loss: 0.3262460231781006\n",
      "361:Epoch: 18, Train_Loss: 0.3486558794975281, Test_Loss: 0.3278479278087616\n",
      "362:Epoch: 18, Train_Loss: 0.3150339126586914, Test_Loss: 0.3224997818470001 *\n",
      "363:Epoch: 18, Train_Loss: 0.3159673810005188, Test_Loss: 0.3263407051563263\n",
      "364:Epoch: 18, Train_Loss: 0.4145090878009796, Test_Loss: 0.37205827236175537\n",
      "365:Epoch: 18, Train_Loss: 0.44255393743515015, Test_Loss: 0.3380868434906006 *\n",
      "366:Epoch: 18, Train_Loss: 0.4132893681526184, Test_Loss: 0.5593842267990112\n",
      "367:Epoch: 18, Train_Loss: 0.40689218044281006, Test_Loss: 0.7777373194694519\n",
      "368:Epoch: 18, Train_Loss: 0.3448489010334015, Test_Loss: 0.5473468899726868 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369:Epoch: 18, Train_Loss: 0.3946964740753174, Test_Loss: 0.38590848445892334 *\n",
      "370:Epoch: 18, Train_Loss: 0.37201327085494995, Test_Loss: 0.3476504981517792 *\n",
      "371:Epoch: 18, Train_Loss: 0.3582303524017334, Test_Loss: 0.32966119050979614 *\n",
      "372:Epoch: 18, Train_Loss: 0.41050249338150024, Test_Loss: 0.4374772608280182\n",
      "373:Epoch: 18, Train_Loss: 0.3569524884223938, Test_Loss: 0.9461041688919067\n",
      "374:Epoch: 18, Train_Loss: 0.32885488867759705, Test_Loss: 1.1747015714645386\n",
      "375:Epoch: 18, Train_Loss: 0.3179973065853119, Test_Loss: 0.5018986463546753 *\n",
      "376:Epoch: 18, Train_Loss: 0.31695717573165894, Test_Loss: 0.4022257328033447 *\n",
      "377:Epoch: 18, Train_Loss: 0.31646740436553955, Test_Loss: 0.3168489634990692 *\n",
      "378:Epoch: 18, Train_Loss: 0.3167293667793274, Test_Loss: 0.32549363374710083\n",
      "379:Epoch: 18, Train_Loss: 0.31605324149131775, Test_Loss: 0.3222103714942932 *\n",
      "380:Epoch: 18, Train_Loss: 4.653537750244141, Test_Loss: 0.3407735228538513\n",
      "381:Epoch: 18, Train_Loss: 0.9454530477523804, Test_Loss: 0.35465872287750244\n",
      "382:Epoch: 18, Train_Loss: 0.31760522723197937, Test_Loss: 0.3604852855205536\n",
      "383:Epoch: 18, Train_Loss: 0.33829471468925476, Test_Loss: 0.32157719135284424 *\n",
      "384:Epoch: 18, Train_Loss: 0.31977957487106323, Test_Loss: 0.42610499262809753\n",
      "385:Epoch: 18, Train_Loss: 0.31328627467155457, Test_Loss: 0.6982464790344238\n",
      "386:Epoch: 18, Train_Loss: 0.3145751953125, Test_Loss: 0.40620946884155273 *\n",
      "387:Epoch: 18, Train_Loss: 0.31378230452537537, Test_Loss: 0.5222268104553223\n",
      "388:Epoch: 18, Train_Loss: 0.31342440843582153, Test_Loss: 0.33503952622413635 *\n",
      "389:Epoch: 18, Train_Loss: 0.3131466209888458, Test_Loss: 0.3372917175292969\n",
      "390:Epoch: 18, Train_Loss: 0.3627653121948242, Test_Loss: 0.3367801308631897 *\n",
      "391:Epoch: 18, Train_Loss: 0.41577383875846863, Test_Loss: 0.3414821922779083\n",
      "392:Epoch: 18, Train_Loss: 0.419941782951355, Test_Loss: 0.35550457239151\n",
      "393:Epoch: 18, Train_Loss: 0.41155922412872314, Test_Loss: 5.081746578216553\n",
      "394:Epoch: 18, Train_Loss: 0.3163864314556122, Test_Loss: 1.061586856842041 *\n",
      "395:Epoch: 18, Train_Loss: 0.36284011602401733, Test_Loss: 0.32412150502204895 *\n",
      "396:Epoch: 18, Train_Loss: 0.5207873582839966, Test_Loss: 0.3163014054298401 *\n",
      "397:Epoch: 18, Train_Loss: 0.5145442485809326, Test_Loss: 0.318200945854187\n",
      "398:Epoch: 18, Train_Loss: 0.5301085710525513, Test_Loss: 0.3241380751132965\n",
      "399:Epoch: 18, Train_Loss: 0.3278690278530121, Test_Loss: 0.31703948974609375 *\n",
      "400:Epoch: 18, Train_Loss: 0.31218910217285156, Test_Loss: 0.31652793288230896 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 18\n",
      "401:Epoch: 18, Train_Loss: 0.3129548132419586, Test_Loss: 0.31398507952690125 *\n",
      "402:Epoch: 18, Train_Loss: 0.31883084774017334, Test_Loss: 0.3144444525241852\n",
      "403:Epoch: 18, Train_Loss: 0.3226109743118286, Test_Loss: 0.3159152567386627\n",
      "404:Epoch: 18, Train_Loss: 0.3210531771183014, Test_Loss: 0.31762444972991943\n",
      "405:Epoch: 18, Train_Loss: 0.31756162643432617, Test_Loss: 0.32324281334877014\n",
      "406:Epoch: 18, Train_Loss: 0.3129443824291229, Test_Loss: 0.3494327664375305\n",
      "407:Epoch: 18, Train_Loss: 0.3139241337776184, Test_Loss: 0.333126962184906 *\n",
      "408:Epoch: 18, Train_Loss: 0.3222717344760895, Test_Loss: 0.313961923122406 *\n",
      "409:Epoch: 18, Train_Loss: 0.4420623779296875, Test_Loss: 0.31251010298728943 *\n",
      "410:Epoch: 18, Train_Loss: 0.4950212240219116, Test_Loss: 0.31496530771255493\n",
      "411:Epoch: 18, Train_Loss: 0.4954395890235901, Test_Loss: 0.3141944706439972 *\n",
      "412:Epoch: 18, Train_Loss: 0.37478208541870117, Test_Loss: 0.3124704957008362 *\n",
      "413:Epoch: 18, Train_Loss: 0.45586761832237244, Test_Loss: 0.3157947063446045\n",
      "414:Epoch: 18, Train_Loss: 0.46165430545806885, Test_Loss: 0.31224972009658813 *\n",
      "415:Epoch: 18, Train_Loss: 0.3293339014053345, Test_Loss: 0.31388863921165466\n",
      "416:Epoch: 18, Train_Loss: 0.47209006547927856, Test_Loss: 0.3156524896621704\n",
      "417:Epoch: 18, Train_Loss: 0.43190470337867737, Test_Loss: 0.3152393400669098 *\n",
      "418:Epoch: 18, Train_Loss: 0.5497942566871643, Test_Loss: 0.3134092688560486 *\n",
      "419:Epoch: 18, Train_Loss: 0.32729512453079224, Test_Loss: 0.31301748752593994 *\n",
      "420:Epoch: 18, Train_Loss: 1.5652048587799072, Test_Loss: 0.31662213802337646\n",
      "421:Epoch: 18, Train_Loss: 2.273404598236084, Test_Loss: 0.31293928623199463 *\n",
      "422:Epoch: 18, Train_Loss: 0.3512210249900818, Test_Loss: 0.31529325246810913\n",
      "423:Epoch: 18, Train_Loss: 0.3692135214805603, Test_Loss: 0.37190139293670654\n",
      "424:Epoch: 18, Train_Loss: 0.3681803047657013, Test_Loss: 0.6766514182090759\n",
      "425:Epoch: 18, Train_Loss: 0.36227527260780334, Test_Loss: 5.406137466430664\n",
      "426:Epoch: 18, Train_Loss: 0.3117918372154236, Test_Loss: 0.31935280561447144 *\n",
      "427:Epoch: 18, Train_Loss: 0.32219406962394714, Test_Loss: 0.3132246434688568 *\n",
      "428:Epoch: 18, Train_Loss: 0.4396446943283081, Test_Loss: 0.35162773728370667\n",
      "429:Epoch: 18, Train_Loss: 0.409940242767334, Test_Loss: 0.3737754225730896\n",
      "430:Epoch: 18, Train_Loss: 0.4137863218784332, Test_Loss: 0.3727087080478668 *\n",
      "431:Epoch: 18, Train_Loss: 0.3915722370147705, Test_Loss: 0.31555265188217163 *\n",
      "432:Epoch: 18, Train_Loss: 0.3701692223548889, Test_Loss: 0.41694581508636475\n",
      "433:Epoch: 18, Train_Loss: 0.33709433674812317, Test_Loss: 0.34151554107666016 *\n",
      "434:Epoch: 18, Train_Loss: 0.33386746048927307, Test_Loss: 0.31528326869010925 *\n",
      "435:Epoch: 18, Train_Loss: 0.3373338282108307, Test_Loss: 0.3418867886066437\n",
      "436:Epoch: 18, Train_Loss: 0.34467631578445435, Test_Loss: 0.33893707394599915 *\n",
      "437:Epoch: 18, Train_Loss: 0.3209141194820404, Test_Loss: 0.31364211440086365 *\n",
      "438:Epoch: 18, Train_Loss: 0.310713529586792, Test_Loss: 0.36844536662101746\n",
      "439:Epoch: 18, Train_Loss: 0.3555850088596344, Test_Loss: 0.4289058446884155\n",
      "440:Epoch: 18, Train_Loss: 0.36379873752593994, Test_Loss: 0.357786625623703 *\n",
      "441:Epoch: 18, Train_Loss: 0.3278653919696808, Test_Loss: 0.38265594840049744\n",
      "442:Epoch: 18, Train_Loss: 0.312534362077713, Test_Loss: 0.34354230761528015 *\n",
      "443:Epoch: 18, Train_Loss: 0.3108345568180084, Test_Loss: 0.3737115263938904\n",
      "444:Epoch: 18, Train_Loss: 0.3100828230381012, Test_Loss: 0.33391132950782776 *\n",
      "445:Epoch: 18, Train_Loss: 0.31058353185653687, Test_Loss: 0.3268846273422241 *\n",
      "446:Epoch: 18, Train_Loss: 0.3109048008918762, Test_Loss: 0.3331657648086548\n",
      "447:Epoch: 18, Train_Loss: 0.31222692131996155, Test_Loss: 0.3397800326347351\n",
      "448:Epoch: 18, Train_Loss: 0.3188883364200592, Test_Loss: 0.33542531728744507 *\n",
      "449:Epoch: 18, Train_Loss: 0.31022876501083374, Test_Loss: 0.33752375841140747\n",
      "450:Epoch: 18, Train_Loss: 0.3096773624420166, Test_Loss: 0.33126264810562134 *\n",
      "451:Epoch: 18, Train_Loss: 0.3130597174167633, Test_Loss: 0.3323206305503845\n",
      "452:Epoch: 18, Train_Loss: 0.32659363746643066, Test_Loss: 0.3406989574432373\n",
      "453:Epoch: 18, Train_Loss: 0.32910647988319397, Test_Loss: 0.31493479013442993 *\n",
      "454:Epoch: 18, Train_Loss: 0.32838714122772217, Test_Loss: 0.33095547556877136\n",
      "1:Epoch: 19, Train_Loss: 0.33497124910354614, Test_Loss: 0.3921254873275757 *\n",
      "2:Epoch: 19, Train_Loss: 0.3172709047794342, Test_Loss: 0.33026808500289917 *\n",
      "3:Epoch: 19, Train_Loss: 0.3339255154132843, Test_Loss: 0.6945170164108276\n",
      "4:Epoch: 19, Train_Loss: 0.3134687840938568, Test_Loss: 0.9138423204421997\n",
      "5:Epoch: 19, Train_Loss: 0.3129205107688904, Test_Loss: 0.5456143617630005 *\n",
      "6:Epoch: 19, Train_Loss: 0.3304417133331299, Test_Loss: 0.3688619136810303 *\n",
      "7:Epoch: 19, Train_Loss: 0.31546255946159363, Test_Loss: 0.35290589928627014 *\n",
      "8:Epoch: 19, Train_Loss: 0.31328171491622925, Test_Loss: 0.3210936486721039 *\n",
      "9:Epoch: 19, Train_Loss: 0.3104601204395294, Test_Loss: 0.41911202669143677\n",
      "10:Epoch: 19, Train_Loss: 0.3199802339076996, Test_Loss: 1.0628063678741455\n",
      "11:Epoch: 19, Train_Loss: 0.3503986895084381, Test_Loss: 1.2440840005874634\n",
      "12:Epoch: 19, Train_Loss: 0.35083091259002686, Test_Loss: 0.3764975368976593 *\n",
      "13:Epoch: 19, Train_Loss: 0.3508622348308563, Test_Loss: 0.4172833263874054\n",
      "14:Epoch: 19, Train_Loss: 0.3085308372974396, Test_Loss: 0.3100443482398987 *\n",
      "15:Epoch: 19, Train_Loss: 0.3587621748447418, Test_Loss: 0.314376562833786\n",
      "16:Epoch: 19, Train_Loss: 0.336291640996933, Test_Loss: 0.3166312873363495\n",
      "17:Epoch: 19, Train_Loss: 0.3106204867362976, Test_Loss: 0.32528701424598694\n",
      "18:Epoch: 19, Train_Loss: 0.33265745639801025, Test_Loss: 0.348924458026886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:Epoch: 19, Train_Loss: 0.3368664085865021, Test_Loss: 0.3342399597167969 *\n",
      "20:Epoch: 19, Train_Loss: 0.40731385350227356, Test_Loss: 0.3139471113681793 *\n",
      "21:Epoch: 19, Train_Loss: 0.3955304026603699, Test_Loss: 0.43368157744407654\n",
      "22:Epoch: 19, Train_Loss: 0.36877143383026123, Test_Loss: 0.7212244272232056\n",
      "23:Epoch: 19, Train_Loss: 0.33256399631500244, Test_Loss: 0.4727444052696228 *\n",
      "24:Epoch: 19, Train_Loss: 0.3131383955478668, Test_Loss: 0.43146049976348877 *\n",
      "25:Epoch: 19, Train_Loss: 0.32996851205825806, Test_Loss: 0.3314955532550812 *\n",
      "26:Epoch: 19, Train_Loss: 0.3085686266422272, Test_Loss: 0.3321380019187927\n",
      "27:Epoch: 19, Train_Loss: 0.3139754831790924, Test_Loss: 0.3323882520198822\n",
      "28:Epoch: 19, Train_Loss: 0.3297968804836273, Test_Loss: 0.33835306763648987\n",
      "29:Epoch: 19, Train_Loss: 0.33170729875564575, Test_Loss: 0.3814883232116699\n",
      "30:Epoch: 19, Train_Loss: 0.41309940814971924, Test_Loss: 5.545699119567871\n",
      "31:Epoch: 19, Train_Loss: 0.30894532799720764, Test_Loss: 0.48409754037857056 *\n",
      "32:Epoch: 19, Train_Loss: 0.3834700286388397, Test_Loss: 0.3184811770915985 *\n",
      "33:Epoch: 19, Train_Loss: 0.314439982175827, Test_Loss: 0.3118855059146881 *\n",
      "34:Epoch: 19, Train_Loss: 0.34593093395233154, Test_Loss: 0.31619855761528015\n",
      "35:Epoch: 19, Train_Loss: 0.33133840560913086, Test_Loss: 0.318243145942688\n",
      "36:Epoch: 19, Train_Loss: 0.5989338159561157, Test_Loss: 0.31204941868782043 *\n",
      "37:Epoch: 19, Train_Loss: 0.3364490568637848, Test_Loss: 0.3099174201488495 *\n",
      "38:Epoch: 19, Train_Loss: 0.3442981541156769, Test_Loss: 0.3087272644042969 *\n",
      "39:Epoch: 19, Train_Loss: 0.30762919783592224, Test_Loss: 0.3082228899002075 *\n",
      "40:Epoch: 19, Train_Loss: 0.307218462228775, Test_Loss: 0.3090946078300476\n",
      "41:Epoch: 19, Train_Loss: 0.3084760904312134, Test_Loss: 0.3111990988254547\n",
      "42:Epoch: 19, Train_Loss: 0.30720144510269165, Test_Loss: 0.3283403813838959\n",
      "43:Epoch: 19, Train_Loss: 0.3192919194698334, Test_Loss: 0.3457255959510803\n",
      "44:Epoch: 19, Train_Loss: 0.32323622703552246, Test_Loss: 0.3270025849342346 *\n",
      "45:Epoch: 19, Train_Loss: 0.3307924270629883, Test_Loss: 0.30744755268096924 *\n",
      "46:Epoch: 19, Train_Loss: 0.3244553506374359, Test_Loss: 0.3071560859680176 *\n",
      "47:Epoch: 19, Train_Loss: 0.3227936327457428, Test_Loss: 0.3080446124076843\n",
      "48:Epoch: 19, Train_Loss: 0.32013165950775146, Test_Loss: 0.3089069426059723\n",
      "49:Epoch: 19, Train_Loss: 0.3089119493961334, Test_Loss: 0.3063609302043915 *\n",
      "50:Epoch: 19, Train_Loss: 0.30608585476875305, Test_Loss: 0.30870747566223145\n",
      "51:Epoch: 19, Train_Loss: 0.32212042808532715, Test_Loss: 0.30661728978157043 *\n",
      "52:Epoch: 19, Train_Loss: 0.3323812484741211, Test_Loss: 0.3082687556743622\n",
      "53:Epoch: 19, Train_Loss: 0.34214162826538086, Test_Loss: 0.3085094392299652\n",
      "54:Epoch: 19, Train_Loss: 0.3065846860408783, Test_Loss: 0.3083954155445099 *\n",
      "55:Epoch: 19, Train_Loss: 0.3514874577522278, Test_Loss: 0.3071015775203705 *\n",
      "56:Epoch: 19, Train_Loss: 0.36482319235801697, Test_Loss: 0.3069021701812744 *\n",
      "57:Epoch: 19, Train_Loss: 0.3529968857765198, Test_Loss: 0.3094811737537384\n",
      "58:Epoch: 19, Train_Loss: 0.30617523193359375, Test_Loss: 0.3067471385002136 *\n",
      "59:Epoch: 19, Train_Loss: 0.3447461426258087, Test_Loss: 0.3108275234699249\n",
      "60:Epoch: 19, Train_Loss: 0.3078613579273224, Test_Loss: 0.36703839898109436\n",
      "61:Epoch: 19, Train_Loss: 0.32459571957588196, Test_Loss: 1.8248512744903564\n",
      "62:Epoch: 19, Train_Loss: 0.3072769343852997, Test_Loss: 4.2380218505859375\n",
      "63:Epoch: 19, Train_Loss: 0.33422714471817017, Test_Loss: 0.3113417327404022 *\n",
      "64:Epoch: 19, Train_Loss: 0.39056259393692017, Test_Loss: 0.3062039613723755 *\n",
      "65:Epoch: 19, Train_Loss: 3.4538145065307617, Test_Loss: 0.36058568954467773\n",
      "66:Epoch: 19, Train_Loss: 2.5785391330718994, Test_Loss: 0.36260318756103516\n",
      "67:Epoch: 19, Train_Loss: 0.32374107837677, Test_Loss: 0.3628600537776947\n",
      "68:Epoch: 19, Train_Loss: 0.3067918121814728, Test_Loss: 0.3308122158050537 *\n",
      "69:Epoch: 19, Train_Loss: 0.43442806601524353, Test_Loss: 0.4236968755722046\n",
      "70:Epoch: 19, Train_Loss: 0.42118769884109497, Test_Loss: 0.3173919916152954 *\n",
      "71:Epoch: 19, Train_Loss: 0.3271350562572479, Test_Loss: 0.3161240518093109 *\n",
      "72:Epoch: 19, Train_Loss: 0.30620381236076355, Test_Loss: 0.33740898966789246\n",
      "73:Epoch: 19, Train_Loss: 0.3614715337753296, Test_Loss: 0.3244870901107788 *\n",
      "74:Epoch: 19, Train_Loss: 0.33650529384613037, Test_Loss: 0.31286102533340454 *\n",
      "75:Epoch: 19, Train_Loss: 0.32154741883277893, Test_Loss: 0.3756923973560333\n",
      "76:Epoch: 19, Train_Loss: 0.4415300786495209, Test_Loss: 0.4040759801864624\n",
      "77:Epoch: 19, Train_Loss: 1.4441384077072144, Test_Loss: 0.3690876364707947 *\n",
      "78:Epoch: 19, Train_Loss: 1.4626795053482056, Test_Loss: 0.39100873470306396\n",
      "79:Epoch: 19, Train_Loss: 0.38811948895454407, Test_Loss: 0.3299531042575836 *\n",
      "80:Epoch: 19, Train_Loss: 0.3662497103214264, Test_Loss: 0.36474159359931946\n",
      "81:Epoch: 19, Train_Loss: 2.165679931640625, Test_Loss: 0.32201138138771057 *\n",
      "82:Epoch: 19, Train_Loss: 1.5156540870666504, Test_Loss: 0.3092564046382904 *\n",
      "83:Epoch: 19, Train_Loss: 0.34713858366012573, Test_Loss: 0.3163612484931946\n",
      "84:Epoch: 19, Train_Loss: 0.32902470231056213, Test_Loss: 0.32972452044487\n",
      "85:Epoch: 19, Train_Loss: 0.5973796844482422, Test_Loss: 0.32538270950317383 *\n",
      "86:Epoch: 19, Train_Loss: 1.0328888893127441, Test_Loss: 0.32799872756004333\n",
      "87:Epoch: 19, Train_Loss: 0.9614107608795166, Test_Loss: 0.3181125521659851 *\n",
      "88:Epoch: 19, Train_Loss: 0.30845966935157776, Test_Loss: 0.32418739795684814\n",
      "89:Epoch: 19, Train_Loss: 0.3288710415363312, Test_Loss: 0.31308770179748535 *\n",
      "90:Epoch: 19, Train_Loss: 0.4305894374847412, Test_Loss: 0.31906959414482117\n",
      "91:Epoch: 19, Train_Loss: 0.8494576811790466, Test_Loss: 0.30733516812324524 *\n",
      "92:Epoch: 19, Train_Loss: 0.34519854187965393, Test_Loss: 0.32037216424942017\n",
      "93:Epoch: 19, Train_Loss: 0.3571341633796692, Test_Loss: 0.3372320234775543\n",
      "94:Epoch: 19, Train_Loss: 0.35715651512145996, Test_Loss: 0.5261769890785217\n",
      "95:Epoch: 19, Train_Loss: 0.41970357298851013, Test_Loss: 0.5503219366073608\n",
      "96:Epoch: 19, Train_Loss: 0.44643786549568176, Test_Loss: 0.4728551208972931 *\n",
      "97:Epoch: 19, Train_Loss: 0.38292092084884644, Test_Loss: 0.3311457335948944 *\n",
      "98:Epoch: 19, Train_Loss: 0.361751914024353, Test_Loss: 0.33933743834495544\n",
      "99:Epoch: 19, Train_Loss: 0.35892656445503235, Test_Loss: 0.3134087920188904 *\n",
      "100:Epoch: 19, Train_Loss: 0.4159078896045685, Test_Loss: 0.34627920389175415\n",
      "Model saved at location ../Saver/model.ckpt at epoch 19\n",
      "101:Epoch: 19, Train_Loss: 0.37276124954223633, Test_Loss: 0.3891580104827881\n",
      "102:Epoch: 19, Train_Loss: 0.43255481123924255, Test_Loss: 0.5396700501441956\n",
      "103:Epoch: 19, Train_Loss: 0.47750115394592285, Test_Loss: 0.38198643922805786 *\n",
      "104:Epoch: 19, Train_Loss: 0.33782631158828735, Test_Loss: 0.37087199091911316 *\n",
      "105:Epoch: 19, Train_Loss: 0.4487987756729126, Test_Loss: 0.36899736523628235 *\n",
      "106:Epoch: 19, Train_Loss: 0.39435040950775146, Test_Loss: 0.399156391620636\n",
      "107:Epoch: 19, Train_Loss: 0.3382939100265503, Test_Loss: 0.4695225954055786\n",
      "108:Epoch: 19, Train_Loss: 0.31449928879737854, Test_Loss: 0.38830918073654175 *\n",
      "109:Epoch: 19, Train_Loss: 0.30616462230682373, Test_Loss: 0.47000667452812195\n",
      "110:Epoch: 19, Train_Loss: 0.30678457021713257, Test_Loss: 0.4005344808101654 *\n",
      "111:Epoch: 19, Train_Loss: 0.3066912889480591, Test_Loss: 0.4033541977405548\n",
      "112:Epoch: 19, Train_Loss: 0.31609678268432617, Test_Loss: 0.5513614416122437\n",
      "113:Epoch: 19, Train_Loss: 0.3393203318119049, Test_Loss: 0.5926728248596191\n",
      "114:Epoch: 19, Train_Loss: 0.33610057830810547, Test_Loss: 1.0726412534713745\n",
      "115:Epoch: 19, Train_Loss: 0.3837752342224121, Test_Loss: 0.5765582323074341 *\n",
      "116:Epoch: 19, Train_Loss: 0.4675480127334595, Test_Loss: 0.4453561305999756 *\n",
      "117:Epoch: 19, Train_Loss: 0.576133131980896, Test_Loss: 0.43186116218566895 *\n",
      "118:Epoch: 19, Train_Loss: 0.3213675618171692, Test_Loss: 0.4252721071243286 *\n",
      "119:Epoch: 19, Train_Loss: 0.3533642590045929, Test_Loss: 0.614072322845459\n",
      "120:Epoch: 19, Train_Loss: 0.48559534549713135, Test_Loss: 0.7390697002410889\n",
      "121:Epoch: 19, Train_Loss: 0.37614956498146057, Test_Loss: 6.32208251953125\n",
      "122:Epoch: 19, Train_Loss: 0.466468870639801, Test_Loss: 0.4350428879261017 *\n",
      "123:Epoch: 19, Train_Loss: 0.40105175971984863, Test_Loss: 0.3465613126754761 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124:Epoch: 19, Train_Loss: 0.5556513071060181, Test_Loss: 0.3615972697734833\n",
      "125:Epoch: 19, Train_Loss: 0.4283910095691681, Test_Loss: 0.31778404116630554 *\n",
      "126:Epoch: 19, Train_Loss: 0.4384840726852417, Test_Loss: 0.35015034675598145\n",
      "127:Epoch: 19, Train_Loss: 0.330894410610199, Test_Loss: 0.4262089729309082\n",
      "128:Epoch: 19, Train_Loss: 0.3625146150588989, Test_Loss: 0.40072426199913025 *\n",
      "129:Epoch: 19, Train_Loss: 0.4831836521625519, Test_Loss: 0.30893269181251526 *\n",
      "130:Epoch: 19, Train_Loss: 1.0991544723510742, Test_Loss: 0.3361700177192688\n",
      "131:Epoch: 19, Train_Loss: 0.6623417139053345, Test_Loss: 0.36212393641471863\n",
      "132:Epoch: 19, Train_Loss: 0.351970911026001, Test_Loss: 0.4495841860771179\n",
      "133:Epoch: 19, Train_Loss: 0.3802032768726349, Test_Loss: 0.3459707498550415 *\n",
      "134:Epoch: 19, Train_Loss: 0.3078044056892395, Test_Loss: 0.358578622341156\n",
      "135:Epoch: 19, Train_Loss: 0.5652797818183899, Test_Loss: 0.38135769963264465\n",
      "136:Epoch: 19, Train_Loss: 0.5214649438858032, Test_Loss: 0.31761541962623596 *\n",
      "137:Epoch: 19, Train_Loss: 0.31121811270713806, Test_Loss: 0.30991706252098083 *\n",
      "138:Epoch: 19, Train_Loss: 0.5525155663490295, Test_Loss: 0.345348984003067\n",
      "139:Epoch: 19, Train_Loss: 0.32066380977630615, Test_Loss: 0.4280504584312439\n",
      "140:Epoch: 19, Train_Loss: 0.32905685901641846, Test_Loss: 0.3115125596523285 *\n",
      "141:Epoch: 19, Train_Loss: 0.33230143785476685, Test_Loss: 0.3253302276134491\n",
      "142:Epoch: 19, Train_Loss: 0.46829530596733093, Test_Loss: 0.31979668140411377 *\n",
      "143:Epoch: 19, Train_Loss: 0.37466979026794434, Test_Loss: 0.38253650069236755\n",
      "144:Epoch: 19, Train_Loss: 0.488894522190094, Test_Loss: 0.40606722235679626\n",
      "145:Epoch: 19, Train_Loss: 0.3232770562171936, Test_Loss: 0.39142948389053345 *\n",
      "146:Epoch: 19, Train_Loss: 0.4068882465362549, Test_Loss: 0.3164946734905243 *\n",
      "147:Epoch: 19, Train_Loss: 0.33834177255630493, Test_Loss: 0.3265049159526825\n",
      "148:Epoch: 19, Train_Loss: 0.33169880509376526, Test_Loss: 0.3455500602722168\n",
      "149:Epoch: 19, Train_Loss: 0.31374239921569824, Test_Loss: 0.31433042883872986 *\n",
      "150:Epoch: 19, Train_Loss: 0.34644317626953125, Test_Loss: 0.35488656163215637\n",
      "151:Epoch: 19, Train_Loss: 0.42202121019363403, Test_Loss: 0.44280585646629333\n",
      "152:Epoch: 19, Train_Loss: 0.5369720458984375, Test_Loss: 2.9454708099365234\n",
      "153:Epoch: 19, Train_Loss: 0.5316042304039001, Test_Loss: 2.786540985107422 *\n",
      "154:Epoch: 19, Train_Loss: 0.788727343082428, Test_Loss: 0.32638615369796753 *\n",
      "155:Epoch: 19, Train_Loss: 0.6074572801589966, Test_Loss: 0.32582002878189087 *\n",
      "156:Epoch: 19, Train_Loss: 0.5117232203483582, Test_Loss: 0.3350366950035095\n",
      "157:Epoch: 19, Train_Loss: 0.38151517510414124, Test_Loss: 0.30443352460861206 *\n",
      "158:Epoch: 19, Train_Loss: 0.32890215516090393, Test_Loss: 0.32199153304100037\n",
      "159:Epoch: 19, Train_Loss: 0.32394087314605713, Test_Loss: 0.3630494773387909\n",
      "160:Epoch: 19, Train_Loss: 0.30971696972846985, Test_Loss: 0.36476385593414307\n",
      "161:Epoch: 19, Train_Loss: 0.392844021320343, Test_Loss: 0.3080596625804901 *\n",
      "162:Epoch: 19, Train_Loss: 0.6442726850509644, Test_Loss: 0.33397117257118225\n",
      "163:Epoch: 19, Train_Loss: 0.6120488047599792, Test_Loss: 0.3277761936187744 *\n",
      "164:Epoch: 19, Train_Loss: 1.4538788795471191, Test_Loss: 0.43612292408943176\n",
      "165:Epoch: 19, Train_Loss: 1.2362029552459717, Test_Loss: 0.33734801411628723 *\n",
      "166:Epoch: 19, Train_Loss: 0.46291324496269226, Test_Loss: 0.3586848974227905\n",
      "167:Epoch: 19, Train_Loss: 0.44811445474624634, Test_Loss: 0.34859809279441833 *\n",
      "168:Epoch: 19, Train_Loss: 0.3058791160583496, Test_Loss: 0.38571101427078247\n",
      "169:Epoch: 19, Train_Loss: 0.41265296936035156, Test_Loss: 0.30858275294303894 *\n",
      "170:Epoch: 19, Train_Loss: 0.8216118216514587, Test_Loss: 0.3541944622993469\n",
      "171:Epoch: 19, Train_Loss: 1.0870929956436157, Test_Loss: 0.4345639944076538\n",
      "172:Epoch: 19, Train_Loss: 0.3227132558822632, Test_Loss: 0.3673378825187683 *\n",
      "173:Epoch: 19, Train_Loss: 0.3536037504673004, Test_Loss: 0.39839252829551697\n",
      "174:Epoch: 19, Train_Loss: 0.38165831565856934, Test_Loss: 0.4035117030143738\n",
      "175:Epoch: 19, Train_Loss: 0.525141716003418, Test_Loss: 0.3519827127456665 *\n",
      "176:Epoch: 19, Train_Loss: 0.45448803901672363, Test_Loss: 0.36492812633514404\n",
      "177:Epoch: 19, Train_Loss: 0.4748958349227905, Test_Loss: 0.4046485126018524\n",
      "178:Epoch: 19, Train_Loss: 0.4571150839328766, Test_Loss: 0.3766363859176636 *\n",
      "179:Epoch: 19, Train_Loss: 0.477419376373291, Test_Loss: 0.38786470890045166\n",
      "180:Epoch: 19, Train_Loss: 0.32343047857284546, Test_Loss: 0.32146185636520386 *\n",
      "181:Epoch: 19, Train_Loss: 0.3268822729587555, Test_Loss: 0.37037578225135803\n",
      "182:Epoch: 19, Train_Loss: 0.31455591320991516, Test_Loss: 0.3350399434566498 *\n",
      "183:Epoch: 19, Train_Loss: 0.39175790548324585, Test_Loss: 0.31705442070961 *\n",
      "184:Epoch: 19, Train_Loss: 0.34580251574516296, Test_Loss: 0.3705599904060364\n",
      "185:Epoch: 19, Train_Loss: 0.4161878526210785, Test_Loss: 0.43047934770584106\n",
      "186:Epoch: 19, Train_Loss: 15.306465148925781, Test_Loss: 0.471219539642334\n",
      "187:Epoch: 19, Train_Loss: 0.44701045751571655, Test_Loss: 0.37391239404678345 *\n",
      "188:Epoch: 19, Train_Loss: 1.5475412607192993, Test_Loss: 0.31004106998443604 *\n",
      "189:Epoch: 19, Train_Loss: 1.3008530139923096, Test_Loss: 0.32564592361450195\n",
      "190:Epoch: 19, Train_Loss: 0.3221535086631775, Test_Loss: 0.31812784075737 *\n",
      "191:Epoch: 19, Train_Loss: 0.420820951461792, Test_Loss: 0.4179185628890991\n",
      "192:Epoch: 19, Train_Loss: 3.6716699600219727, Test_Loss: 0.4145395755767822 *\n",
      "193:Epoch: 19, Train_Loss: 4.762423038482666, Test_Loss: 0.6073895692825317\n",
      "194:Epoch: 19, Train_Loss: 0.42676782608032227, Test_Loss: 0.41173288226127625 *\n",
      "195:Epoch: 19, Train_Loss: 0.4553988575935364, Test_Loss: 0.4147665202617645\n",
      "196:Epoch: 19, Train_Loss: 6.166231632232666, Test_Loss: 0.44679754972457886\n",
      "197:Epoch: 19, Train_Loss: 0.4402638077735901, Test_Loss: 0.5243638753890991\n",
      "198:Epoch: 19, Train_Loss: 0.3560006320476532, Test_Loss: 0.5877784490585327\n",
      "199:Epoch: 19, Train_Loss: 0.39036646485328674, Test_Loss: 0.36984944343566895 *\n",
      "200:Epoch: 19, Train_Loss: 0.48561516404151917, Test_Loss: 0.5266520977020264\n",
      "Model saved at location ../Saver/model.ckpt at epoch 19\n",
      "201:Epoch: 19, Train_Loss: 0.34828248620033264, Test_Loss: 0.4012894630432129 *\n",
      "202:Epoch: 19, Train_Loss: 0.3031008541584015, Test_Loss: 0.46900710463523865\n",
      "203:Epoch: 19, Train_Loss: 0.3126656413078308, Test_Loss: 0.9040098190307617\n",
      "204:Epoch: 19, Train_Loss: 0.29841652512550354, Test_Loss: 0.5567022562026978 *\n",
      "205:Epoch: 19, Train_Loss: 0.30026212334632874, Test_Loss: 0.9766464233398438\n",
      "206:Epoch: 19, Train_Loss: 0.32272639870643616, Test_Loss: 0.49716678261756897 *\n",
      "207:Epoch: 19, Train_Loss: 0.31697484850883484, Test_Loss: 0.4752591848373413 *\n",
      "208:Epoch: 19, Train_Loss: 0.39815953373908997, Test_Loss: 0.4677008390426636 *\n",
      "209:Epoch: 19, Train_Loss: 0.3343655467033386, Test_Loss: 0.4619048535823822 *\n",
      "210:Epoch: 19, Train_Loss: 0.33796602487564087, Test_Loss: 0.5922783613204956\n",
      "211:Epoch: 19, Train_Loss: 0.31288450956344604, Test_Loss: 1.8040828704833984\n",
      "212:Epoch: 19, Train_Loss: 0.31355634331703186, Test_Loss: 5.688295841217041\n",
      "213:Epoch: 19, Train_Loss: 0.3262445330619812, Test_Loss: 0.45894551277160645 *\n",
      "214:Epoch: 19, Train_Loss: 0.3268865942955017, Test_Loss: 0.3991691768169403 *\n",
      "215:Epoch: 19, Train_Loss: 0.3061120808124542, Test_Loss: 0.46619415283203125\n",
      "216:Epoch: 19, Train_Loss: 0.30048036575317383, Test_Loss: 0.33502504229545593 *\n",
      "217:Epoch: 19, Train_Loss: 0.2993275821208954, Test_Loss: 0.3969714343547821\n",
      "218:Epoch: 19, Train_Loss: 0.2982426583766937, Test_Loss: 0.5822007656097412\n",
      "219:Epoch: 19, Train_Loss: 0.3016165494918823, Test_Loss: 0.4099200367927551 *\n",
      "220:Epoch: 19, Train_Loss: 0.29953423142433167, Test_Loss: 0.3081287741661072 *\n",
      "221:Epoch: 19, Train_Loss: 0.2999369502067566, Test_Loss: 0.32760560512542725\n",
      "222:Epoch: 19, Train_Loss: 0.3169001638889313, Test_Loss: 0.3219519555568695 *\n",
      "223:Epoch: 19, Train_Loss: 0.3310616910457611, Test_Loss: 0.5172939896583557\n",
      "224:Epoch: 19, Train_Loss: 0.3788841962814331, Test_Loss: 0.34069469571113586 *\n",
      "225:Epoch: 19, Train_Loss: 0.3817037045955658, Test_Loss: 0.4530598521232605\n",
      "226:Epoch: 19, Train_Loss: 0.711767852306366, Test_Loss: 0.41302168369293213 *\n",
      "227:Epoch: 19, Train_Loss: 3.029000759124756, Test_Loss: 0.30727440118789673 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228:Epoch: 19, Train_Loss: 4.385321140289307, Test_Loss: 0.3220021426677704\n",
      "229:Epoch: 19, Train_Loss: 0.3695370554924011, Test_Loss: 0.31776461005210876 *\n",
      "230:Epoch: 19, Train_Loss: 0.3981707990169525, Test_Loss: 0.7939044237136841\n",
      "231:Epoch: 19, Train_Loss: 0.4445750117301941, Test_Loss: 0.35311123728752136 *\n",
      "232:Epoch: 19, Train_Loss: 0.4566246271133423, Test_Loss: 0.4337412714958191\n",
      "233:Epoch: 19, Train_Loss: 0.4082537591457367, Test_Loss: 0.3417370915412903 *\n",
      "234:Epoch: 19, Train_Loss: 0.3419564664363861, Test_Loss: 0.5637521743774414\n",
      "235:Epoch: 19, Train_Loss: 0.3304160535335541, Test_Loss: 0.4816815257072449 *\n",
      "236:Epoch: 19, Train_Loss: 0.4550189971923828, Test_Loss: 0.41778120398521423 *\n",
      "237:Epoch: 19, Train_Loss: 0.38244402408599854, Test_Loss: 0.31745830178260803 *\n",
      "238:Epoch: 19, Train_Loss: 0.32890722155570984, Test_Loss: 0.30892401933670044 *\n",
      "239:Epoch: 19, Train_Loss: 0.3159092962741852, Test_Loss: 0.3231814503669739\n",
      "240:Epoch: 19, Train_Loss: 0.3424830734729767, Test_Loss: 0.307853639125824 *\n",
      "241:Epoch: 19, Train_Loss: 0.3735707700252533, Test_Loss: 0.3963792324066162\n",
      "242:Epoch: 19, Train_Loss: 0.3489716053009033, Test_Loss: 0.39969366788864136\n",
      "243:Epoch: 19, Train_Loss: 0.31811198592185974, Test_Loss: 4.364514350891113\n",
      "244:Epoch: 19, Train_Loss: 0.3341839015483856, Test_Loss: 2.290027141571045 *\n",
      "245:Epoch: 19, Train_Loss: 0.30494821071624756, Test_Loss: 0.3061664402484894 *\n",
      "246:Epoch: 19, Train_Loss: 0.34537890553474426, Test_Loss: 0.3216229975223541\n",
      "247:Epoch: 19, Train_Loss: 0.35704782605171204, Test_Loss: 0.3421129584312439\n",
      "248:Epoch: 19, Train_Loss: 0.3166676461696625, Test_Loss: 0.3240642845630646 *\n",
      "249:Epoch: 19, Train_Loss: 0.2974700629711151, Test_Loss: 0.3136855661869049 *\n",
      "250:Epoch: 19, Train_Loss: 0.29671987891197205, Test_Loss: 0.36837729811668396\n",
      "251:Epoch: 19, Train_Loss: 0.30350759625434875, Test_Loss: 0.3726293742656708\n",
      "252:Epoch: 19, Train_Loss: 3.5195834636688232, Test_Loss: 0.30093351006507874 *\n",
      "253:Epoch: 19, Train_Loss: 3.0392074584960938, Test_Loss: 0.3252575695514679\n",
      "254:Epoch: 19, Train_Loss: 0.3010804355144501, Test_Loss: 0.31869158148765564 *\n",
      "255:Epoch: 19, Train_Loss: 0.3091103136539459, Test_Loss: 0.31763458251953125 *\n",
      "256:Epoch: 19, Train_Loss: 0.3095311224460602, Test_Loss: 0.30183789134025574 *\n",
      "257:Epoch: 19, Train_Loss: 0.30488643050193787, Test_Loss: 0.34412088990211487\n",
      "258:Epoch: 19, Train_Loss: 0.30753710865974426, Test_Loss: 0.3406996428966522 *\n",
      "259:Epoch: 19, Train_Loss: 0.3011379539966583, Test_Loss: 0.40776702761650085\n",
      "260:Epoch: 19, Train_Loss: 0.32363489270210266, Test_Loss: 0.38184434175491333 *\n",
      "261:Epoch: 19, Train_Loss: 0.3507581353187561, Test_Loss: 0.3315544128417969 *\n",
      "262:Epoch: 19, Train_Loss: 0.3106459081172943, Test_Loss: 0.3132825493812561 *\n",
      "263:Epoch: 19, Train_Loss: 0.3011573255062103, Test_Loss: 0.30277320742607117 *\n",
      "264:Epoch: 19, Train_Loss: 0.29868900775909424, Test_Loss: 0.3009328544139862 *\n",
      "265:Epoch: 19, Train_Loss: 0.30324044823646545, Test_Loss: 0.2989291846752167 *\n",
      "266:Epoch: 19, Train_Loss: 0.3246617317199707, Test_Loss: 0.30278101563453674\n",
      "267:Epoch: 19, Train_Loss: 0.305940717458725, Test_Loss: 0.2992195785045624 *\n",
      "268:Epoch: 19, Train_Loss: 0.301321804523468, Test_Loss: 0.2960207164287567 *\n",
      "269:Epoch: 19, Train_Loss: 0.33286675810813904, Test_Loss: 0.30086955428123474\n",
      "270:Epoch: 19, Train_Loss: 0.3389994502067566, Test_Loss: 0.2993132770061493 *\n",
      "271:Epoch: 19, Train_Loss: 0.2969176769256592, Test_Loss: 0.3008842170238495\n",
      "272:Epoch: 19, Train_Loss: 0.2956869304180145, Test_Loss: 0.3098814785480499\n",
      "273:Epoch: 19, Train_Loss: 0.3572588860988617, Test_Loss: 0.3045596778392792 *\n",
      "274:Epoch: 19, Train_Loss: 0.4043603539466858, Test_Loss: 0.3240479528903961\n",
      "275:Epoch: 19, Train_Loss: 0.34499216079711914, Test_Loss: 0.3993927240371704\n",
      "276:Epoch: 19, Train_Loss: 0.3330477476119995, Test_Loss: 0.5495537519454956\n",
      "277:Epoch: 19, Train_Loss: 0.34988588094711304, Test_Loss: 0.5535686612129211\n",
      "278:Epoch: 19, Train_Loss: 0.397044837474823, Test_Loss: 0.3644866943359375 *\n",
      "279:Epoch: 19, Train_Loss: 0.3521954119205475, Test_Loss: 0.3041733503341675 *\n",
      "280:Epoch: 19, Train_Loss: 0.37292730808258057, Test_Loss: 0.3108876049518585\n",
      "281:Epoch: 19, Train_Loss: 0.416034460067749, Test_Loss: 0.3892417252063751\n",
      "282:Epoch: 19, Train_Loss: 0.3681028187274933, Test_Loss: 0.7404686808586121\n",
      "283:Epoch: 19, Train_Loss: 0.3132617473602295, Test_Loss: 1.0443532466888428\n",
      "284:Epoch: 19, Train_Loss: 0.2985762655735016, Test_Loss: 0.7394689321517944 *\n",
      "285:Epoch: 19, Train_Loss: 0.298110693693161, Test_Loss: 0.3687297999858856 *\n",
      "286:Epoch: 19, Train_Loss: 0.2961486577987671, Test_Loss: 0.30617794394493103 *\n",
      "287:Epoch: 19, Train_Loss: 0.2957378923892975, Test_Loss: 0.30689460039138794\n",
      "288:Epoch: 19, Train_Loss: 0.2965778708457947, Test_Loss: 0.30367276072502136 *\n",
      "289:Epoch: 19, Train_Loss: 3.386518955230713, Test_Loss: 0.313920795917511\n",
      "290:Epoch: 19, Train_Loss: 2.2465639114379883, Test_Loss: 0.32931840419769287\n",
      "291:Epoch: 19, Train_Loss: 0.29468613862991333, Test_Loss: 0.3552839756011963\n",
      "292:Epoch: 19, Train_Loss: 0.316348671913147, Test_Loss: 0.2981147766113281 *\n",
      "293:Epoch: 19, Train_Loss: 0.3027696907520294, Test_Loss: 0.3785061240196228\n",
      "294:Epoch: 19, Train_Loss: 0.2939266860485077, Test_Loss: 0.5241373777389526\n",
      "295:Epoch: 19, Train_Loss: 0.2949793338775635, Test_Loss: 0.5487511157989502\n",
      "296:Epoch: 19, Train_Loss: 0.2939874827861786, Test_Loss: 0.51961350440979 *\n",
      "297:Epoch: 19, Train_Loss: 0.29383671283721924, Test_Loss: 0.3072981834411621 *\n",
      "298:Epoch: 19, Train_Loss: 0.29356446862220764, Test_Loss: 0.30736926198005676\n",
      "299:Epoch: 19, Train_Loss: 0.31332242488861084, Test_Loss: 0.30752408504486084\n",
      "300:Epoch: 19, Train_Loss: 0.38154563307762146, Test_Loss: 0.30832189321517944\n",
      "Model saved at location ../Saver/model.ckpt at epoch 19\n",
      "301:Epoch: 19, Train_Loss: 0.37483447790145874, Test_Loss: 0.3265332877635956\n",
      "302:Epoch: 19, Train_Loss: 0.39374518394470215, Test_Loss: 2.9647979736328125\n",
      "303:Epoch: 19, Train_Loss: 0.3179941475391388, Test_Loss: 3.082869529724121\n",
      "304:Epoch: 19, Train_Loss: 0.3071550130844116, Test_Loss: 0.30665385723114014 *\n",
      "305:Epoch: 19, Train_Loss: 0.5061084032058716, Test_Loss: 0.29936549067497253 *\n",
      "306:Epoch: 19, Train_Loss: 0.5085660219192505, Test_Loss: 0.2988051474094391 *\n",
      "307:Epoch: 19, Train_Loss: 0.5137187242507935, Test_Loss: 0.3073287308216095\n",
      "308:Epoch: 19, Train_Loss: 0.3624141812324524, Test_Loss: 0.29662537574768066 *\n",
      "309:Epoch: 19, Train_Loss: 0.2932237386703491, Test_Loss: 0.2987755835056305\n",
      "310:Epoch: 19, Train_Loss: 0.29338863492012024, Test_Loss: 0.29461807012557983 *\n",
      "311:Epoch: 19, Train_Loss: 0.29806429147720337, Test_Loss: 0.295131117105484\n",
      "312:Epoch: 19, Train_Loss: 0.30378907918930054, Test_Loss: 0.2958959937095642\n",
      "313:Epoch: 19, Train_Loss: 0.30824369192123413, Test_Loss: 0.29398858547210693 *\n",
      "314:Epoch: 19, Train_Loss: 0.3023070991039276, Test_Loss: 0.30216485261917114\n",
      "315:Epoch: 19, Train_Loss: 0.29299503564834595, Test_Loss: 0.33113521337509155\n",
      "316:Epoch: 19, Train_Loss: 0.2928515076637268, Test_Loss: 0.32538604736328125 *\n",
      "317:Epoch: 19, Train_Loss: 0.30373191833496094, Test_Loss: 0.2946537733078003 *\n",
      "318:Epoch: 19, Train_Loss: 0.3775479793548584, Test_Loss: 0.2937605679035187 *\n",
      "319:Epoch: 19, Train_Loss: 0.4996092915534973, Test_Loss: 0.2936203181743622 *\n",
      "320:Epoch: 19, Train_Loss: 0.4764508605003357, Test_Loss: 0.2952862083911896\n",
      "321:Epoch: 19, Train_Loss: 0.4026097357273102, Test_Loss: 0.29360225796699524 *\n",
      "322:Epoch: 19, Train_Loss: 0.4060764014720917, Test_Loss: 0.29517799615859985\n",
      "323:Epoch: 19, Train_Loss: 0.4526882767677307, Test_Loss: 0.2925446331501007 *\n",
      "324:Epoch: 19, Train_Loss: 0.3100857734680176, Test_Loss: 0.29330208897590637\n",
      "325:Epoch: 19, Train_Loss: 0.43850600719451904, Test_Loss: 0.2957470118999481\n",
      "326:Epoch: 19, Train_Loss: 0.3946611285209656, Test_Loss: 0.2935207188129425 *\n",
      "327:Epoch: 19, Train_Loss: 0.5597217082977295, Test_Loss: 0.294554740190506\n",
      "328:Epoch: 19, Train_Loss: 0.30232375860214233, Test_Loss: 0.2931278944015503 *\n",
      "329:Epoch: 19, Train_Loss: 0.684951663017273, Test_Loss: 0.292865514755249 *\n",
      "330:Epoch: 19, Train_Loss: 3.042569637298584, Test_Loss: 0.2939918041229248\n",
      "331:Epoch: 19, Train_Loss: 0.34720486402511597, Test_Loss: 0.29334649443626404 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332:Epoch: 19, Train_Loss: 0.3567112684249878, Test_Loss: 0.3263396918773651\n",
      "333:Epoch: 19, Train_Loss: 0.3565964698791504, Test_Loss: 0.325700044631958 *\n",
      "334:Epoch: 19, Train_Loss: 0.3572329580783844, Test_Loss: 5.218894958496094\n",
      "335:Epoch: 19, Train_Loss: 0.29444846510887146, Test_Loss: 0.9162236452102661 *\n",
      "336:Epoch: 19, Train_Loss: 0.2948008179664612, Test_Loss: 0.29394134879112244 *\n",
      "337:Epoch: 19, Train_Loss: 0.38758954405784607, Test_Loss: 0.31650158762931824\n",
      "338:Epoch: 19, Train_Loss: 0.4015043079853058, Test_Loss: 0.36118167638778687\n",
      "339:Epoch: 19, Train_Loss: 0.3911895751953125, Test_Loss: 0.36533215641975403\n",
      "340:Epoch: 19, Train_Loss: 0.36168181896209717, Test_Loss: 0.3019450902938843 *\n",
      "341:Epoch: 19, Train_Loss: 0.3628491163253784, Test_Loss: 0.37537261843681335\n",
      "342:Epoch: 19, Train_Loss: 0.3102077841758728, Test_Loss: 0.3530210852622986 *\n",
      "343:Epoch: 19, Train_Loss: 0.32404300570487976, Test_Loss: 0.29580458998680115 *\n",
      "344:Epoch: 19, Train_Loss: 0.3024812340736389, Test_Loss: 0.322476863861084\n",
      "345:Epoch: 19, Train_Loss: 0.3274344801902771, Test_Loss: 0.3140626847743988 *\n",
      "346:Epoch: 19, Train_Loss: 0.30544450879096985, Test_Loss: 0.30047252774238586 *\n",
      "347:Epoch: 19, Train_Loss: 0.2914719879627228, Test_Loss: 0.30222344398498535\n",
      "348:Epoch: 19, Train_Loss: 0.32353416085243225, Test_Loss: 0.44361358880996704\n",
      "349:Epoch: 19, Train_Loss: 0.3405437171459198, Test_Loss: 0.3329463303089142 *\n",
      "350:Epoch: 19, Train_Loss: 0.3195556104183197, Test_Loss: 0.37514162063598633\n",
      "351:Epoch: 19, Train_Loss: 0.29310858249664307, Test_Loss: 0.3372611105442047 *\n",
      "352:Epoch: 19, Train_Loss: 0.2922727167606354, Test_Loss: 0.34682905673980713\n",
      "353:Epoch: 19, Train_Loss: 0.2915622293949127, Test_Loss: 0.3248917758464813 *\n",
      "354:Epoch: 19, Train_Loss: 0.29130372405052185, Test_Loss: 0.3172568082809448 *\n",
      "355:Epoch: 19, Train_Loss: 0.292503297328949, Test_Loss: 0.32375386357307434\n",
      "356:Epoch: 19, Train_Loss: 0.29539573192596436, Test_Loss: 0.328380823135376\n",
      "357:Epoch: 19, Train_Loss: 0.2962953746318817, Test_Loss: 0.32986247539520264\n",
      "358:Epoch: 19, Train_Loss: 0.29230451583862305, Test_Loss: 0.32869651913642883 *\n",
      "359:Epoch: 19, Train_Loss: 0.29134225845336914, Test_Loss: 0.31728753447532654 *\n",
      "360:Epoch: 19, Train_Loss: 0.2928157150745392, Test_Loss: 0.3295542597770691\n",
      "361:Epoch: 19, Train_Loss: 0.299407422542572, Test_Loss: 0.32184088230133057 *\n",
      "362:Epoch: 19, Train_Loss: 0.3154789209365845, Test_Loss: 0.3116872310638428 *\n",
      "363:Epoch: 19, Train_Loss: 0.3196312487125397, Test_Loss: 0.30786770582199097 *\n",
      "364:Epoch: 19, Train_Loss: 0.3247501850128174, Test_Loss: 0.3649245500564575\n",
      "365:Epoch: 19, Train_Loss: 0.30018651485443115, Test_Loss: 0.3398914933204651 *\n",
      "366:Epoch: 19, Train_Loss: 0.3033170700073242, Test_Loss: 0.575899600982666\n",
      "367:Epoch: 19, Train_Loss: 0.2906467020511627, Test_Loss: 0.9097107648849487\n",
      "368:Epoch: 19, Train_Loss: 0.2923566997051239, Test_Loss: 0.6348037123680115 *\n",
      "369:Epoch: 19, Train_Loss: 0.31387272477149963, Test_Loss: 0.40302222967147827 *\n",
      "370:Epoch: 19, Train_Loss: 0.30742889642715454, Test_Loss: 0.34959056973457336 *\n",
      "371:Epoch: 19, Train_Loss: 0.29303622245788574, Test_Loss: 0.2978205978870392 *\n",
      "372:Epoch: 19, Train_Loss: 0.29222843050956726, Test_Loss: 0.3604843020439148\n",
      "373:Epoch: 19, Train_Loss: 0.295085072517395, Test_Loss: 0.8590967655181885\n",
      "374:Epoch: 19, Train_Loss: 0.339765340089798, Test_Loss: 1.2940179109573364\n",
      "375:Epoch: 19, Train_Loss: 0.3279964327812195, Test_Loss: 0.5241829752922058 *\n",
      "376:Epoch: 19, Train_Loss: 0.34895896911621094, Test_Loss: 0.42069536447525024 *\n",
      "377:Epoch: 19, Train_Loss: 0.2943708896636963, Test_Loss: 0.29578304290771484 *\n",
      "378:Epoch: 19, Train_Loss: 0.3145143687725067, Test_Loss: 0.2950155436992645 *\n",
      "379:Epoch: 19, Train_Loss: 0.3524891436100006, Test_Loss: 0.2909395098686218 *\n",
      "380:Epoch: 19, Train_Loss: 0.29195258021354675, Test_Loss: 0.3070387542247772\n",
      "381:Epoch: 19, Train_Loss: 0.3016703724861145, Test_Loss: 0.3127043545246124\n",
      "382:Epoch: 19, Train_Loss: 0.3134070038795471, Test_Loss: 0.3250943124294281\n",
      "383:Epoch: 19, Train_Loss: 0.3525572419166565, Test_Loss: 0.2926250696182251 *\n",
      "384:Epoch: 19, Train_Loss: 0.3724658191204071, Test_Loss: 0.43225669860839844\n",
      "385:Epoch: 19, Train_Loss: 0.34507596492767334, Test_Loss: 0.6760048270225525\n",
      "386:Epoch: 19, Train_Loss: 0.3141178488731384, Test_Loss: 0.4173399806022644 *\n",
      "387:Epoch: 19, Train_Loss: 0.2943333387374878, Test_Loss: 0.5206220746040344\n",
      "388:Epoch: 19, Train_Loss: 0.3105866014957428, Test_Loss: 0.3131321668624878 *\n",
      "389:Epoch: 19, Train_Loss: 0.2906831204891205, Test_Loss: 0.31451135873794556\n",
      "390:Epoch: 19, Train_Loss: 0.29221710562705994, Test_Loss: 0.3143828213214874 *\n",
      "391:Epoch: 19, Train_Loss: 0.30653902888298035, Test_Loss: 0.31545495986938477\n",
      "392:Epoch: 19, Train_Loss: 0.3195149898529053, Test_Loss: 0.3368401527404785\n",
      "393:Epoch: 19, Train_Loss: 0.386877179145813, Test_Loss: 4.310957908630371\n",
      "394:Epoch: 19, Train_Loss: 0.32336270809173584, Test_Loss: 1.6531317234039307 *\n",
      "395:Epoch: 19, Train_Loss: 0.3413430452346802, Test_Loss: 0.3042585253715515 *\n",
      "396:Epoch: 19, Train_Loss: 0.2990584373474121, Test_Loss: 0.2969532608985901 *\n",
      "397:Epoch: 19, Train_Loss: 0.3234323561191559, Test_Loss: 0.29675576090812683 *\n",
      "398:Epoch: 19, Train_Loss: 0.3040502667427063, Test_Loss: 0.30638816952705383\n",
      "399:Epoch: 19, Train_Loss: 0.5142740607261658, Test_Loss: 0.2945415675640106 *\n",
      "400:Epoch: 19, Train_Loss: 0.4144831895828247, Test_Loss: 0.2920026481151581 *\n",
      "Model saved at location ../Saver/model.ckpt at epoch 19\n",
      "401:Epoch: 19, Train_Loss: 0.30370765924453735, Test_Loss: 0.28923043608665466 *\n",
      "402:Epoch: 19, Train_Loss: 0.3143216073513031, Test_Loss: 0.2895517647266388\n",
      "403:Epoch: 19, Train_Loss: 0.2886708378791809, Test_Loss: 0.29044023156166077\n",
      "404:Epoch: 19, Train_Loss: 0.28883421421051025, Test_Loss: 0.2903558611869812 *\n",
      "405:Epoch: 19, Train_Loss: 0.28948092460632324, Test_Loss: 0.3000573217868805\n",
      "406:Epoch: 19, Train_Loss: 0.2928817570209503, Test_Loss: 0.3322887420654297\n",
      "407:Epoch: 19, Train_Loss: 0.29728320240974426, Test_Loss: 0.31793174147605896 *\n",
      "408:Epoch: 19, Train_Loss: 0.30769920349121094, Test_Loss: 0.2885206639766693 *\n",
      "409:Epoch: 19, Train_Loss: 0.29920995235443115, Test_Loss: 0.29100480675697327\n",
      "410:Epoch: 19, Train_Loss: 0.3020491600036621, Test_Loss: 0.2889252305030823 *\n",
      "411:Epoch: 19, Train_Loss: 0.30221375823020935, Test_Loss: 0.291884183883667\n",
      "412:Epoch: 19, Train_Loss: 0.289729505777359, Test_Loss: 0.2885605990886688 *\n",
      "413:Epoch: 19, Train_Loss: 0.2891838252544403, Test_Loss: 0.28903928399086\n",
      "414:Epoch: 19, Train_Loss: 0.2901574671268463, Test_Loss: 0.2879502475261688 *\n",
      "415:Epoch: 19, Train_Loss: 0.3251119554042816, Test_Loss: 0.2880450487136841\n",
      "416:Epoch: 19, Train_Loss: 0.32116809487342834, Test_Loss: 0.2887015640735626\n",
      "417:Epoch: 19, Train_Loss: 0.29575544595718384, Test_Loss: 0.2885890603065491 *\n",
      "418:Epoch: 19, Train_Loss: 0.31011784076690674, Test_Loss: 0.2884487211704254 *\n",
      "419:Epoch: 19, Train_Loss: 0.33762651681900024, Test_Loss: 0.28801658749580383 *\n",
      "420:Epoch: 19, Train_Loss: 0.3301250636577606, Test_Loss: 0.28851118683815\n",
      "421:Epoch: 19, Train_Loss: 0.2904294729232788, Test_Loss: 0.28950735926628113\n",
      "422:Epoch: 19, Train_Loss: 0.3205154538154602, Test_Loss: 0.28883740305900574 *\n",
      "423:Epoch: 19, Train_Loss: 0.3005588948726654, Test_Loss: 0.3343501389026642\n",
      "424:Epoch: 19, Train_Loss: 0.29993826150894165, Test_Loss: 0.35916364192962646\n",
      "425:Epoch: 19, Train_Loss: 0.2891157865524292, Test_Loss: 5.76259708404541\n",
      "426:Epoch: 19, Train_Loss: 0.31821489334106445, Test_Loss: 0.3283655047416687 *\n",
      "427:Epoch: 19, Train_Loss: 0.34775489568710327, Test_Loss: 0.2904213070869446 *\n",
      "428:Epoch: 19, Train_Loss: 2.5845530033111572, Test_Loss: 0.3276435434818268\n",
      "429:Epoch: 19, Train_Loss: 3.386063575744629, Test_Loss: 0.3611457645893097\n",
      "430:Epoch: 19, Train_Loss: 0.30444037914276123, Test_Loss: 0.34989413619041443 *\n",
      "431:Epoch: 19, Train_Loss: 0.28810927271842957, Test_Loss: 0.29193809628486633 *\n",
      "432:Epoch: 19, Train_Loss: 0.36181992292404175, Test_Loss: 0.390866756439209\n",
      "433:Epoch: 19, Train_Loss: 0.43131017684936523, Test_Loss: 0.3314734995365143 *\n",
      "434:Epoch: 19, Train_Loss: 0.31286898255348206, Test_Loss: 0.29036393761634827 *\n",
      "435:Epoch: 19, Train_Loss: 0.2916966378688812, Test_Loss: 0.31817030906677246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436:Epoch: 19, Train_Loss: 0.31416988372802734, Test_Loss: 0.3112045228481293 *\n",
      "437:Epoch: 19, Train_Loss: 0.34391558170318604, Test_Loss: 0.2904990315437317 *\n",
      "438:Epoch: 19, Train_Loss: 0.2990841567516327, Test_Loss: 0.3263956606388092\n",
      "439:Epoch: 19, Train_Loss: 0.2969615161418915, Test_Loss: 0.41246750950813293\n",
      "440:Epoch: 19, Train_Loss: 1.2566218376159668, Test_Loss: 0.3275725841522217 *\n",
      "441:Epoch: 19, Train_Loss: 1.3470675945281982, Test_Loss: 0.36790403723716736\n",
      "442:Epoch: 19, Train_Loss: 0.601862907409668, Test_Loss: 0.32568076252937317 *\n",
      "443:Epoch: 19, Train_Loss: 0.36573103070259094, Test_Loss: 0.3474903702735901\n",
      "444:Epoch: 19, Train_Loss: 1.4757580757141113, Test_Loss: 0.31368014216423035 *\n",
      "445:Epoch: 19, Train_Loss: 2.108463764190674, Test_Loss: 0.2960585057735443 *\n",
      "446:Epoch: 19, Train_Loss: 0.390129953622818, Test_Loss: 0.30433332920074463\n",
      "447:Epoch: 19, Train_Loss: 0.30985525250434875, Test_Loss: 0.3211725354194641\n",
      "448:Epoch: 19, Train_Loss: 0.405555784702301, Test_Loss: 0.3165322244167328 *\n",
      "449:Epoch: 19, Train_Loss: 1.0686190128326416, Test_Loss: 0.30440056324005127 *\n",
      "450:Epoch: 19, Train_Loss: 0.9384780526161194, Test_Loss: 0.29305437207221985 *\n",
      "451:Epoch: 19, Train_Loss: 0.2945115268230438, Test_Loss: 0.2938932180404663\n",
      "452:Epoch: 19, Train_Loss: 0.314912885427475, Test_Loss: 0.293680876493454 *\n",
      "453:Epoch: 19, Train_Loss: 0.3055439889431, Test_Loss: 0.3005668818950653\n",
      "454:Epoch: 19, Train_Loss: 0.7221457958221436, Test_Loss: 0.3026217222213745\n",
      "1:Epoch: 20, Train_Loss: 0.3437327742576599, Test_Loss: 0.29736119508743286 *\n",
      "2:Epoch: 20, Train_Loss: 0.3639572858810425, Test_Loss: 0.31072893738746643\n",
      "3:Epoch: 20, Train_Loss: 0.294824481010437, Test_Loss: 0.5458366870880127\n",
      "4:Epoch: 20, Train_Loss: 0.38095933198928833, Test_Loss: 0.5493502616882324\n",
      "5:Epoch: 20, Train_Loss: 0.44874903559684753, Test_Loss: 0.5072523355484009 *\n",
      "6:Epoch: 20, Train_Loss: 0.4090573191642761, Test_Loss: 0.3427974581718445 *\n",
      "7:Epoch: 20, Train_Loss: 0.3412628769874573, Test_Loss: 0.31977379322052 *\n",
      "8:Epoch: 20, Train_Loss: 0.34565404057502747, Test_Loss: 0.2920916974544525 *\n",
      "9:Epoch: 20, Train_Loss: 0.38381099700927734, Test_Loss: 0.3254653811454773\n",
      "10:Epoch: 20, Train_Loss: 0.3302536904811859, Test_Loss: 0.4716973900794983\n",
      "11:Epoch: 20, Train_Loss: 0.40388649702072144, Test_Loss: 0.42347121238708496 *\n",
      "12:Epoch: 20, Train_Loss: 0.4294203519821167, Test_Loss: 0.39616504311561584 *\n",
      "13:Epoch: 20, Train_Loss: 0.3142170011997223, Test_Loss: 0.3630479574203491 *\n",
      "14:Epoch: 20, Train_Loss: 0.41293925046920776, Test_Loss: 0.3402465879917145 *\n",
      "15:Epoch: 20, Train_Loss: 0.40704482793807983, Test_Loss: 0.39853209257125854\n",
      "16:Epoch: 20, Train_Loss: 0.31495165824890137, Test_Loss: 0.41454648971557617\n",
      "17:Epoch: 20, Train_Loss: 0.3018009662628174, Test_Loss: 0.4208870530128479\n",
      "18:Epoch: 20, Train_Loss: 0.2897818982601166, Test_Loss: 0.34257620573043823 *\n",
      "19:Epoch: 20, Train_Loss: 0.2900499999523163, Test_Loss: 0.36927711963653564\n",
      "20:Epoch: 20, Train_Loss: 0.28724488615989685, Test_Loss: 0.32409921288490295 *\n",
      "21:Epoch: 20, Train_Loss: 0.29610475897789, Test_Loss: 0.566300630569458\n",
      "22:Epoch: 20, Train_Loss: 0.32543808221817017, Test_Loss: 0.672700047492981\n",
      "23:Epoch: 20, Train_Loss: 0.31641384959220886, Test_Loss: 0.4832289516925812 *\n",
      "24:Epoch: 20, Train_Loss: 0.35075265169143677, Test_Loss: 0.9263733625411987\n",
      "25:Epoch: 20, Train_Loss: 0.4746944308280945, Test_Loss: 0.4700424075126648 *\n",
      "26:Epoch: 20, Train_Loss: 0.4794071316719055, Test_Loss: 0.4719688892364502\n",
      "27:Epoch: 20, Train_Loss: 0.2923964858055115, Test_Loss: 0.4632960855960846 *\n",
      "28:Epoch: 20, Train_Loss: 0.33173251152038574, Test_Loss: 0.5036503076553345\n",
      "29:Epoch: 20, Train_Loss: 0.4091666340827942, Test_Loss: 0.5362902283668518\n",
      "30:Epoch: 20, Train_Loss: 0.39608389139175415, Test_Loss: 5.970300197601318\n",
      "31:Epoch: 20, Train_Loss: 0.5287288427352905, Test_Loss: 0.8069251179695129 *\n",
      "32:Epoch: 20, Train_Loss: 0.3813788890838623, Test_Loss: 0.31982412934303284 *\n",
      "33:Epoch: 20, Train_Loss: 0.4829949140548706, Test_Loss: 0.3158026933670044 *\n",
      "34:Epoch: 20, Train_Loss: 0.4414340853691101, Test_Loss: 0.31261146068573 *\n",
      "35:Epoch: 20, Train_Loss: 0.3966527581214905, Test_Loss: 0.30260151624679565 *\n",
      "36:Epoch: 20, Train_Loss: 0.3064996898174286, Test_Loss: 0.381402850151062\n",
      "37:Epoch: 20, Train_Loss: 0.301315575838089, Test_Loss: 0.40160804986953735\n",
      "38:Epoch: 20, Train_Loss: 0.37205666303634644, Test_Loss: 0.3120602071285248 *\n",
      "39:Epoch: 20, Train_Loss: 0.97673499584198, Test_Loss: 0.31136271357536316 *\n",
      "40:Epoch: 20, Train_Loss: 0.7388252019882202, Test_Loss: 0.33313867449760437\n",
      "41:Epoch: 20, Train_Loss: 0.30674874782562256, Test_Loss: 0.4409695267677307\n",
      "42:Epoch: 20, Train_Loss: 0.35462215542793274, Test_Loss: 0.38386568427085876 *\n",
      "43:Epoch: 20, Train_Loss: 0.3069971799850464, Test_Loss: 0.3378071188926697 *\n",
      "44:Epoch: 20, Train_Loss: 0.4284055829048157, Test_Loss: 0.36409807205200195\n",
      "45:Epoch: 20, Train_Loss: 0.5638431310653687, Test_Loss: 0.3090280294418335 *\n",
      "46:Epoch: 20, Train_Loss: 0.2910865843296051, Test_Loss: 0.29985302686691284 *\n",
      "47:Epoch: 20, Train_Loss: 0.5107327699661255, Test_Loss: 0.3373190760612488\n",
      "48:Epoch: 20, Train_Loss: 0.31451913714408875, Test_Loss: 0.40366414189338684\n",
      "49:Epoch: 20, Train_Loss: 0.314040869474411, Test_Loss: 0.3229007124900818 *\n",
      "50:Epoch: 20, Train_Loss: 0.32041382789611816, Test_Loss: 0.3065928816795349 *\n",
      "51:Epoch: 20, Train_Loss: 0.3817659914493561, Test_Loss: 0.3192746639251709\n",
      "52:Epoch: 20, Train_Loss: 0.35518261790275574, Test_Loss: 0.38143008947372437\n",
      "53:Epoch: 20, Train_Loss: 0.37839874625205994, Test_Loss: 0.3565593957901001 *\n",
      "54:Epoch: 20, Train_Loss: 0.28828632831573486, Test_Loss: 0.4117901623249054\n",
      "55:Epoch: 20, Train_Loss: 0.34833383560180664, Test_Loss: 0.32784032821655273 *\n",
      "56:Epoch: 20, Train_Loss: 0.33935514092445374, Test_Loss: 0.3229096829891205 *\n",
      "57:Epoch: 20, Train_Loss: 0.3421564996242523, Test_Loss: 0.3393319845199585\n",
      "58:Epoch: 20, Train_Loss: 0.31098756194114685, Test_Loss: 0.30849868059158325 *\n",
      "59:Epoch: 20, Train_Loss: 0.2995275855064392, Test_Loss: 0.3207632303237915\n",
      "60:Epoch: 20, Train_Loss: 0.37181738018989563, Test_Loss: 0.4241674840450287\n",
      "61:Epoch: 20, Train_Loss: 0.46991825103759766, Test_Loss: 1.029527187347412\n",
      "62:Epoch: 20, Train_Loss: 0.5046420097351074, Test_Loss: 5.087762832641602\n",
      "63:Epoch: 20, Train_Loss: 0.6912627220153809, Test_Loss: 0.30827853083610535 *\n",
      "64:Epoch: 20, Train_Loss: 0.5598797798156738, Test_Loss: 0.3243517279624939\n",
      "65:Epoch: 20, Train_Loss: 0.49196767807006836, Test_Loss: 0.3178419768810272 *\n",
      "66:Epoch: 20, Train_Loss: 0.38110774755477905, Test_Loss: 0.29051342606544495 *\n",
      "67:Epoch: 20, Train_Loss: 0.3573192358016968, Test_Loss: 0.29356709122657776\n",
      "68:Epoch: 20, Train_Loss: 0.3066729009151459, Test_Loss: 0.3275088667869568\n",
      "69:Epoch: 20, Train_Loss: 0.29911085963249207, Test_Loss: 0.3722347617149353\n",
      "70:Epoch: 20, Train_Loss: 0.3367244005203247, Test_Loss: 0.30562663078308105 *\n",
      "71:Epoch: 20, Train_Loss: 0.6039927005767822, Test_Loss: 0.2923979163169861 *\n",
      "72:Epoch: 20, Train_Loss: 0.7194223403930664, Test_Loss: 0.3099052309989929\n",
      "73:Epoch: 20, Train_Loss: 1.1187694072723389, Test_Loss: 0.3746280074119568\n",
      "74:Epoch: 20, Train_Loss: 1.4089308977127075, Test_Loss: 0.3217300772666931 *\n",
      "75:Epoch: 20, Train_Loss: 0.3886526823043823, Test_Loss: 0.36284124851226807\n",
      "76:Epoch: 20, Train_Loss: 0.5343648195266724, Test_Loss: 0.3067198693752289 *\n",
      "77:Epoch: 20, Train_Loss: 0.28884080052375793, Test_Loss: 0.39140403270721436\n",
      "78:Epoch: 20, Train_Loss: 0.3371296525001526, Test_Loss: 0.3036545217037201 *\n",
      "79:Epoch: 20, Train_Loss: 0.5645900964736938, Test_Loss: 0.3209758400917053\n",
      "80:Epoch: 20, Train_Loss: 1.1655395030975342, Test_Loss: 0.4136362373828888\n",
      "81:Epoch: 20, Train_Loss: 0.3231735825538635, Test_Loss: 0.33270683884620667 *\n",
      "82:Epoch: 20, Train_Loss: 0.31750789284706116, Test_Loss: 0.4457862675189972\n",
      "83:Epoch: 20, Train_Loss: 0.33079060912132263, Test_Loss: 0.3985000252723694 *\n",
      "84:Epoch: 20, Train_Loss: 0.4457140564918518, Test_Loss: 0.38526153564453125 *\n",
      "85:Epoch: 20, Train_Loss: 0.5219784379005432, Test_Loss: 0.3692680895328522 *\n",
      "86:Epoch: 20, Train_Loss: 0.5610466003417969, Test_Loss: 0.3896391689777374\n",
      "87:Epoch: 20, Train_Loss: 0.484394371509552, Test_Loss: 0.4084394574165344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88:Epoch: 20, Train_Loss: 0.5908075571060181, Test_Loss: 0.41765040159225464\n",
      "89:Epoch: 20, Train_Loss: 0.306479811668396, Test_Loss: 0.31760334968566895 *\n",
      "90:Epoch: 20, Train_Loss: 0.298235148191452, Test_Loss: 0.3816845417022705\n",
      "91:Epoch: 20, Train_Loss: 0.2944398522377014, Test_Loss: 0.3482140004634857 *\n",
      "92:Epoch: 20, Train_Loss: 0.3692695200443268, Test_Loss: 0.30322423577308655 *\n",
      "93:Epoch: 20, Train_Loss: 0.3036400377750397, Test_Loss: 0.35096675157546997\n",
      "94:Epoch: 20, Train_Loss: 0.4156383275985718, Test_Loss: 0.4478389322757721\n",
      "95:Epoch: 20, Train_Loss: 14.838896751403809, Test_Loss: 0.39251816272735596 *\n",
      "96:Epoch: 20, Train_Loss: 0.5045776963233948, Test_Loss: 0.3723265826702118 *\n",
      "97:Epoch: 20, Train_Loss: 1.5745259523391724, Test_Loss: 0.3072463870048523 *\n",
      "98:Epoch: 20, Train_Loss: 1.362275242805481, Test_Loss: 0.3063933551311493 *\n",
      "99:Epoch: 20, Train_Loss: 0.2935657799243927, Test_Loss: 0.2893783152103424 *\n",
      "100:Epoch: 20, Train_Loss: 0.38270825147628784, Test_Loss: 0.3212933838367462\n",
      "Model saved at location ../Saver/model.ckpt at epoch 20\n",
      "101:Epoch: 20, Train_Loss: 2.149458408355713, Test_Loss: 0.4429817199707031\n",
      "102:Epoch: 20, Train_Loss: 5.474179267883301, Test_Loss: 0.49581289291381836\n",
      "103:Epoch: 20, Train_Loss: 0.3808714747428894, Test_Loss: 0.35068145394325256 *\n",
      "104:Epoch: 20, Train_Loss: 0.35563430190086365, Test_Loss: 0.37604162096977234\n",
      "105:Epoch: 20, Train_Loss: 5.1379475593566895, Test_Loss: 0.44796648621559143\n",
      "106:Epoch: 20, Train_Loss: 0.4928416609764099, Test_Loss: 0.5371254682540894\n",
      "107:Epoch: 20, Train_Loss: 0.449693500995636, Test_Loss: 0.920034646987915\n",
      "108:Epoch: 20, Train_Loss: 0.2977771759033203, Test_Loss: 0.601479709148407 *\n",
      "109:Epoch: 20, Train_Loss: 0.39811891317367554, Test_Loss: 0.582991361618042 *\n",
      "110:Epoch: 20, Train_Loss: 0.34567153453826904, Test_Loss: 0.43562954664230347 *\n",
      "111:Epoch: 20, Train_Loss: 0.2824251651763916, Test_Loss: 0.5546468496322632\n",
      "112:Epoch: 20, Train_Loss: 0.30021896958351135, Test_Loss: 1.0596537590026855\n",
      "113:Epoch: 20, Train_Loss: 0.280939519405365, Test_Loss: 1.4767521619796753\n",
      "114:Epoch: 20, Train_Loss: 0.281005859375, Test_Loss: 0.6983872652053833 *\n",
      "115:Epoch: 20, Train_Loss: 0.31452226638793945, Test_Loss: 0.4796396493911743 *\n",
      "116:Epoch: 20, Train_Loss: 0.3376893997192383, Test_Loss: 0.33712655305862427 *\n",
      "117:Epoch: 20, Train_Loss: 0.38820308446884155, Test_Loss: 0.300530344247818 *\n",
      "118:Epoch: 20, Train_Loss: 0.33055830001831055, Test_Loss: 0.2838590741157532 *\n",
      "119:Epoch: 20, Train_Loss: 0.30711379647254944, Test_Loss: 0.40346431732177734\n",
      "120:Epoch: 20, Train_Loss: 0.2890956401824951, Test_Loss: 0.5306984782218933\n",
      "121:Epoch: 20, Train_Loss: 0.2917630076408386, Test_Loss: 7.957368850708008\n",
      "122:Epoch: 20, Train_Loss: 0.2898322343826294, Test_Loss: 0.5909638404846191 *\n",
      "123:Epoch: 20, Train_Loss: 0.3040201663970947, Test_Loss: 0.67243891954422\n",
      "124:Epoch: 20, Train_Loss: 0.28538668155670166, Test_Loss: 0.6741379499435425\n",
      "125:Epoch: 20, Train_Loss: 0.28384634852409363, Test_Loss: 0.5878187417984009 *\n",
      "126:Epoch: 20, Train_Loss: 0.28195756673812866, Test_Loss: 0.4488562047481537 *\n",
      "127:Epoch: 20, Train_Loss: 0.28143802285194397, Test_Loss: 1.1891087293624878\n",
      "128:Epoch: 20, Train_Loss: 0.2848874628543854, Test_Loss: 0.9561483263969421 *\n",
      "129:Epoch: 20, Train_Loss: 0.2828032374382019, Test_Loss: 0.30660903453826904 *\n",
      "130:Epoch: 20, Train_Loss: 0.28288009762763977, Test_Loss: 0.4716000556945801\n",
      "131:Epoch: 20, Train_Loss: 0.29982757568359375, Test_Loss: 0.4269142150878906 *\n",
      "132:Epoch: 20, Train_Loss: 0.32263684272766113, Test_Loss: 1.0651108026504517\n",
      "133:Epoch: 20, Train_Loss: 0.336226224899292, Test_Loss: 0.7697206735610962 *\n",
      "134:Epoch: 20, Train_Loss: 0.4885956645011902, Test_Loss: 1.063377022743225\n",
      "135:Epoch: 20, Train_Loss: 0.3128516972064972, Test_Loss: 0.8816941976547241 *\n",
      "136:Epoch: 20, Train_Loss: 1.0507986545562744, Test_Loss: 0.29429101943969727 *\n",
      "137:Epoch: 20, Train_Loss: 5.135763645172119, Test_Loss: 0.29888060688972473\n",
      "138:Epoch: 20, Train_Loss: 0.39178159832954407, Test_Loss: 0.3242309093475342\n",
      "139:Epoch: 20, Train_Loss: 0.4043788015842438, Test_Loss: 1.2586495876312256\n",
      "140:Epoch: 20, Train_Loss: 0.5162057280540466, Test_Loss: 0.5740542411804199 *\n",
      "141:Epoch: 20, Train_Loss: 0.5978325605392456, Test_Loss: 0.8271355032920837\n",
      "142:Epoch: 20, Train_Loss: 0.48147836327552795, Test_Loss: 0.4304879605770111 *\n",
      "143:Epoch: 20, Train_Loss: 0.4357995390892029, Test_Loss: 0.9479437470436096\n",
      "144:Epoch: 20, Train_Loss: 0.31511053442955017, Test_Loss: 0.5992991924285889 *\n",
      "145:Epoch: 20, Train_Loss: 0.39937669038772583, Test_Loss: 0.4374406933784485 *\n",
      "146:Epoch: 20, Train_Loss: 0.3483569920063019, Test_Loss: 0.3366205096244812 *\n",
      "147:Epoch: 20, Train_Loss: 0.3136967420578003, Test_Loss: 0.30529356002807617 *\n",
      "148:Epoch: 20, Train_Loss: 0.28818559646606445, Test_Loss: 0.314251571893692\n",
      "149:Epoch: 20, Train_Loss: 0.32262900471687317, Test_Loss: 0.28858327865600586 *\n",
      "150:Epoch: 20, Train_Loss: 0.300368994474411, Test_Loss: 0.2904238998889923\n",
      "151:Epoch: 20, Train_Loss: 0.32893288135528564, Test_Loss: 0.38033565878868103\n",
      "152:Epoch: 20, Train_Loss: 0.2923215925693512, Test_Loss: 2.2455129623413086\n",
      "153:Epoch: 20, Train_Loss: 0.3267606794834137, Test_Loss: 3.5574698448181152\n",
      "154:Epoch: 20, Train_Loss: 0.2869202494621277, Test_Loss: 0.288985937833786 *\n",
      "155:Epoch: 20, Train_Loss: 0.3191162347793579, Test_Loss: 0.32453399896621704\n",
      "156:Epoch: 20, Train_Loss: 0.3657088875770569, Test_Loss: 0.3689819872379303\n",
      "157:Epoch: 20, Train_Loss: 0.3099297881126404, Test_Loss: 0.3080452084541321 *\n",
      "158:Epoch: 20, Train_Loss: 0.2821010947227478, Test_Loss: 0.31601813435554504\n",
      "159:Epoch: 20, Train_Loss: 0.2798820436000824, Test_Loss: 0.31092745065689087 *\n",
      "160:Epoch: 20, Train_Loss: 0.28009065985679626, Test_Loss: 0.3423292636871338\n",
      "161:Epoch: 20, Train_Loss: 1.5978319644927979, Test_Loss: 0.28930729627609253 *\n",
      "162:Epoch: 20, Train_Loss: 4.97573709487915, Test_Loss: 0.2960740625858307\n",
      "163:Epoch: 20, Train_Loss: 0.28502070903778076, Test_Loss: 0.3029076159000397\n",
      "164:Epoch: 20, Train_Loss: 0.29711925983428955, Test_Loss: 0.32857751846313477\n",
      "165:Epoch: 20, Train_Loss: 0.29375648498535156, Test_Loss: 0.2939935624599457 *\n",
      "166:Epoch: 20, Train_Loss: 0.28824520111083984, Test_Loss: 0.35451436042785645\n",
      "167:Epoch: 20, Train_Loss: 0.28451892733573914, Test_Loss: 0.3315882086753845 *\n",
      "168:Epoch: 20, Train_Loss: 0.28830328583717346, Test_Loss: 0.35504770278930664\n",
      "169:Epoch: 20, Train_Loss: 0.2910041809082031, Test_Loss: 0.3693287968635559\n",
      "170:Epoch: 20, Train_Loss: 0.34244200587272644, Test_Loss: 0.31224262714385986 *\n",
      "171:Epoch: 20, Train_Loss: 0.297367662191391, Test_Loss: 0.32606416940689087\n",
      "172:Epoch: 20, Train_Loss: 0.288202166557312, Test_Loss: 0.2841978669166565 *\n",
      "173:Epoch: 20, Train_Loss: 0.2864763140678406, Test_Loss: 0.28039515018463135 *\n",
      "174:Epoch: 20, Train_Loss: 0.2810209095478058, Test_Loss: 0.2820713222026825\n",
      "175:Epoch: 20, Train_Loss: 0.30473780632019043, Test_Loss: 0.28518980741500854\n",
      "176:Epoch: 20, Train_Loss: 0.29286786913871765, Test_Loss: 0.28207632899284363 *\n",
      "177:Epoch: 20, Train_Loss: 0.2843775451183319, Test_Loss: 0.284532368183136\n",
      "178:Epoch: 20, Train_Loss: 0.31125566363334656, Test_Loss: 0.28631591796875\n",
      "179:Epoch: 20, Train_Loss: 0.34371137619018555, Test_Loss: 0.2831766903400421 *\n",
      "180:Epoch: 20, Train_Loss: 0.2899858355522156, Test_Loss: 0.28517746925354004\n",
      "181:Epoch: 20, Train_Loss: 0.2788774073123932, Test_Loss: 0.28778913617134094\n",
      "182:Epoch: 20, Train_Loss: 0.30406710505485535, Test_Loss: 0.2851938009262085 *\n",
      "183:Epoch: 20, Train_Loss: 0.394375205039978, Test_Loss: 0.31450486183166504\n",
      "184:Epoch: 20, Train_Loss: 0.3468766212463379, Test_Loss: 0.29257410764694214 *\n",
      "185:Epoch: 20, Train_Loss: 0.3311941921710968, Test_Loss: 0.6407701969146729\n",
      "186:Epoch: 20, Train_Loss: 0.3067779242992401, Test_Loss: 0.6211045980453491 *\n",
      "187:Epoch: 20, Train_Loss: 0.3715607225894928, Test_Loss: 0.37075352668762207 *\n",
      "188:Epoch: 20, Train_Loss: 0.336564302444458, Test_Loss: 0.2872394025325775 *\n",
      "189:Epoch: 20, Train_Loss: 0.35706692934036255, Test_Loss: 0.29562830924987793\n",
      "190:Epoch: 20, Train_Loss: 0.30701035261154175, Test_Loss: 0.3356449007987976\n",
      "191:Epoch: 20, Train_Loss: 0.3944716453552246, Test_Loss: 0.5495129227638245\n",
      "192:Epoch: 20, Train_Loss: 0.2981090545654297, Test_Loss: 0.9499353766441345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193:Epoch: 20, Train_Loss: 0.28013288974761963, Test_Loss: 0.8398003578186035 *\n",
      "194:Epoch: 20, Train_Loss: 0.2810026705265045, Test_Loss: 0.33293023705482483 *\n",
      "195:Epoch: 20, Train_Loss: 0.2789786458015442, Test_Loss: 0.32725030183792114 *\n",
      "196:Epoch: 20, Train_Loss: 0.278621107339859, Test_Loss: 0.2835628092288971 *\n",
      "197:Epoch: 20, Train_Loss: 0.27988776564598083, Test_Loss: 0.2839309573173523\n",
      "198:Epoch: 20, Train_Loss: 1.5254452228546143, Test_Loss: 0.28751060366630554\n",
      "199:Epoch: 20, Train_Loss: 3.98465895652771, Test_Loss: 0.29345977306365967\n",
      "200:Epoch: 20, Train_Loss: 0.280424565076828, Test_Loss: 0.34533751010894775\n",
      "Model saved at location ../Saver/model.ckpt at epoch 20\n",
      "201:Epoch: 20, Train_Loss: 0.2936868965625763, Test_Loss: 0.2804316282272339 *\n",
      "202:Epoch: 20, Train_Loss: 0.2885313928127289, Test_Loss: 0.309732586145401\n",
      "203:Epoch: 20, Train_Loss: 0.2778734266757965, Test_Loss: 0.4127051830291748\n",
      "204:Epoch: 20, Train_Loss: 0.27787548303604126, Test_Loss: 0.6664779186248779\n",
      "205:Epoch: 20, Train_Loss: 0.2782166600227356, Test_Loss: 0.588620662689209 *\n",
      "206:Epoch: 20, Train_Loss: 0.2773577868938446, Test_Loss: 0.3038176894187927 *\n",
      "207:Epoch: 20, Train_Loss: 0.2773384749889374, Test_Loss: 0.2949930727481842 *\n",
      "208:Epoch: 20, Train_Loss: 0.2814006805419922, Test_Loss: 0.29548418521881104\n",
      "209:Epoch: 20, Train_Loss: 0.35142138600349426, Test_Loss: 0.29743558168411255\n",
      "210:Epoch: 20, Train_Loss: 0.3501739501953125, Test_Loss: 0.3291964530944824\n",
      "211:Epoch: 20, Train_Loss: 0.3775351643562317, Test_Loss: 1.0483146905899048\n",
      "212:Epoch: 20, Train_Loss: 0.317891925573349, Test_Loss: 4.88079833984375\n",
      "213:Epoch: 20, Train_Loss: 0.2786778509616852, Test_Loss: 0.305123895406723 *\n",
      "214:Epoch: 20, Train_Loss: 0.4555419981479645, Test_Loss: 0.28491127490997314 *\n",
      "215:Epoch: 20, Train_Loss: 0.4914754629135132, Test_Loss: 0.2841868996620178 *\n",
      "216:Epoch: 20, Train_Loss: 0.4947901964187622, Test_Loss: 0.28842049837112427\n",
      "217:Epoch: 20, Train_Loss: 0.3997851610183716, Test_Loss: 0.28285160660743713 *\n",
      "218:Epoch: 20, Train_Loss: 0.27838295698165894, Test_Loss: 0.28150367736816406 *\n",
      "219:Epoch: 20, Train_Loss: 0.2771355211734772, Test_Loss: 0.2804706394672394 *\n",
      "220:Epoch: 20, Train_Loss: 0.27991339564323425, Test_Loss: 0.27912768721580505 *\n",
      "221:Epoch: 20, Train_Loss: 0.2865038216114044, Test_Loss: 0.27939584851264954\n",
      "222:Epoch: 20, Train_Loss: 0.291886568069458, Test_Loss: 0.27859142422676086 *\n",
      "223:Epoch: 20, Train_Loss: 0.2873852849006653, Test_Loss: 0.2832273542881012\n",
      "224:Epoch: 20, Train_Loss: 0.2782558798789978, Test_Loss: 0.3069429397583008\n",
      "225:Epoch: 20, Train_Loss: 0.2767791152000427, Test_Loss: 0.30772507190704346\n",
      "226:Epoch: 20, Train_Loss: 0.28596779704093933, Test_Loss: 0.28996777534484863 *\n",
      "227:Epoch: 20, Train_Loss: 0.31984439492225647, Test_Loss: 0.2814103066921234 *\n",
      "228:Epoch: 20, Train_Loss: 0.474348783493042, Test_Loss: 0.2785954773426056 *\n",
      "229:Epoch: 20, Train_Loss: 0.4577050507068634, Test_Loss: 0.28530317544937134\n",
      "230:Epoch: 20, Train_Loss: 0.42944031953811646, Test_Loss: 0.27785739302635193 *\n",
      "231:Epoch: 20, Train_Loss: 0.35451269149780273, Test_Loss: 0.2775224447250366 *\n",
      "232:Epoch: 20, Train_Loss: 0.4150390028953552, Test_Loss: 0.27863195538520813\n",
      "233:Epoch: 20, Train_Loss: 0.3489104211330414, Test_Loss: 0.27697134017944336 *\n",
      "234:Epoch: 20, Train_Loss: 0.3957992196083069, Test_Loss: 0.2793850302696228\n",
      "235:Epoch: 20, Train_Loss: 0.3765600323677063, Test_Loss: 0.2777532935142517 *\n",
      "236:Epoch: 20, Train_Loss: 0.5543701648712158, Test_Loss: 0.278008371591568\n",
      "237:Epoch: 20, Train_Loss: 0.2863902151584625, Test_Loss: 0.27828705310821533\n",
      "238:Epoch: 20, Train_Loss: 0.29277798533439636, Test_Loss: 0.277326375246048 *\n",
      "239:Epoch: 20, Train_Loss: 3.1418237686157227, Test_Loss: 0.2805209457874298\n",
      "240:Epoch: 20, Train_Loss: 0.5155783891677856, Test_Loss: 0.2769666314125061 *\n",
      "241:Epoch: 20, Train_Loss: 0.3349519371986389, Test_Loss: 0.29141584038734436\n",
      "242:Epoch: 20, Train_Loss: 0.3561186194419861, Test_Loss: 0.3311072587966919\n",
      "243:Epoch: 20, Train_Loss: 0.3390021026134491, Test_Loss: 3.3354601860046387\n",
      "244:Epoch: 20, Train_Loss: 0.29458558559417725, Test_Loss: 2.5728044509887695 *\n",
      "245:Epoch: 20, Train_Loss: 0.2789500057697296, Test_Loss: 0.280505895614624 *\n",
      "246:Epoch: 20, Train_Loss: 0.333670049905777, Test_Loss: 0.2854083478450775\n",
      "247:Epoch: 20, Train_Loss: 0.39189085364341736, Test_Loss: 0.3530106544494629\n",
      "248:Epoch: 20, Train_Loss: 0.36472442746162415, Test_Loss: 0.33600127696990967 *\n",
      "249:Epoch: 20, Train_Loss: 0.3424094319343567, Test_Loss: 0.31519943475723267 *\n",
      "250:Epoch: 20, Train_Loss: 0.3437228798866272, Test_Loss: 0.326613187789917\n",
      "251:Epoch: 20, Train_Loss: 0.295486181974411, Test_Loss: 0.3430866003036499\n",
      "252:Epoch: 20, Train_Loss: 0.3075348138809204, Test_Loss: 0.28365400433540344 *\n",
      "253:Epoch: 20, Train_Loss: 0.28171345591545105, Test_Loss: 0.29696303606033325\n",
      "254:Epoch: 20, Train_Loss: 0.31642240285873413, Test_Loss: 0.2996852397918701\n",
      "255:Epoch: 20, Train_Loss: 0.2985226809978485, Test_Loss: 0.3094428777694702\n",
      "256:Epoch: 20, Train_Loss: 0.2757261097431183, Test_Loss: 0.2859383523464203 *\n",
      "257:Epoch: 20, Train_Loss: 0.29090580344200134, Test_Loss: 0.3920464813709259\n",
      "258:Epoch: 20, Train_Loss: 0.3180018663406372, Test_Loss: 0.35049906373023987 *\n",
      "259:Epoch: 20, Train_Loss: 0.30662381649017334, Test_Loss: 0.33216458559036255 *\n",
      "260:Epoch: 20, Train_Loss: 0.2787204682826996, Test_Loss: 0.3219665586948395 *\n",
      "261:Epoch: 20, Train_Loss: 0.276608407497406, Test_Loss: 0.3249283730983734\n",
      "262:Epoch: 20, Train_Loss: 0.27834364771842957, Test_Loss: 0.3320150077342987\n",
      "263:Epoch: 20, Train_Loss: 0.2760802209377289, Test_Loss: 0.29645803570747375 *\n",
      "264:Epoch: 20, Train_Loss: 0.2761105000972748, Test_Loss: 0.3006558418273926\n",
      "265:Epoch: 20, Train_Loss: 0.28173261880874634, Test_Loss: 0.3143508732318878\n",
      "266:Epoch: 20, Train_Loss: 0.2813121974468231, Test_Loss: 0.32878363132476807\n",
      "267:Epoch: 20, Train_Loss: 0.28079044818878174, Test_Loss: 0.31808197498321533 *\n",
      "268:Epoch: 20, Train_Loss: 0.27613887190818787, Test_Loss: 0.29755547642707825 *\n",
      "269:Epoch: 20, Train_Loss: 0.2779802978038788, Test_Loss: 0.2988949120044708\n",
      "270:Epoch: 20, Train_Loss: 0.28066661953926086, Test_Loss: 0.2930925488471985 *\n",
      "271:Epoch: 20, Train_Loss: 0.3029836118221283, Test_Loss: 0.30683720111846924\n",
      "272:Epoch: 20, Train_Loss: 0.3113288879394531, Test_Loss: 0.28435248136520386 *\n",
      "273:Epoch: 20, Train_Loss: 0.3030517101287842, Test_Loss: 0.3125561475753784\n",
      "274:Epoch: 20, Train_Loss: 0.29792746901512146, Test_Loss: 0.3618110418319702\n",
      "275:Epoch: 20, Train_Loss: 0.30042096972465515, Test_Loss: 0.36183252930641174\n",
      "276:Epoch: 20, Train_Loss: 0.2758804261684418, Test_Loss: 0.8426734209060669\n",
      "277:Epoch: 20, Train_Loss: 0.2772773504257202, Test_Loss: 0.7618381977081299 *\n",
      "278:Epoch: 20, Train_Loss: 0.3005768656730652, Test_Loss: 0.41410425305366516 *\n",
      "279:Epoch: 20, Train_Loss: 0.2993928790092468, Test_Loss: 0.3151682913303375 *\n",
      "280:Epoch: 20, Train_Loss: 0.2754741311073303, Test_Loss: 0.29798421263694763 *\n",
      "281:Epoch: 20, Train_Loss: 0.28216975927352905, Test_Loss: 0.3168776333332062\n",
      "282:Epoch: 20, Train_Loss: 0.2781898081302643, Test_Loss: 0.5745617151260376\n",
      "283:Epoch: 20, Train_Loss: 0.3055877089500427, Test_Loss: 0.9281669855117798\n",
      "284:Epoch: 20, Train_Loss: 0.3211328387260437, Test_Loss: 0.683459997177124 *\n",
      "285:Epoch: 20, Train_Loss: 0.33061647415161133, Test_Loss: 0.3886547088623047 *\n",
      "286:Epoch: 20, Train_Loss: 0.28966495394706726, Test_Loss: 0.31374654173851013 *\n",
      "287:Epoch: 20, Train_Loss: 0.2800936698913574, Test_Loss: 0.2811500132083893 *\n",
      "288:Epoch: 20, Train_Loss: 0.3554605543613434, Test_Loss: 0.2744901180267334 *\n",
      "289:Epoch: 20, Train_Loss: 0.277151882648468, Test_Loss: 0.2892295718193054\n",
      "290:Epoch: 20, Train_Loss: 0.28379249572753906, Test_Loss: 0.2864812910556793 *\n",
      "291:Epoch: 20, Train_Loss: 0.29675164818763733, Test_Loss: 0.31737780570983887\n",
      "292:Epoch: 20, Train_Loss: 0.2900129556655884, Test_Loss: 0.28060755133628845 *\n",
      "293:Epoch: 20, Train_Loss: 0.388876736164093, Test_Loss: 0.3538435101509094\n",
      "294:Epoch: 20, Train_Loss: 0.32949191331863403, Test_Loss: 0.41218042373657227\n",
      "295:Epoch: 20, Train_Loss: 0.2979152500629425, Test_Loss: 0.6798759698867798\n",
      "296:Epoch: 20, Train_Loss: 0.2827170491218567, Test_Loss: 0.6042653322219849 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297:Epoch: 20, Train_Loss: 0.2966076135635376, Test_Loss: 0.31782421469688416 *\n",
      "298:Epoch: 20, Train_Loss: 0.2771865427494049, Test_Loss: 0.30983421206474304 *\n",
      "299:Epoch: 20, Train_Loss: 0.27708837389945984, Test_Loss: 0.3085438311100006 *\n",
      "300:Epoch: 20, Train_Loss: 0.2884155511856079, Test_Loss: 0.3087061941623688\n",
      "Model saved at location ../Saver/model.ckpt at epoch 20\n",
      "301:Epoch: 20, Train_Loss: 0.294241338968277, Test_Loss: 0.3479263186454773\n",
      "302:Epoch: 20, Train_Loss: 0.333354651927948, Test_Loss: 2.144441604614258\n",
      "303:Epoch: 20, Train_Loss: 0.35860371589660645, Test_Loss: 3.687833547592163\n",
      "304:Epoch: 20, Train_Loss: 0.2955453395843506, Test_Loss: 0.28760045766830444 *\n",
      "305:Epoch: 20, Train_Loss: 0.3105344772338867, Test_Loss: 0.2808469235897064 *\n",
      "306:Epoch: 20, Train_Loss: 0.3046916723251343, Test_Loss: 0.2801405191421509 *\n",
      "307:Epoch: 20, Train_Loss: 0.28937309980392456, Test_Loss: 0.2864769697189331\n",
      "308:Epoch: 20, Train_Loss: 0.378328412771225, Test_Loss: 0.2777242958545685 *\n",
      "309:Epoch: 20, Train_Loss: 0.5096054673194885, Test_Loss: 0.27952390909194946\n",
      "310:Epoch: 20, Train_Loss: 0.2789994180202484, Test_Loss: 0.2744300365447998 *\n",
      "311:Epoch: 20, Train_Loss: 0.30990496277809143, Test_Loss: 0.27423322200775146 *\n",
      "312:Epoch: 20, Train_Loss: 0.2730760872364044, Test_Loss: 0.27468305826187134\n",
      "313:Epoch: 20, Train_Loss: 0.2731305956840515, Test_Loss: 0.2736642062664032 *\n",
      "314:Epoch: 20, Train_Loss: 0.2741173207759857, Test_Loss: 0.2799498438835144\n",
      "315:Epoch: 20, Train_Loss: 0.2734573781490326, Test_Loss: 0.3174857497215271\n",
      "316:Epoch: 20, Train_Loss: 0.2813080847263336, Test_Loss: 0.3134930729866028 *\n",
      "317:Epoch: 20, Train_Loss: 0.289915531873703, Test_Loss: 0.28129124641418457 *\n",
      "318:Epoch: 20, Train_Loss: 0.28232428431510925, Test_Loss: 0.2781824767589569 *\n",
      "319:Epoch: 20, Train_Loss: 0.28778529167175293, Test_Loss: 0.27423080801963806 *\n",
      "320:Epoch: 20, Train_Loss: 0.28778505325317383, Test_Loss: 0.28070223331451416\n",
      "321:Epoch: 20, Train_Loss: 0.2745221257209778, Test_Loss: 0.2744443118572235 *\n",
      "322:Epoch: 20, Train_Loss: 0.2745799720287323, Test_Loss: 0.2736131250858307 *\n",
      "323:Epoch: 20, Train_Loss: 0.27221184968948364, Test_Loss: 0.273393839597702 *\n",
      "324:Epoch: 20, Train_Loss: 0.3044039309024811, Test_Loss: 0.2724177837371826 *\n",
      "325:Epoch: 20, Train_Loss: 0.29937800765037537, Test_Loss: 0.27439627051353455\n",
      "326:Epoch: 20, Train_Loss: 0.28982922434806824, Test_Loss: 0.2731758654117584 *\n",
      "327:Epoch: 20, Train_Loss: 0.28142887353897095, Test_Loss: 0.2735225260257721\n",
      "328:Epoch: 20, Train_Loss: 0.3195110261440277, Test_Loss: 0.27419671416282654\n",
      "329:Epoch: 20, Train_Loss: 0.312130868434906, Test_Loss: 0.2733447253704071 *\n",
      "330:Epoch: 20, Train_Loss: 0.2902385890483856, Test_Loss: 0.27572542428970337\n",
      "331:Epoch: 20, Train_Loss: 0.29210227727890015, Test_Loss: 0.2727491855621338 *\n",
      "332:Epoch: 20, Train_Loss: 0.2995971441268921, Test_Loss: 0.3058232069015503\n",
      "333:Epoch: 20, Train_Loss: 0.2776925265789032, Test_Loss: 0.3135796785354614\n",
      "334:Epoch: 20, Train_Loss: 0.2814632058143616, Test_Loss: 4.509574890136719\n",
      "335:Epoch: 20, Train_Loss: 0.3010023534297943, Test_Loss: 1.5482137203216553 *\n",
      "336:Epoch: 20, Train_Loss: 0.3155045211315155, Test_Loss: 0.2762046456336975 *\n",
      "337:Epoch: 20, Train_Loss: 2.4023563861846924, Test_Loss: 0.292818158864975\n",
      "338:Epoch: 20, Train_Loss: 3.4946367740631104, Test_Loss: 0.344418466091156\n",
      "339:Epoch: 20, Train_Loss: 0.2799431383609772, Test_Loss: 0.3306586742401123 *\n",
      "340:Epoch: 20, Train_Loss: 0.2804355025291443, Test_Loss: 0.28688064217567444 *\n",
      "341:Epoch: 20, Train_Loss: 0.3085085451602936, Test_Loss: 0.34228113293647766\n",
      "342:Epoch: 20, Train_Loss: 0.4356984496116638, Test_Loss: 0.33856070041656494 *\n",
      "343:Epoch: 20, Train_Loss: 0.3000084161758423, Test_Loss: 0.27683910727500916 *\n",
      "344:Epoch: 20, Train_Loss: 0.2803698778152466, Test_Loss: 0.29967445135116577\n",
      "345:Epoch: 20, Train_Loss: 0.2758867144584656, Test_Loss: 0.2958652377128601 *\n",
      "346:Epoch: 20, Train_Loss: 0.3449556827545166, Test_Loss: 0.2878381311893463 *\n",
      "347:Epoch: 20, Train_Loss: 0.2813265323638916, Test_Loss: 0.27882829308509827 *\n",
      "348:Epoch: 20, Train_Loss: 0.28739017248153687, Test_Loss: 0.3533119261264801\n",
      "349:Epoch: 20, Train_Loss: 0.8858355283737183, Test_Loss: 0.3155248165130615 *\n",
      "350:Epoch: 20, Train_Loss: 1.128199815750122, Test_Loss: 0.3524529039859772\n",
      "351:Epoch: 20, Train_Loss: 0.7685962915420532, Test_Loss: 0.3107665777206421 *\n",
      "352:Epoch: 20, Train_Loss: 0.33434635400772095, Test_Loss: 0.331633061170578\n",
      "353:Epoch: 20, Train_Loss: 0.8817081451416016, Test_Loss: 0.3105615973472595 *\n",
      "354:Epoch: 20, Train_Loss: 2.0189738273620605, Test_Loss: 0.27612122893333435 *\n",
      "355:Epoch: 20, Train_Loss: 0.643694281578064, Test_Loss: 0.28809961676597595\n",
      "356:Epoch: 20, Train_Loss: 0.2870856821537018, Test_Loss: 0.3002778887748718\n",
      "357:Epoch: 20, Train_Loss: 0.29983270168304443, Test_Loss: 0.3216625154018402\n",
      "358:Epoch: 20, Train_Loss: 0.8209916353225708, Test_Loss: 0.300293892621994 *\n",
      "359:Epoch: 20, Train_Loss: 0.7841664552688599, Test_Loss: 0.2740497291088104 *\n",
      "360:Epoch: 20, Train_Loss: 0.4488460123538971, Test_Loss: 0.2759096324443817\n",
      "361:Epoch: 20, Train_Loss: 0.2885715663433075, Test_Loss: 0.289276123046875\n",
      "362:Epoch: 20, Train_Loss: 0.27580708265304565, Test_Loss: 0.28332898020744324 *\n",
      "363:Epoch: 20, Train_Loss: 0.6309919357299805, Test_Loss: 0.30381593108177185\n",
      "364:Epoch: 20, Train_Loss: 0.44418686628341675, Test_Loss: 0.27616074681282043 *\n",
      "365:Epoch: 20, Train_Loss: 0.3328188955783844, Test_Loss: 0.32305800914764404\n",
      "366:Epoch: 20, Train_Loss: 0.2952471673488617, Test_Loss: 0.5173612833023071\n",
      "367:Epoch: 20, Train_Loss: 0.34531083703041077, Test_Loss: 0.5198573470115662\n",
      "368:Epoch: 20, Train_Loss: 0.44768640398979187, Test_Loss: 0.5567050576210022\n",
      "369:Epoch: 20, Train_Loss: 0.3957855701446533, Test_Loss: 0.3521031439304352 *\n",
      "370:Epoch: 20, Train_Loss: 0.39085447788238525, Test_Loss: 0.2912042438983917 *\n",
      "371:Epoch: 20, Train_Loss: 0.3494875133037567, Test_Loss: 0.2844265401363373 *\n",
      "372:Epoch: 20, Train_Loss: 0.3579394817352295, Test_Loss: 0.3326992988586426\n",
      "373:Epoch: 20, Train_Loss: 0.3396017849445343, Test_Loss: 0.525407075881958\n",
      "374:Epoch: 20, Train_Loss: 0.3820386528968811, Test_Loss: 0.41678953170776367 *\n",
      "375:Epoch: 20, Train_Loss: 0.4093935489654541, Test_Loss: 0.546763002872467\n",
      "376:Epoch: 20, Train_Loss: 0.3469124734401703, Test_Loss: 0.34805750846862793 *\n",
      "377:Epoch: 20, Train_Loss: 0.4045482873916626, Test_Loss: 0.35231584310531616\n",
      "378:Epoch: 20, Train_Loss: 0.40797773003578186, Test_Loss: 0.33315858244895935 *\n",
      "379:Epoch: 20, Train_Loss: 0.31393635272979736, Test_Loss: 0.35576140880584717\n",
      "380:Epoch: 20, Train_Loss: 0.2824387550354004, Test_Loss: 0.40379106998443604\n",
      "381:Epoch: 20, Train_Loss: 0.27180197834968567, Test_Loss: 0.2853383421897888 *\n",
      "382:Epoch: 20, Train_Loss: 0.2724955379962921, Test_Loss: 0.3172949552536011\n",
      "383:Epoch: 20, Train_Loss: 0.2750859260559082, Test_Loss: 0.3096013367176056 *\n",
      "384:Epoch: 20, Train_Loss: 0.2819828987121582, Test_Loss: 0.5336152911186218\n",
      "385:Epoch: 20, Train_Loss: 0.3128504753112793, Test_Loss: 0.6456925868988037\n",
      "386:Epoch: 20, Train_Loss: 0.31410354375839233, Test_Loss: 0.4731229245662689 *\n",
      "387:Epoch: 20, Train_Loss: 0.3393668532371521, Test_Loss: 1.115435242652893\n",
      "388:Epoch: 20, Train_Loss: 0.42604419589042664, Test_Loss: 0.5568528175354004 *\n",
      "389:Epoch: 20, Train_Loss: 0.4022284746170044, Test_Loss: 0.5522096157073975 *\n",
      "390:Epoch: 20, Train_Loss: 0.28834328055381775, Test_Loss: 0.5394448041915894 *\n",
      "391:Epoch: 20, Train_Loss: 0.30869004130363464, Test_Loss: 0.5301104784011841 *\n",
      "392:Epoch: 20, Train_Loss: 0.3572179675102234, Test_Loss: 0.7078628540039062\n",
      "393:Epoch: 20, Train_Loss: 0.40982794761657715, Test_Loss: 3.7380189895629883\n",
      "394:Epoch: 20, Train_Loss: 0.45402342081069946, Test_Loss: 2.4724602699279785 *\n",
      "395:Epoch: 20, Train_Loss: 0.3299644887447357, Test_Loss: 0.3111608028411865 *\n",
      "396:Epoch: 20, Train_Loss: 0.36760443449020386, Test_Loss: 0.2993561625480652 *\n",
      "397:Epoch: 20, Train_Loss: 0.4840388298034668, Test_Loss: 0.3215833008289337\n",
      "398:Epoch: 20, Train_Loss: 0.35472437739372253, Test_Loss: 0.27721408009529114 *\n",
      "399:Epoch: 20, Train_Loss: 0.3309137225151062, Test_Loss: 0.3584800660610199\n",
      "400:Epoch: 20, Train_Loss: 0.2853725850582123, Test_Loss: 0.3997373580932617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at location ../Saver/model.ckpt at epoch 20\n",
      "401:Epoch: 20, Train_Loss: 0.3494536280632019, Test_Loss: 0.3064066171646118 *\n",
      "402:Epoch: 20, Train_Loss: 0.6320288181304932, Test_Loss: 0.28690603375434875 *\n",
      "403:Epoch: 20, Train_Loss: 0.7695275545120239, Test_Loss: 0.30480143427848816\n",
      "404:Epoch: 20, Train_Loss: 0.29746031761169434, Test_Loss: 0.3384074866771698\n",
      "405:Epoch: 20, Train_Loss: 0.31914982199668884, Test_Loss: 0.4237174987792969\n",
      "406:Epoch: 20, Train_Loss: 0.2845500707626343, Test_Loss: 0.3128739893436432 *\n",
      "407:Epoch: 20, Train_Loss: 0.29596641659736633, Test_Loss: 0.3373091220855713\n",
      "408:Epoch: 20, Train_Loss: 0.48750531673431396, Test_Loss: 0.32337522506713867 *\n",
      "409:Epoch: 20, Train_Loss: 0.27920007705688477, Test_Loss: 0.2979128360748291 *\n",
      "410:Epoch: 20, Train_Loss: 0.41696596145629883, Test_Loss: 0.3292555510997772\n",
      "411:Epoch: 20, Train_Loss: 0.35896891355514526, Test_Loss: 0.396112322807312\n",
      "412:Epoch: 20, Train_Loss: 0.3237616717815399, Test_Loss: 0.3479437530040741 *\n",
      "413:Epoch: 20, Train_Loss: 0.31159651279449463, Test_Loss: 0.2960693836212158 *\n",
      "414:Epoch: 20, Train_Loss: 0.369617760181427, Test_Loss: 0.3064543902873993\n",
      "415:Epoch: 20, Train_Loss: 0.3591773509979248, Test_Loss: 0.3295024335384369\n",
      "416:Epoch: 20, Train_Loss: 0.2984630763530731, Test_Loss: 0.36027345061302185\n",
      "417:Epoch: 20, Train_Loss: 0.3035988509654999, Test_Loss: 0.40491074323654175\n",
      "418:Epoch: 20, Train_Loss: 0.2932155430316925, Test_Loss: 0.34365254640579224 *\n",
      "419:Epoch: 20, Train_Loss: 0.3619687557220459, Test_Loss: 0.3041643798351288 *\n",
      "420:Epoch: 20, Train_Loss: 0.33231231570243835, Test_Loss: 0.3133382499217987\n",
      "421:Epoch: 20, Train_Loss: 0.3202417194843292, Test_Loss: 0.3108243942260742 *\n",
      "422:Epoch: 20, Train_Loss: 0.28947293758392334, Test_Loss: 0.28573405742645264 *\n",
      "423:Epoch: 20, Train_Loss: 0.32584816217422485, Test_Loss: 0.3985667824745178\n",
      "424:Epoch: 20, Train_Loss: 0.5108000636100769, Test_Loss: 0.3964938521385193 *\n",
      "425:Epoch: 20, Train_Loss: 0.5576335787773132, Test_Loss: 5.999410152435303\n",
      "426:Epoch: 20, Train_Loss: 0.6811302304267883, Test_Loss: 0.5360062122344971 *\n",
      "427:Epoch: 20, Train_Loss: 0.5469719171524048, Test_Loss: 0.30939510464668274 *\n",
      "428:Epoch: 20, Train_Loss: 0.4863119125366211, Test_Loss: 0.31360435485839844\n",
      "429:Epoch: 20, Train_Loss: 0.42650651931762695, Test_Loss: 0.2742998003959656 *\n",
      "430:Epoch: 20, Train_Loss: 0.34912192821502686, Test_Loss: 0.2752126157283783\n",
      "431:Epoch: 20, Train_Loss: 0.28471195697784424, Test_Loss: 0.3015914261341095\n",
      "432:Epoch: 20, Train_Loss: 0.289898157119751, Test_Loss: 0.3597945272922516\n",
      "433:Epoch: 20, Train_Loss: 0.31129908561706543, Test_Loss: 0.3176594376564026 *\n",
      "434:Epoch: 20, Train_Loss: 0.5402925610542297, Test_Loss: 0.2738056778907776 *\n",
      "435:Epoch: 20, Train_Loss: 0.6207393407821655, Test_Loss: 0.285855770111084\n",
      "436:Epoch: 20, Train_Loss: 0.8047293424606323, Test_Loss: 0.3181915581226349\n",
      "437:Epoch: 20, Train_Loss: 1.3675471544265747, Test_Loss: 0.3021495044231415 *\n",
      "438:Epoch: 20, Train_Loss: 0.48647719621658325, Test_Loss: 0.30222687125205994\n",
      "439:Epoch: 20, Train_Loss: 0.4937050938606262, Test_Loss: 0.2753787040710449 *\n",
      "440:Epoch: 20, Train_Loss: 0.27782702445983887, Test_Loss: 0.3622349500656128\n",
      "441:Epoch: 20, Train_Loss: 0.2868822515010834, Test_Loss: 0.32556986808776855 *\n",
      "442:Epoch: 20, Train_Loss: 0.49672335386276245, Test_Loss: 0.3021482229232788 *\n",
      "443:Epoch: 20, Train_Loss: 1.0569524765014648, Test_Loss: 0.3581748604774475\n",
      "444:Epoch: 20, Train_Loss: 0.35517892241477966, Test_Loss: 0.31404608488082886 *\n",
      "445:Epoch: 20, Train_Loss: 0.313264399766922, Test_Loss: 0.44223570823669434\n",
      "446:Epoch: 20, Train_Loss: 0.3042179346084595, Test_Loss: 0.3746756315231323 *\n",
      "447:Epoch: 20, Train_Loss: 0.3450568616390228, Test_Loss: 0.40659719705581665\n",
      "448:Epoch: 20, Train_Loss: 0.5723326206207275, Test_Loss: 0.3694216310977936 *\n",
      "449:Epoch: 20, Train_Loss: 0.5195646286010742, Test_Loss: 0.38580965995788574\n",
      "450:Epoch: 20, Train_Loss: 0.43092817068099976, Test_Loss: 0.46168291568756104\n",
      "451:Epoch: 20, Train_Loss: 0.5437185764312744, Test_Loss: 0.394351065158844 *\n",
      "452:Epoch: 20, Train_Loss: 0.3022761940956116, Test_Loss: 0.3472190499305725 *\n",
      "453:Epoch: 20, Train_Loss: 0.2872644364833832, Test_Loss: 0.3254775106906891 *\n",
      "454:Epoch: 20, Train_Loss: 0.2931024134159088, Test_Loss: 0.35874930024147034\n"
     ]
    }
   ],
   "source": [
    "SAVEDIR = \"../Saver/\"\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "train_vars = tf.trainable_variables() #it will return all the variables. Here, all the weights and biases are variables which\n",
    "#are trainable.\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_true, y_predicted))) + tf.add_n([tf.nn.l2_loss(w) for w in train_vars]) * L2NormConst\n",
    "#since this is a regression problem so above loss is mean-squared-error loss\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = 10**-4).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "epoch_number, train_loss, test_loss,  = [], [], []\n",
    "print(len(x)/batch_size)\n",
    "for epoch in range(epochs):\n",
    "    train_avg_loss = 0\n",
    "    test_avg_loss = 0\n",
    "    te_loss_old = 10000  #any big number can be given\n",
    "    \n",
    "    for i in range(int(len(x)/batch_size)):\n",
    "        train_batch_x, train_batch_y = loadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 0.8})\n",
    "        tr_loss = loss.eval(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 1.0})\n",
    "        train_avg_loss += tr_loss / batch_size\n",
    "    \n",
    "        test_batch_x, test_batch_y = loadTestBatch(batch_size)\n",
    "        te_loss_new = loss.eval(feed_dict = {x_input: test_batch_x, y_true: test_batch_y, keep_prob: 1.0})\n",
    "        test_avg_loss += te_loss_new / batch_size\n",
    "        \n",
    "        if te_loss_new < te_loss_old:\n",
    "            print(\"{}:Epoch: {}, Train_Loss: {}, Test_Loss: {} *\".format(i+1,epoch+1, tr_loss, te_loss_new))\n",
    "        else:\n",
    "            print(\"{}:Epoch: {}, Train_Loss: {}, Test_Loss: {}\".format(i+1,epoch+1, tr_loss, te_loss_new))\n",
    "        te_loss_old = te_loss_new\n",
    "        \n",
    "        if (i+1) % batch_size == 0:\n",
    "            if not os.path.exists(SAVEDIR):\n",
    "                os.makedirs(SAVEDIR)\n",
    "            save_path = os.path.join(SAVEDIR, \"model.ckpt\")\n",
    "            saver.save(sess = sess, save_path = save_path)\n",
    "            print(\"Model saved at location {} at epoch {}\".format(save_path, epoch + 1))\n",
    "        \n",
    "    epoch_number.append(epoch)\n",
    "    train_loss.append(train_avg_loss)\n",
    "    test_loss.append(test_avg_loss)\n",
    "    \n",
    "#creating dataframe and record all the losses and accuracies at each epoch\n",
    "log_frame = pd.DataFrame(columns = [\"Epoch\", \"Train Loss\", \"Test Loss\"])\n",
    "log_frame[\"Epoch\"] = epoch_number\n",
    "log_frame[\"Train Loss\"] = train_loss\n",
    "log_frame[\"Test Loss\"] = test_loss\n",
    "log_frame.to_csv(os.path.join(SAVEDIR, \"log.csv\"), index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAVEDIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b6b495a05a9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSAVEDIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"log.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SAVEDIR' is not defined"
     ]
    }
   ],
   "source": [
    "frame = pd.read_csv(os.path.join(SAVEDIR, \"log.csv\"))\n",
    "print(frame)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making Predictions from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Saver/model.ckpt\n",
      "8.594136749587319\n",
      "23.143932078048127\n",
      "27.440312311282906\n",
      "25.662977560935115\n",
      "29.511922346926113\n",
      "35.927044336171775\n",
      "36.12341568990423\n",
      "29.65385367787898\n",
      "25.74112175522923\n",
      "19.287610687753514\n",
      "19.36888823132937\n",
      "26.822542191422762\n",
      "32.09402413623066\n",
      "24.707234313017107\n",
      "39.055533938320664\n",
      "10.95800204207891\n",
      "18.539208077374997\n",
      "15.64124248235592\n",
      "10.678442399350288\n",
      "19.688672565627492\n",
      "12.059940366539161\n",
      "-3.8094923326845374\n",
      "14.865102473757984\n",
      "0.620931643672213\n",
      "-5.014590419274077\n",
      "14.874331766873963\n",
      "3.2387058034644425\n",
      "1.2752189430430063\n",
      "-2.598510891898678\n",
      "-4.693161716933277\n",
      "-0.20597075772408138\n",
      "0.9077586076777284\n",
      "1.014856614193763\n",
      "1.8752371878068748\n",
      "-0.5354001729039052\n",
      "3.849690557271001\n",
      "-0.32766722637396045\n",
      "0.2865546102123612\n",
      "-0.45611960588662837\n",
      "0.8314525880439922\n",
      "-0.49182271222557755\n",
      "-0.14702409075275572\n",
      "2.253449735029634\n",
      "0.14458144435082992\n",
      "-1.6102431369267167\n",
      "0.23565732117789556\n",
      "-0.11766708392639466\n",
      "1.6397647086233929\n",
      "2.278624745093243\n",
      "0.8327381577118356\n",
      "5.464169265256772\n",
      "-3.227920507878169\n",
      "11.438650991709169\n",
      "1.3299481816452607\n",
      "0.6397851002155638\n",
      "14.221777201118238\n",
      "-3.8803179792829847\n",
      "12.329410112316383\n",
      "14.179570047142144\n",
      "11.990188767187698\n",
      "10.106344046922727\n",
      "14.628514966192498\n",
      "18.43760389082918\n",
      "16.63989639373453\n",
      "14.611492427233532\n",
      "17.06614800926752\n",
      "17.937886760583538\n",
      "19.219081692263483\n",
      "19.57707922742053\n",
      "15.042088043078646\n",
      "13.331381788084327\n",
      "19.80537488533362\n",
      "18.2288033003546\n",
      "14.92607044983681\n",
      "24.17090907636749\n",
      "32.76364563717495\n",
      "37.20317639012945\n",
      "39.0062814442157\n",
      "42.13189212162943\n",
      "35.819291271825655\n",
      "30.75394102107416\n",
      "26.176507041229815\n",
      "28.147872097972037\n",
      "28.58766797862916\n",
      "27.538649959858166\n",
      "29.478232438845026\n",
      "28.892560792989357\n",
      "32.266769865624035\n",
      "31.87204640329983\n",
      "34.152110397317244\n",
      "32.25616941203218\n",
      "30.329355971698657\n",
      "27.405270025746155\n",
      "26.009066661274247\n",
      "27.680564215338105\n",
      "25.705102752641167\n",
      "21.49639482759267\n",
      "21.825399917270307\n",
      "23.60245633740942\n",
      "24.637784949536417\n",
      "25.911172974995036\n",
      "28.70664304378161\n",
      "26.42262095514018\n",
      "28.74423640497336\n",
      "28.249886095868057\n",
      "27.816703545874798\n",
      "26.555686700550364\n",
      "26.76805606486628\n",
      "24.632455694436516\n",
      "24.598868239192978\n",
      "26.051603371877793\n",
      "25.895135690823846\n",
      "26.931538350197826\n",
      "28.69743936387502\n",
      "27.437144811055315\n",
      "26.835379531967803\n",
      "27.494542305745515\n",
      "28.05908476140375\n",
      "27.083264172420492\n",
      "24.59659720129395\n",
      "26.114412083937935\n",
      "27.950735763052545\n",
      "24.425360943707144\n",
      "24.882713825625782\n",
      "25.361059293958455\n",
      "25.99890675488385\n",
      "28.19979178094789\n",
      "27.48691298444261\n",
      "28.89297060433956\n",
      "28.696138212838132\n",
      "28.04740684547032\n",
      "28.250102954374203\n",
      "29.178824266388347\n",
      "28.256147671789666\n",
      "27.981323057703154\n",
      "26.524472736043407\n",
      "25.034316704443384\n",
      "24.610032183391365\n",
      "27.441803000069264\n",
      "26.43528412586138\n",
      "24.83733063368563\n",
      "23.253361953835487\n",
      "26.02226087920342\n",
      "27.13394759115657\n",
      "28.415335475680568\n",
      "29.668203905325175\n",
      "29.740709778459447\n",
      "28.610808659537163\n",
      "26.661252396814792\n",
      "25.054506743629943\n",
      "26.884966705342094\n",
      "27.153516083128657\n",
      "26.692831776442343\n",
      "24.701218623905618\n",
      "24.472598532006952\n",
      "25.258382767712945\n",
      "26.338708666091986\n",
      "27.732938105893762\n",
      "25.125096748702024\n",
      "27.079830294815267\n",
      "30.019026326759125\n",
      "30.115601786528522\n",
      "31.013150115170728\n",
      "26.13189907576046\n",
      "27.35205943701717\n",
      "26.69957488070044\n",
      "22.442061838937615\n",
      "23.31700224142708\n",
      "21.47475849584936\n",
      "20.81541984470052\n",
      "18.688124984301492\n",
      "17.40884592068374\n",
      "17.15305704381405\n",
      "19.58155300132689\n",
      "20.700069902450053\n",
      "20.513861870202582\n",
      "21.839774050378598\n",
      "22.115225334321458\n",
      "22.710566820494574\n",
      "21.872133779118826\n",
      "22.547315054047512\n",
      "23.72580784627256\n",
      "24.310624010934685\n",
      "27.33685202083014\n",
      "26.88011897857868\n",
      "28.36265422661226\n",
      "25.094333576680288\n",
      "23.59690851625608\n",
      "25.28965649637514\n",
      "28.57590810042569\n",
      "29.013509782811944\n",
      "29.16922785060448\n",
      "28.522335511670693\n",
      "30.3521688035265\n",
      "30.18398564049869\n",
      "29.47932526911223\n",
      "31.137244407106103\n",
      "30.318598423755887\n",
      "26.133700538154052\n",
      "27.1392478179525\n",
      "26.896840989214162\n",
      "27.25733325096556\n",
      "30.477041737027257\n",
      "29.77571108286118\n",
      "29.47977606159745\n",
      "31.08361717683775\n",
      "30.33454350037328\n",
      "28.836170751201735\n",
      "24.74310305144343\n",
      "19.937701262766154\n",
      "21.436907295016443\n",
      "20.719474469882062\n",
      "18.371989678567452\n",
      "21.435602728884973\n",
      "19.804121545620923\n",
      "19.1173323641978\n",
      "19.749114617140226\n",
      "20.628276076536757\n",
      "20.19390507362792\n",
      "20.112700954585637\n",
      "18.98225854317163\n",
      "19.696015018985253\n",
      "20.994785734946934\n",
      "20.706271714216427\n",
      "21.482167543551533\n",
      "20.915236229231088\n",
      "19.195549983025494\n",
      "18.242974235335083\n",
      "15.890583660649114\n",
      "10.787897888347034\n",
      "14.703208207408464\n",
      "14.425108517614932\n",
      "12.464077537086938\n",
      "15.878231263535145\n",
      "13.9697585886704\n",
      "14.123770816717725\n",
      "13.631179281323698\n",
      "12.209053641404296\n",
      "11.732702588309266\n",
      "8.824590747278135\n",
      "9.765593593193607\n",
      "5.2790156434624205\n",
      "14.28980930034615\n",
      "7.51153981330925\n",
      "7.6174999529982435\n",
      "1.78044675528838\n",
      "2.017273319474817\n",
      "1.434119878988768\n",
      "0.9908356926564744\n",
      "1.234654663005712\n",
      "1.5052665444782058\n",
      "0.9055131829880867\n",
      "0.5682589323403671\n",
      "0.5202862449311363\n",
      "0.2974397973152459\n",
      "-0.06593480426589497\n",
      "0.1475086072970035\n",
      "-0.27083450295075373\n",
      "0.06396984421904692\n",
      "0.046249345304271966\n",
      "0.32144321649278634\n",
      "-0.1227858838224961\n",
      "0.03610139174492552\n",
      "1.0075545016407847\n",
      "0.7760230416084\n",
      "-0.023147511097173213\n",
      "0.5727160576606235\n",
      "0.35191738613526763\n",
      "-1.1974982204774312\n",
      "-0.709304473135246\n",
      "0.3630885874096466\n",
      "2.066305326535635\n",
      "1.7499794158350688\n",
      "3.648857809350062\n",
      "4.561735216923892\n",
      "6.888496787409857\n",
      "7.682470419930697\n",
      "3.588209571501757\n",
      "1.6442593999840842\n",
      "4.193138935722276\n",
      "5.82067397719335\n",
      "7.956896321724408\n",
      "11.19736516776836\n",
      "10.162013503752917\n",
      "13.055534353169607\n",
      "14.947022055115823\n",
      "12.662706268340079\n",
      "15.584019157489553\n",
      "16.00366768764243\n",
      "16.154324585259985\n",
      "14.810286790574047\n",
      "14.838505717129953\n",
      "15.911079350801028\n",
      "14.96594850930593\n",
      "17.05815498039131\n",
      "19.18630361443659\n",
      "18.6096563560218\n",
      "23.660297794395674\n",
      "22.953229781326787\n",
      "22.00001882604356\n",
      "17.41948223276874\n",
      "12.38917939019587\n",
      "7.763587454061061\n",
      "7.621980557093772\n",
      "8.368552676774723\n",
      "10.548204352256484\n",
      "9.738516162002737\n",
      "9.720233453141907\n",
      "13.25400086742996\n",
      "11.151327131441535\n",
      "13.678075360165005\n",
      "15.576483751287737\n",
      "16.507059459661537\n",
      "17.05017390434615\n",
      "16.083490400925076\n",
      "18.12088289637381\n",
      "17.46232117924306\n",
      "16.484577892008442\n",
      "17.41717533637657\n",
      "15.11254315191462\n",
      "14.919757647496425\n",
      "15.632242000577136\n",
      "14.68697114020405\n",
      "10.979918411578188\n",
      "7.681434792497794\n",
      "8.88828738393038\n",
      "11.974318822651174\n",
      "9.487020059026406\n",
      "8.480139184792513\n",
      "8.455845055688425\n",
      "9.797404345479295\n",
      "11.160993556664396\n",
      "7.597032437376654\n",
      "9.357633224257915\n",
      "6.552247428343776\n",
      "9.144928326899025\n",
      "9.388372490617558\n",
      "5.917937578521624\n",
      "4.250723833727831\n",
      "2.5823506643224277\n",
      "1.6972936844551176\n",
      "1.6164022600107757\n",
      "2.112504512638189\n",
      "1.4800832101215455\n",
      "1.6517676988726324\n",
      "0.6770235050138849\n",
      "0.6843207083683972\n",
      "1.2938504184383317\n",
      "1.4740092509587002\n",
      "1.6455238387541828\n",
      "1.7167552412780112\n",
      "1.1317132534864534\n",
      "1.6253192854156389\n",
      "1.5845170159744522\n",
      "1.3942435270669409\n",
      "0.47567230304625924\n",
      "-0.054476308159552324\n",
      "0.975694230483614\n",
      "0.7833815032220309\n",
      "0.7693561366482329\n",
      "1.0116099264604799\n",
      "0.8337046294793924\n",
      "3.1922831421099813\n",
      "2.8734196026893444\n",
      "2.9765234426448024\n",
      "4.427374723779163\n",
      "5.536520605361071\n",
      "4.0005207709387465\n",
      "7.154057103660892\n",
      "9.594668109214037\n",
      "9.822559932192453\n",
      "9.809114704811282\n",
      "9.948906479006641\n",
      "11.854850276331188\n",
      "13.336103156348099\n",
      "9.565792630724163\n",
      "7.4418428554711475\n",
      "12.322643102396192\n",
      "14.36609397563873\n",
      "11.419384735607856\n",
      "7.806196735424539\n",
      "7.8588446873206435\n",
      "8.997905089919914\n",
      "15.493312537764492\n",
      "15.47277928157214\n",
      "12.206302782716072\n",
      "13.792786679728076\n",
      "15.320303846088068\n",
      "19.43305785858164\n",
      "19.077920465139535\n",
      "18.769574990153895\n",
      "15.160865032745168\n",
      "18.19239839207843\n",
      "21.58505409811402\n",
      "14.518169845056356\n",
      "14.491163277078126\n",
      "16.462664937603748\n",
      "17.373221361520244\n",
      "21.005944556503444\n",
      "23.545417427650644\n",
      "23.922834898165334\n",
      "22.735251124155\n",
      "22.018106874513048\n",
      "21.516554130927965\n",
      "22.0988619086174\n",
      "20.6025057727983\n",
      "18.059197750432133\n",
      "19.4120669797149\n",
      "21.691376237829026\n",
      "20.99797543328933\n",
      "19.518546213780812\n",
      "21.73575368441389\n",
      "20.73757788627718\n",
      "22.052811065685884\n",
      "22.756470814358934\n",
      "22.580366339441206\n",
      "23.117864661080773\n",
      "22.76524589989511\n",
      "22.014536393124423\n",
      "18.868052657607137\n",
      "18.907572132144832\n",
      "15.715437119762475\n",
      "16.847050908619142\n",
      "17.444262159077546\n",
      "15.76048221733871\n",
      "17.846661046481554\n",
      "17.884643728455995\n",
      "15.245255435035261\n",
      "15.219567093566843\n",
      "15.074719271838383\n",
      "14.762398626755795\n",
      "13.603368458570698\n",
      "14.560976348132119\n",
      "12.886265190063767\n",
      "12.450562300266776\n",
      "13.47891216660931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.719552850092304\n",
      "14.878458908679944\n",
      "12.137579126834696\n",
      "12.96815232802305\n",
      "12.380680927321084\n",
      "8.850706829343324\n",
      "13.592539193641644\n",
      "10.587469402247596\n",
      "3.495480788895548\n",
      "5.693346017572795\n",
      "2.941232495533581\n",
      "2.201342008408195\n",
      "3.5645354955082578\n",
      "6.227366919151181\n",
      "10.150525979342602\n",
      "12.199921678483989\n",
      "18.93848556832831\n",
      "18.192208854328964\n",
      "17.147241137735787\n",
      "16.980838946534057\n",
      "13.50994854619785\n",
      "12.49222986929844\n",
      "15.539995173194235\n",
      "15.2860402021167\n",
      "16.590257993941464\n",
      "18.422080578393032\n",
      "17.113925182511757\n",
      "14.95933859373665\n",
      "11.784400290137093\n",
      "12.901619455317956\n",
      "8.413444095094631\n",
      "9.331973057319823\n",
      "5.841271693295992\n",
      "5.4750284122634465\n",
      "5.264594552803585\n",
      "7.4336167463895135\n",
      "9.772547579542326\n",
      "6.317631711014307\n",
      "8.153581010385034\n",
      "7.7008367986088615\n",
      "3.514140865708023\n",
      "6.414716446763691\n",
      "4.995573891964292\n",
      "4.461507740382655\n",
      "4.701969245868009\n",
      "3.3990519647498965\n",
      "3.406750014831324\n",
      "2.4126913266602497\n",
      "2.1515917642674682\n",
      "1.0743761632817235\n",
      "0.6701508406049711\n",
      "0.1737254346524512\n",
      "-0.21800298972070511\n",
      "0.12863252575202702\n",
      "-0.12225184840676573\n",
      "-1.142452018409006\n",
      "-0.8073975155608536\n",
      "-0.8240939129869497\n",
      "-0.16631724072392612\n",
      "-0.7284460782842068\n",
      "-0.0766914984350188\n",
      "-0.09387027797124711\n",
      "-0.1567558296595545\n",
      "0.3059719842490612\n",
      "0.3830886619599133\n",
      "0.17353803133709897\n",
      "-0.45480906333963217\n",
      "-1.953546373858129\n",
      "-1.505647327524434\n",
      "1.4149259802039742\n",
      "0.5227105351996677\n",
      "-0.61234652277233\n",
      "-0.16047999030450397\n",
      "1.8665323251531052\n",
      "0.4610753350162958\n",
      "0.26583096249689625\n",
      "-3.8429073256515323\n",
      "-4.24647929804549\n",
      "-2.5289526722780913\n",
      "-4.276959870990319\n",
      "-6.646847682782568\n",
      "-5.102961114302994\n",
      "-1.0510026156117052\n",
      "-3.174900310672299\n",
      "-0.2646745260930485\n",
      "-0.019973180680409788\n",
      "-0.007841057167174639\n",
      "0.07064763479320377\n",
      "-2.866645335693161\n",
      "-2.853601381925729\n",
      "0.363578653482595\n",
      "-0.4287185944838297\n",
      "-0.7726527701398218\n",
      "-1.7590191712015808\n",
      "-1.5639105486919351\n",
      "0.08264315452303866\n",
      "-2.337642061833796\n",
      "-2.8452373884002253\n",
      "-6.8411247297607645\n",
      "-4.137032346785206\n",
      "-3.1647327203190887\n",
      "-3.994339449739885\n",
      "-3.969981287612327\n",
      "-2.9142082117521912\n",
      "-1.6058197356654873\n",
      "-1.4068226010839888\n",
      "-0.2715738709284076\n",
      "0.1782628146954553\n",
      "0.29277947386718156\n",
      "-0.549652216380783\n",
      "-1.756261482324188\n",
      "-1.7179223498524372\n",
      "-0.4614497147601771\n",
      "-2.01516236413446\n",
      "-3.526143642513851\n",
      "-3.8802804132425495\n",
      "-3.7718383535639033\n",
      "-4.714667421308708\n",
      "-5.749094448465262\n",
      "-3.3811669009648067\n",
      "-4.266260379655494\n",
      "-3.491374564543901\n",
      "-3.373882504214988\n",
      "-6.08616684532633\n",
      "-9.85354167026763\n",
      "-15.024035853102303\n",
      "-7.554558052249283\n",
      "-7.2861111273002725\n",
      "-10.18775307164011\n",
      "-10.432893683593743\n",
      "-4.600143931947812\n",
      "-3.977161523977303\n",
      "-4.503755448606946\n",
      "-4.743290182799293\n",
      "-6.311325738863093\n",
      "-5.1114228649109945\n",
      "-6.743399236889869\n",
      "-9.994014755832698\n",
      "-8.557262265805713\n",
      "-4.374786109151473\n",
      "-4.486627895489493\n",
      "-5.735623607874705\n",
      "-8.572753134843301\n",
      "-5.795180296116277\n",
      "-5.993266027330402\n",
      "-6.449518395019022\n",
      "-8.08444242051158\n",
      "-10.613581215444555\n",
      "-10.653425123967823\n",
      "-7.813362457637529\n",
      "-5.813339207798402\n",
      "-4.680380725448891\n",
      "-4.321162293977703\n",
      "-8.679686576489022\n",
      "-15.102729877624604\n",
      "-13.266351556996636\n",
      "-12.477133443685418\n",
      "-6.115292479494557\n",
      "-3.1589287670718704\n",
      "-2.0764897789183583\n",
      "-3.3256827130158544\n",
      "-3.6254511779512515\n",
      "-5.234338949214544\n",
      "-1.5974326902515348\n",
      "-1.7792156136904853\n",
      "-1.3141433373492626\n",
      "-1.9763310311556452\n",
      "-2.256293655045298\n",
      "-1.4808392266853014\n",
      "-1.734156428849088\n",
      "-1.5593522511945979\n",
      "-1.3686239143770462\n",
      "-0.25264485541736353\n",
      "-0.7052157511433471\n",
      "-1.1011558409134625\n",
      "-0.8493545138585956\n",
      "0.5143606289393271\n",
      "0.43691396771419877\n",
      "-1.5228888594236618\n",
      "-1.7838934394982977\n",
      "-4.135592884417626\n",
      "-2.6107702276852045\n",
      "-1.883804592903598\n",
      "-1.911587241126271\n",
      "-0.6213559691744\n",
      "-1.5047500114222234\n",
      "-0.928555679926771\n",
      "0.9634197402148664\n",
      "1.4987735958984623\n",
      "0.08603477033277311\n",
      "0.04606108821527349\n",
      "-0.1383890240945671\n",
      "-2.9196322357268274\n",
      "-3.302436164175716\n",
      "-3.0203506321146585\n",
      "-3.1245993825297402\n",
      "-6.1258007255326214\n",
      "-2.9477427330296613\n",
      "-2.198045801803429\n",
      "-4.964921283629744\n",
      "-4.131331700149184\n",
      "3.6495715641183284\n",
      "3.100075374871398\n",
      "3.6716770444802513\n",
      "2.3718107234870196\n",
      "0.13584605928920673\n",
      "-4.792241294807129\n",
      "-6.623248525426527\n",
      "-2.415705147631518\n",
      "-4.824049485771877\n",
      "-5.3632391330046705\n",
      "-3.0065549306517023\n",
      "-6.136531806492358\n",
      "-7.8406985822432125\n",
      "-2.3758813027889354\n",
      "1.2391015430422136\n",
      "0.7601370888160082\n",
      "2.6911585660084927\n",
      "-7.854879762507451\n",
      "-7.45627846028197\n",
      "-5.952876557447177\n",
      "-6.316987965685033\n",
      "-3.7918593455685032\n",
      "-1.8742658068408522\n",
      "-1.76197578933855\n",
      "-7.791872974961376\n",
      "-6.925479273557714\n",
      "-8.638939375448018\n",
      "-5.005482362015865\n",
      "-10.402205643652877\n",
      "-17.31034605511569\n",
      "-17.32862876397652\n",
      "-16.203570249175776\n",
      "-15.5308700404631\n",
      "-11.01853630114544\n",
      "-11.621240731243828\n",
      "-10.519403152073833\n",
      "-11.56656165184329\n",
      "-13.39895797218514\n",
      "-13.010041878202738\n",
      "-12.281376806978267\n",
      "-8.06654220224427\n",
      "-10.875887800329723\n",
      "-15.812690475806995\n",
      "-19.337341295100376\n",
      "-8.980107324490154\n",
      "-21.928853377533905\n",
      "-23.24455100980617\n",
      "-21.74342227930452\n",
      "-27.579207623149703\n",
      "-26.20859014730866\n",
      "-22.631664475202584\n",
      "-16.3464680518962\n",
      "-15.477374291336266\n",
      "-15.630343207987872\n",
      "-21.286103548331734\n",
      "-17.857370783100134\n",
      "-14.09038997469658\n",
      "-12.48899065208456\n",
      "-13.311413730045796\n",
      "-17.75511943613046\n",
      "-18.68536046523493\n",
      "-21.68372642595861\n",
      "-18.300658597970425\n",
      "-19.69933449092188\n",
      "-16.35705655265701\n",
      "-15.183221949445912\n",
      "-17.1354624765121\n",
      "-17.56919997937542\n",
      "-18.58490545801697\n",
      "-16.683358594970617\n",
      "-19.17993787813014\n",
      "-19.273953724508072\n",
      "-17.406375099751486\n",
      "-18.630954593401192\n",
      "-15.93578756012355\n",
      "-12.81075403369469\n",
      "-12.94994645879038\n",
      "-12.854272583991428\n",
      "-11.77391424221383\n",
      "-13.91876781142167\n",
      "-13.224680572870389\n",
      "-15.416903211519559\n",
      "-21.125855357572185\n",
      "-21.8300256628857\n",
      "-23.16450290028091\n",
      "-16.765223534815096\n",
      "-19.25534487411437\n",
      "-15.976360591340718\n",
      "-15.309051109430472\n",
      "-17.77318187139056\n",
      "-24.612146126939482\n",
      "-23.021569239067347\n",
      "-31.45914780759422\n",
      "-28.899599302929058\n",
      "-37.05802462498292\n",
      "-29.610417089852284\n",
      "-32.6018726066832\n",
      "-31.8235383998144\n",
      "-30.01252057157469\n",
      "-28.88677220766777\n",
      "-25.55555576076374\n",
      "-21.320780418747887\n",
      "-17.544430298350367\n",
      "-19.194995030155432\n",
      "-26.79151605711798\n",
      "-30.86645472727178\n",
      "-34.3707106017035\n",
      "-37.60948726838058\n",
      "-42.98057386185477\n",
      "-43.788448636883885\n",
      "-43.99991470868206\n",
      "-44.91130783077188\n",
      "-48.371314324665676\n",
      "-50.42468775522495\n",
      "-49.97530228897313\n",
      "-54.0062886918179\n",
      "-53.99284004934214\n",
      "-50.836756482946406\n",
      "-50.55118285865359\n",
      "-52.24070528193711\n",
      "-49.590400638675405\n",
      "-50.228524722262186\n",
      "-50.867065447388335\n",
      "-52.69493335231047\n",
      "-53.152337460647885\n",
      "-52.96482827736352\n",
      "-53.540100960207845\n",
      "-53.04077656583948\n",
      "-52.285477171946546\n",
      "-51.660801730835516\n",
      "-50.06546761620647\n",
      "-50.967469228187525\n",
      "-51.64844591862697\n",
      "-52.89876668771116\n",
      "-54.459936196111805\n",
      "-52.96957867393127\n",
      "-52.6503527075978\n",
      "-50.4887480994505\n",
      "-48.483780219539106\n",
      "-49.03397927794031\n",
      "-49.523273539512516\n",
      "-46.77782436111253\n",
      "-51.813558911622856\n",
      "-44.880326092696706\n",
      "-42.66061023509094\n",
      "-40.659805355478674\n",
      "-40.98507945432219\n",
      "-39.302308673033224\n",
      "-45.37234901484233\n",
      "-44.71844036408365\n",
      "-48.17864834855796\n",
      "-49.24470427421355\n",
      "-46.090543406069166\n",
      "-46.94321056167522\n",
      "-48.13102485457004\n",
      "-48.93481859157007\n",
      "-50.883130052316204\n",
      "-53.571314924714805\n",
      "-60.025054803527716\n",
      "-60.16962258750021\n",
      "-59.909303587663516\n",
      "-58.405091026751606\n",
      "-50.66035318725248\n",
      "-49.480438008132786\n",
      "-45.969864208718796\n",
      "-48.26217814701263\n",
      "-48.32981409526868\n",
      "-53.00603480862621\n",
      "-54.56976563796561\n",
      "-54.508252954300474\n",
      "-55.41073267952343\n",
      "-51.00650717438873\n",
      "-49.618219999164864\n",
      "-51.529293268556096\n",
      "-49.624373999607045\n",
      "-48.916663948756174\n",
      "-50.446916605878755\n",
      "-52.54445062451675\n",
      "-54.88324400011251\n",
      "-55.13558533899864\n",
      "-55.97866290900999\n",
      "-54.42523883512814\n",
      "-54.099377340016\n",
      "-52.75425354525203\n",
      "-51.75548864329941\n",
      "-53.66227938408103\n",
      "-52.531514246228745\n",
      "-51.13783805174974\n",
      "-51.00573194791794\n",
      "-55.44397521021388\n",
      "-53.856345548968605\n",
      "-53.46843861543607\n",
      "-55.9892975135477\n",
      "-52.81101924744401\n",
      "-51.527718909952405\n",
      "-51.59742440552697\n",
      "-51.343908274103605\n",
      "-53.208064974086014\n",
      "-53.74389331447352\n",
      "-54.41517796648071\n",
      "-53.6110393049276\n",
      "-57.38994050098012\n",
      "-57.13551253930256\n",
      "-58.777404638408996\n",
      "-58.84942215301761\n",
      "-60.44442158837733\n",
      "-62.41288893736417\n",
      "-63.6533400830977\n",
      "-65.02144746441877\n",
      "-59.11273960589995\n",
      "-56.883294143821495\n",
      "-57.568918777991136\n",
      "-56.87886135105016\n",
      "-55.28107519850908\n",
      "-51.53388315567834\n",
      "-54.755403349417385\n",
      "-50.65843048900113\n",
      "-48.44169942406266\n",
      "-47.602972684552604\n",
      "-47.15610072791535\n",
      "-49.190370119366094\n",
      "-50.16232994392101\n",
      "-49.097984980652505\n",
      "-50.151080622357995\n",
      "-49.265010426615994\n",
      "-51.53702162760196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48.86537605827855\n",
      "-49.40753598402665\n",
      "-48.35523947445405\n",
      "-44.844566637297106\n",
      "-46.06282649841726\n",
      "-48.24194371159646\n",
      "-49.37607271761499\n",
      "-54.13160900270929\n",
      "-54.85533584716384\n",
      "-54.07620250816215\n",
      "-56.32817051902871\n",
      "-51.0630474803381\n",
      "-50.679255735780494\n",
      "-52.10232223425807\n",
      "-48.335732454184495\n",
      "-54.19664606398615\n",
      "-56.281069534512305\n",
      "-56.48833162487634\n",
      "-56.71167198054659\n",
      "-54.7318255364025\n",
      "-55.56094902993952\n",
      "-51.88881735099264\n",
      "-55.17120477552025\n",
      "-50.637963827153186\n",
      "-51.47359990606381\n",
      "-51.094831765640755\n",
      "-49.16382117406226\n",
      "-46.384596710216556\n",
      "-45.54199725344711\n",
      "-46.97791475284806\n",
      "-45.869060861853164\n",
      "-43.49955212555985\n",
      "-41.952568920065325\n",
      "-36.38830067120108\n",
      "-38.78763342850525\n",
      "-37.46390861641093\n",
      "-38.987211556053026\n",
      "-40.12844395880836\n",
      "-39.625940111381354\n",
      "-35.439768395499314\n",
      "-36.0179643993084\n",
      "-34.58499071143972\n",
      "-29.612309052252378\n",
      "-31.644546462425048\n",
      "-28.279414711197315\n",
      "-28.640359472981224\n",
      "-27.17565272622867\n",
      "-27.578951491055825\n",
      "-29.05197399312287\n",
      "-28.056815431052012\n",
      "-26.661264349645837\n",
      "-25.687562826852574\n",
      "-23.90942894437189\n",
      "-21.813003123926734\n",
      "-21.856567778000368\n",
      "-23.906772000784756\n",
      "-22.270297949237428\n",
      "-24.402862300581123\n",
      "-21.96159901196224\n",
      "-23.598781695635957\n",
      "-23.928328077805315\n",
      "-25.223885189762505\n",
      "-25.800902985939768\n",
      "-26.986142297517496\n",
      "-23.430257023149647\n",
      "-20.44638984648661\n",
      "-20.208388489762932\n",
      "-19.82720929256286\n",
      "-18.64778759461069\n",
      "-17.455668574991467\n",
      "-16.931762329800225\n",
      "-12.696331289624052\n",
      "-12.84514062107112\n",
      "-11.948152367940855\n",
      "-12.430348355418124\n",
      "-11.873638419190602\n",
      "-10.486522621409392\n",
      "-9.081796888400596\n",
      "-7.1045301213286605\n",
      "-7.621949821242507\n",
      "-6.847157494345178\n",
      "-5.770521605665238\n",
      "-6.191014517954257\n",
      "-6.162135624369797\n",
      "-4.855125992721789\n",
      "-4.767086561867617\n",
      "-3.791223284202046\n",
      "-2.4282321414561445\n",
      "-2.509432845403841\n",
      "-1.9317977708803389\n",
      "-1.2407841174555638\n",
      "-1.4655413104917654\n",
      "-1.3726665325920475\n",
      "-2.072106505018503\n",
      "-2.1387460994293783\n",
      "-2.1897368766781087\n",
      "-2.4318739129442304\n",
      "-2.1889727492647135\n",
      "-1.3264816471980676\n",
      "-1.5411954739465885\n",
      "-1.5012384404151908\n",
      "-1.3045170394877759\n",
      "-1.289992642217744\n",
      "-1.7550828478055378\n",
      "-1.1553431466937587\n",
      "-0.5892361508287688\n",
      "-0.46254809455607976\n",
      "-0.09561795262512413\n",
      "0.16551853547785775\n",
      "0.0747999629217484\n",
      "-0.9486680257115198\n",
      "-0.807005206570401\n",
      "-0.7838978228346015\n",
      "0.15680662919150648\n",
      "0.05655610076182121\n",
      "-1.4640553174604645\n",
      "-1.0950880716095526\n",
      "-0.7486420938862882\n",
      "-0.49135996690930916\n",
      "0.11239161656620607\n",
      "0.22429700904087185\n",
      "-0.30328814679206934\n",
      "0.5215161058685617\n",
      "-0.8102521077470958\n",
      "-0.7261694908564769\n",
      "-0.7662934371357168\n",
      "-2.056525135974411\n",
      "-2.3963270471825453\n",
      "-2.131480485699777\n",
      "-2.3733285195866434\n",
      "-0.11519156723908817\n",
      "2.6420917676715905\n",
      "3.192888894511997\n",
      "3.3298126295861845\n",
      "4.0106379886468275\n",
      "5.094095101873495\n",
      "5.208393194991781\n",
      "6.638723599651661\n",
      "4.410392312181572\n",
      "8.077134971873313\n",
      "8.302714775817677\n",
      "10.850422293782977\n",
      "9.461483689267022\n",
      "6.441749054838133\n",
      "-0.26897284951510214\n",
      "0.003512851667502183\n",
      "-1.6467005522821292\n",
      "-3.158262823627794\n",
      "-10.839407759972685\n",
      "-3.950805531517516\n",
      "-2.6632124201325627\n",
      "-12.223562670748695\n",
      "-15.407199220256263\n",
      "-6.155765619195114\n",
      "-15.916898671973879\n",
      "-21.108554488404543\n",
      "-11.413877895589533\n",
      "-10.733311016500293\n",
      "-10.5361063796891\n",
      "-5.37087869959133\n",
      "-0.03473151192951484\n",
      "1.3690755606359133\n",
      "9.526506236391896\n",
      "12.383800616224486\n",
      "13.582845447662102\n",
      "17.342483802707893\n",
      "18.90858641523658\n",
      "15.879296773045667\n",
      "17.31985367844035\n",
      "21.96339364416666\n",
      "22.175476140537434\n",
      "29.03686902977339\n",
      "27.869441144003762\n",
      "26.025424964336427\n",
      "24.12927907337626\n",
      "21.422874671366653\n",
      "22.337163793664562\n",
      "21.748982053288913\n",
      "20.753326594955936\n",
      "18.71389528803995\n",
      "16.222954326040274\n",
      "17.061694725928668\n",
      "16.591726484613016\n",
      "11.663611809760003\n",
      "13.453751457254276\n",
      "14.429174187718383\n",
      "17.57501930054827\n",
      "13.226135403163601\n",
      "4.207323957967923\n",
      "3.0850946355874442\n",
      "6.394555008994283\n",
      "3.31072374495988\n",
      "-0.10376679519177247\n",
      "15.516454926219787\n",
      "9.231978234097191\n",
      "6.37551414913558\n",
      "18.31050773075358\n",
      "15.42259958928735\n",
      "17.29162621414798\n",
      "13.828037286034512\n",
      "15.461055261861814\n",
      "21.315070380601757\n",
      "21.041589606234446\n",
      "24.489530570959413\n",
      "29.786711102519483\n",
      "28.289134070386243\n",
      "25.825525817897663\n",
      "26.711855560828123\n",
      "28.09809709439557\n",
      "24.93143356497548\n",
      "21.54928098233608\n",
      "18.864388261117426\n",
      "19.328924794495627\n",
      "19.931630932141307\n",
      "18.71351279744643\n",
      "19.965872377997872\n",
      "23.042561825481382\n",
      "23.33514834650451\n",
      "22.518197957615747\n",
      "22.37208996598542\n",
      "18.676803945752198\n",
      "15.774893916487438\n",
      "16.417606179649173\n",
      "14.315402019166187\n",
      "15.17119398631752\n",
      "13.100635799806495\n",
      "6.581304333733479\n",
      "9.316017735418674\n",
      "8.207458396331742\n",
      "12.210791924548063\n",
      "11.495937495825359\n",
      "11.208973928036443\n",
      "7.864747678310827\n",
      "3.8539581448417897\n",
      "0.7347080810926391\n",
      "-0.3288642170260052\n",
      "0.33510829058786623\n",
      "-0.46609765849037077\n",
      "-2.665214946219846\n",
      "-0.34552091397755064\n",
      "-3.663018925933614\n",
      "-9.175822980062286\n",
      "-8.135479301537208\n",
      "-12.325633017705366\n",
      "-6.877347784250281\n",
      "-10.325369430584818\n",
      "-2.104525144140322\n",
      "-0.21708689059827715\n",
      "17.38845951355854\n",
      "21.14362067960339\n",
      "22.48478808729063\n",
      "31.939542332677895\n",
      "32.29605088659588\n",
      "37.5502626980874\n",
      "37.74308918364061\n",
      "31.82092585245687\n",
      "23.543720125641897\n",
      "29.700828303895744\n",
      "22.77952441035502\n",
      "15.855689931726811\n",
      "1.0918266562808714\n",
      "4.402554670108983\n",
      "1.707336196969152\n",
      "2.2201869272150834\n",
      "2.3534949308973956\n",
      "2.061709036111039\n",
      "1.8157763356570942\n",
      "1.7202228429422621\n",
      "0.48954953989243266\n",
      "0.650492275513205\n",
      "0.6459414485352725\n",
      "0.8743683741464748\n",
      "-0.8536195401084454\n",
      "-1.1529461771819074\n",
      "-1.4621424376060377\n",
      "-1.220077118326201\n",
      "-2.601049160948984\n",
      "-3.276952728382398\n",
      "-3.536453813065985\n",
      "-3.905692132049582\n",
      "-2.475768977418964\n",
      "-2.518500348413856\n",
      "-3.128203161090568\n",
      "-3.424838276744222\n",
      "-5.360276538452178\n",
      "-6.36851491278361\n",
      "-6.863208438894708\n",
      "-8.80732915169822\n",
      "-7.553386674806626\n",
      "-5.523121054869975\n",
      "-4.524024888742605\n",
      "-3.9860373548037344\n",
      "-3.855978600175644\n",
      "-4.407696521893533\n",
      "-7.04204840457503\n",
      "-10.13094468076582\n",
      "-11.489325872708786\n",
      "-12.489837595541642\n",
      "-10.758401716416335\n",
      "-7.207965225458864\n",
      "-7.85807116839714\n",
      "-10.238829226253465\n",
      "-8.177471304554446\n",
      "-11.69385076476294\n",
      "-17.331967018933366\n",
      "-12.738893613436987\n",
      "-11.30417993487725\n",
      "-2.712433324612613\n",
      "-4.056666633463544\n",
      "0.5337833390613161\n",
      "1.028374198891453\n",
      "0.5977442186570775\n",
      "0.77369864285648\n",
      "1.2060598626020422\n",
      "3.6005961195615748\n",
      "4.200857049484391\n",
      "3.6587722558171585\n",
      "2.6494184262168985\n",
      "2.4993969496355652\n",
      "2.5580601637563354\n",
      "3.9145671091023675\n",
      "2.048581412526275\n",
      "2.416724766808554\n",
      "3.195101448916258\n",
      "6.364293002102896\n",
      "8.953758162219538\n",
      "6.04086561565622\n",
      "5.540905159921396\n",
      "2.7148345629926958\n",
      "3.325404809693999\n",
      "4.886190119953831\n",
      "6.3682195071020065\n",
      "6.17328548130302\n",
      "6.538291644322148\n",
      "10.098463716167824\n",
      "11.677128754127457\n",
      "13.464582429730624\n",
      "13.945067454820803\n",
      "16.894100666716774\n",
      "18.52310249131211\n",
      "22.602339058001135\n",
      "23.575082646763303\n",
      "25.354700387841174\n",
      "17.628672144026012\n",
      "16.077338108030187\n",
      "6.8228023204253825\n",
      "15.092073074972092\n",
      "14.662566874299596\n",
      "15.691784174666722\n",
      "14.810696601924247\n",
      "16.036952907015195\n",
      "16.463747522587195\n",
      "16.515231781003457\n",
      "15.308275882959675\n",
      "9.301940715539278\n",
      "11.847430983345262\n",
      "18.035718975160215\n",
      "18.598014298581514\n",
      "17.462620000019246\n",
      "19.547345706416174\n",
      "31.21203156342316\n",
      "21.865515325813085\n",
      "21.927174858545374\n",
      "15.542191079012396\n",
      "19.56930817969235\n",
      "17.66978134509303\n",
      "11.445100397832954\n",
      "4.229583117586172\n",
      "15.385906105518748\n",
      "12.965906903333408\n",
      "4.996258618428586\n",
      "11.665114451377404\n",
      "4.893837370503306\n",
      "10.018798097236088\n",
      "12.14675548598461\n",
      "13.386518490159256\n",
      "0.3696583756175526\n",
      "0.0894285205766232\n",
      "3.3729267046180094\n",
      "6.447332307597795\n",
      "2.726892408198707\n",
      "1.461322388018813\n",
      "2.7088026521819284\n",
      "3.5511353046869276\n",
      "3.0604647599969668\n",
      "2.5038517270782945\n",
      "4.742849208711005\n",
      "3.89280655329353\n",
      "1.3047808555444678\n",
      "1.7226144763688243\n",
      "6.122834715885544\n",
      "3.888097137860806\n",
      "10.935001380048892\n",
      "10.447480406339952\n",
      "7.667896503789183\n",
      "9.084390652737909\n",
      "5.1064274353067765\n",
      "5.370555546266224\n",
      "3.4157850744427813\n",
      "5.593521949079412\n",
      "11.037537887416416\n",
      "11.365583335515666\n",
      "13.477751034450408\n",
      "11.88012539135482\n",
      "10.286155607012482\n",
      "15.771904001178264\n",
      "12.831466582352755\n",
      "17.569215347301053\n",
      "13.1281034055537\n",
      "16.570612662341215\n",
      "15.643694520267955\n",
      "13.988061788098742\n",
      "16.084344174571328\n",
      "19.046192528897535\n",
      "16.175325709410483\n",
      "17.29727648813887\n",
      "12.994276094051056\n",
      "11.254305893557818\n",
      "11.362962250421672\n",
      "9.215696770663174\n",
      "8.610749477196082\n",
      "6.692477188419661\n",
      "14.292066677866838\n",
      "11.400828819180223\n",
      "13.524633452913376\n",
      "2.69378989638624\n",
      "3.7513038167110833\n",
      "0.3611313113256148\n",
      "0.758675641777037\n",
      "4.553854459282166\n",
      "1.3448535754049324\n",
      "0.23030885617095245\n",
      "1.6568192640940915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848173104575595\n",
      "0.024272784762932813\n",
      "-0.07454084262011103\n",
      "-0.05071586213463718\n",
      "-0.029061174257934877\n",
      "0.6016145880389475\n",
      "-0.9278491822344977\n",
      "-0.9156602827737869\n",
      "-1.241764676488284\n",
      "-0.07531094644902997\n",
      "0.02259938841627969\n",
      "-0.12678154448695356\n",
      "0.5147070475962937\n",
      "1.1582628391205276\n",
      "0.19737026889856596\n",
      "-1.3223241964276455\n",
      "-0.6894004481201826\n",
      "0.8501645316054768\n",
      "0.12038848742382162\n",
      "0.0013523774556625247\n",
      "0.21794792132052188\n",
      "0.19101563164951535\n",
      "0.7791055913581914\n",
      "0.874022595819743\n",
      "0.23307209457704572\n",
      "0.22350299954985786\n",
      "0.28696570222303136\n",
      "0.4469803858903282\n",
      "0.2469685413302598\n",
      "1.3886139570869696\n",
      "1.8086214990580942\n",
      "2.0733773470909482\n",
      "1.2272996164866663\n",
      "3.5965022749277984\n",
      "6.007264500034343\n",
      "10.03281706050754\n",
      "17.99483346278852\n",
      "15.452152110278702\n",
      "14.879440748373133\n",
      "18.24046926345698\n",
      "17.757088238158715\n",
      "20.823206260354336\n",
      "20.166959977352217\n",
      "22.549347035325592\n",
      "23.15399636179014\n",
      "21.91244384805295\n",
      "27.166239018005097\n",
      "21.905700743794856\n",
      "23.24311837762776\n",
      "23.770204075877643\n",
      "22.133519996013337\n",
      "22.048489263488555\n",
      "21.126563989698575\n",
      "21.036371341708556\n",
      "22.760434031624836\n",
      "17.834534045610198\n",
      "19.847167105317638\n",
      "19.650474732694196\n",
      "21.443177408674515\n",
      "22.232287946506307\n",
      "20.95072759970577\n",
      "22.507361862497522\n",
      "19.924349950486075\n",
      "21.15186130283701\n",
      "18.01669519077444\n",
      "16.976539342451535\n",
      "19.15512721597007\n",
      "14.451394500635725\n",
      "11.083785098286572\n",
      "11.914558082508147\n",
      "7.013991268125086\n",
      "9.922734901654444\n",
      "12.654544192281914\n",
      "16.110862384023903\n",
      "15.559799306692687\n",
      "19.0044276296702\n",
      "21.424669303571076\n",
      "17.77335604121439\n",
      "24.18040645440839\n",
      "16.639604403147512\n",
      "18.683029663180662\n",
      "15.671157003373283\n",
      "20.71779082825165\n",
      "18.117305584796018\n",
      "23.4459152318219\n",
      "22.970786782588302\n",
      "17.465177905863417\n",
      "13.260115594284414\n",
      "15.434801722239577\n",
      "13.984020023657386\n",
      "14.076994266186892\n",
      "14.110369985566159\n",
      "14.325707067076443\n",
      "13.408871991765414\n",
      "14.840457443685285\n",
      "12.035126289284506\n",
      "12.298202977998594\n",
      "11.452666539886035\n",
      "12.729432093889226\n",
      "12.859425961720202\n",
      "13.272235764966602\n",
      "12.055258271863117\n",
      "12.402132843956801\n",
      "14.604660530516227\n",
      "11.700872199229714\n",
      "12.690494892978275\n",
      "15.35923592435714\n",
      "16.63467812920864\n",
      "16.315713417610922\n",
      "16.112863629450715\n",
      "15.372274755482696\n",
      "16.906147412865383\n",
      "16.626338468232056\n",
      "17.927650014564982\n",
      "14.761908560682846\n",
      "17.09786228513118\n",
      "16.894606100715354\n",
      "17.627526379792744\n",
      "17.830828667985465\n",
      "18.16419824854274\n",
      "16.851432474971705\n",
      "16.73658284407794\n",
      "14.72129625587795\n",
      "17.134904108547452\n",
      "19.693130971608216\n",
      "14.559478829156593\n",
      "13.931092887778957\n",
      "17.35343088840013\n",
      "16.595500164129447\n",
      "14.602783936041766\n",
      "18.855341675561746\n",
      "17.420146468665525\n",
      "18.356915451069234\n",
      "16.687606972634363\n",
      "18.516861405958007\n",
      "14.943268866166902\n",
      "18.23451333850073\n",
      "18.83183899462773\n",
      "17.93691858126869\n",
      "16.42763801999263\n",
      "18.658843963329648\n",
      "18.258282396812373\n",
      "15.96454094898201\n",
      "15.105882009926566\n",
      "13.44578745668204\n",
      "9.121401740302916\n",
      "10.62518485307076\n",
      "14.702584952646701\n",
      "16.904188856120882\n",
      "12.403063457231214\n",
      "10.596709794420976\n",
      "2.5600479622482206\n",
      "1.9206818514495547\n",
      "3.871092528148413\n",
      "7.798272862213678\n",
      "1.2154306686998881\n",
      "-3.926839251493588\n",
      "-8.899232762075327\n",
      "-5.741238023372455\n",
      "-6.185722828894789\n",
      "-14.410088931630074\n",
      "-18.058612061710804\n",
      "-21.022830491679006\n",
      "-28.96890181734259\n",
      "-33.487550066547925\n",
      "-37.8217183213658\n",
      "-38.688008289271444\n",
      "-38.83966922469699\n",
      "-45.267174346907886\n",
      "-37.071247871216066\n",
      "-42.145234896173044\n",
      "-35.24923002331805\n",
      "-36.557040494646245\n",
      "-35.8621233881108\n",
      "-30.593182273674152\n",
      "-33.079159395694525\n",
      "-25.487352906806578\n",
      "-20.062651035895076\n",
      "-16.790565244183135\n",
      "-17.47152315160236\n",
      "-16.01537463187983\n",
      "-16.997489240183256\n",
      "-12.769069389190102\n",
      "-12.96680848830385\n",
      "-14.868884691010878\n",
      "-7.751897585296584\n",
      "-11.93977172582925\n",
      "-11.158570547056334\n",
      "-10.376273122921631\n",
      "-4.608738017470981\n",
      "-5.1727485721476\n",
      "-2.7665548898221477\n",
      "-1.0594088709326983\n",
      "-4.043470707987079\n",
      "-1.2521260734591932\n",
      "-7.420249212410153\n",
      "-3.2960601825855087\n",
      "-1.3184139131278132\n",
      "-3.7460526818998128\n",
      "-3.69436095648751\n",
      "-5.567127109918721\n",
      "-3.951336578725485\n",
      "-5.9307843105767715\n",
      "-6.609072467804166\n",
      "-7.824036335762966\n",
      "-9.495701229461492\n",
      "-9.032429988627037\n",
      "-10.134130964013632\n",
      "-11.398380196362774\n",
      "-9.597141491467228\n",
      "-11.345118381215014\n",
      "-11.617489249842198\n",
      "-11.024480273270644\n",
      "-12.485257953703147\n",
      "-14.172295895676081\n",
      "-12.500653200092357\n",
      "-14.125044646997932\n",
      "-15.903888869152297\n",
      "-15.080028036370775\n",
      "-17.20620957593509\n",
      "-17.964686715339376\n",
      "-17.536601194014242\n",
      "-18.226105375632443\n",
      "-16.70591358715729\n",
      "-15.316122062768729\n",
      "-18.42375738983427\n",
      "-17.412942326638454\n",
      "-17.708197744079765\n",
      "-17.233019775974682\n",
      "-18.198311628352368\n",
      "-15.19435345024574\n",
      "-16.522775724941738\n",
      "-16.77358880900107\n",
      "-16.53149616896455\n",
      "-17.015449222605806\n",
      "-14.694176989778414\n",
      "-15.939392192458024\n",
      "-10.508871000373674\n",
      "-12.444461233790662\n",
      "-10.54869271078214\n",
      "-7.985525913404163\n",
      "-7.640712350892532\n",
      "-8.51926592345293\n",
      "-6.42612243579079\n",
      "-6.1269977161846665\n",
      "-5.5482578585629145\n",
      "-2.9255309578487796\n",
      "-2.1903823295546747\n",
      "-1.5371076057283357\n",
      "-1.145238308703548\n",
      "-1.0307297603814611\n",
      "-1.2845101347483392\n",
      "-1.3227331540042\n",
      "-1.5965101878267598\n",
      "-2.1467459584947557\n",
      "-2.0635521199698847\n",
      "-1.08165479705943\n",
      "0.38244918549887086\n",
      "1.1744130348564343\n",
      "1.546025698350257\n",
      "0.5298011253317871\n",
      "0.747850645716213\n",
      "3.4005838481146835\n",
      "2.603146455911001\n",
      "2.6725166318860007\n",
      "2.8733794753279702\n",
      "2.446944725347864\n",
      "1.622624775788069\n",
      "1.6038594713736936\n",
      "1.4443914162834093\n",
      "-0.5588576038346676\n",
      "-1.0705489094689893\n",
      "-1.8674590964461113\n",
      "-2.768947163578945\n",
      "-1.9836495788513138\n",
      "-2.235920054524796\n",
      "-3.1506727759126174\n",
      "-2.4986932267076423\n",
      "-0.6902051297817747\n",
      "-1.2049584945983776\n",
      "-1.1407615465894279\n",
      "-0.757209712426423\n",
      "-1.9009334266815205\n",
      "-2.800663146989899\n",
      "-1.9727204224056472\n",
      "-5.667238900130891\n",
      "-5.816181420266774\n",
      "-5.387976370631164\n",
      "-3.451304406004375\n",
      "-4.232479971567903\n",
      "-3.3175932077176205\n",
      "-4.5666666135046405\n",
      "-2.1210580439131608\n",
      "-2.6297060733854187\n",
      "-1.3409723204090729\n",
      "-1.5463817219607439\n",
      "-1.413914898763401\n",
      "0.4965351158660632\n",
      "-0.8168573275613212\n",
      "1.2345385497898218\n",
      "1.4982457503416673\n",
      "1.6646741950830177\n",
      "2.898788205927241\n",
      "1.2186141771833487\n",
      "1.7997002047849997\n",
      "0.6598045981162827\n",
      "2.645191392894307\n",
      "3.0106434392001895\n",
      "2.8116900605180617\n",
      "4.040386877576818\n",
      "1.2933972780755836\n",
      "1.763087189182558\n",
      "2.6720865434117016\n",
      "7.218932801718612\n",
      "7.6785413536106475\n",
      "6.128049565316848\n",
      "5.5412240443782705\n",
      "3.1264727753530277\n",
      "5.869249855683657\n",
      "3.416858054472708\n",
      "7.01747551837544\n",
      "1.3931054467964876\n",
      "2.0161471920354113\n",
      "-0.2003909200590041\n",
      "-0.7377547723492879\n",
      "-0.9763537706253433\n",
      "0.626363138166254\n",
      "-0.8071183315785293\n",
      "1.5683713025501882\n",
      "0.2869020960863856\n",
      "1.4457064411420484\n",
      "4.352296430648736\n",
      "5.11994822165564\n",
      "9.451383547031865\n",
      "7.451309497660786\n",
      "6.994013391689623\n",
      "-0.5190965113550847\n",
      "-1.8643709971676192\n",
      "-3.469028746900558\n",
      "-1.148200476369218\n",
      "-2.556659761533071\n",
      "-2.608718609613262\n",
      "-2.263806863132312\n",
      "-4.284884597974826\n",
      "-4.6025251066472075\n",
      "-5.227704274209523\n",
      "-8.280002689280092\n",
      "-8.891005799220046\n",
      "-10.149611587767465\n",
      "-11.033041915395255\n",
      "-11.414574574884874\n",
      "-10.705346514490968\n",
      "-13.279036925832642\n",
      "-12.01544339164382\n",
      "-12.192048177918256\n",
      "-13.031270106143138\n",
      "-16.1010917984162\n",
      "-17.49887122903909\n",
      "-16.03199760477235\n",
      "-17.051645810112287\n",
      "-18.058009297516552\n",
      "-18.67770040808076\n",
      "-18.781367311755922\n",
      "-18.44714737264698\n",
      "-19.004473733447096\n",
      "-21.585410975498153\n",
      "-20.715758846973575\n",
      "-19.866218210460094\n",
      "-19.876553994221617\n",
      "-20.648081917582502\n",
      "-20.934813258939638\n",
      "-21.092061289106255\n",
      "-21.794901415078908\n",
      "-22.79771661920935\n",
      "-21.30909054998341\n",
      "-21.02099658588686\n",
      "-19.884628985367865\n",
      "-21.300887492790224\n",
      "-20.84585687518939\n",
      "-22.402918024804272\n",
      "-22.466324378416793\n",
      "-21.759138544584722\n",
      "-22.531807109537\n",
      "-23.065810081868815\n",
      "-24.25456864841639\n",
      "-21.821822605692514\n",
      "-21.449310918549187\n",
      "-20.682680240823203\n",
      "-18.53781984142619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"../Saver/model.ckpt\")\n",
    "\n",
    "img = cv2.imread('drive_wheel.jpg', 0) #here, second parameter '0' specifies that img.shape will return only height and\n",
    "#width of the image and not the number of channels. It is a colored image so number of channels = 3, which it will not return.\n",
    "rows, cols = img.shape\n",
    "\n",
    "i = 0\n",
    "while(cv2.waitKey(60) != ord(\"q\")):\n",
    "    full_image = cv2.imread(test_x[i])\n",
    "    cv2.imshow('Frame Window', full_image)\n",
    "    image = ((cv2.resize(full_image[-150:], (200, 66)) / 255.0).reshape((1, 66, 200, 3)))\n",
    "    degrees = sess.run(y_predicted, feed_dict = {x_input: image, keep_prob: 1.0})[0][0] *180 / pi #here, we have converted the\n",
    "    #predicted degrees from radians to degrees.\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), -degrees, 1) #this function rotate the image by a given degrees.\n",
    "    print(degrees)\n",
    "    dst = cv2.warpAffine(src = img, M = M, dsize = (cols, rows)) #warpAffine function applies rotation to the image\n",
    "    cv2.imshow(\"Steering Wheel\", dst)\n",
    "    i += 1\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the file \"Visualize_Output.py\" at command prompt to visualize the output better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
